# MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning

[Link to the paper](http://arxiv.org/abs/2310.03731v1)

## Authors
- Ke Wang
- Houxing Ren
- Aojun Zhou
- Zimu Lu
- Sichun Luo
- Weikang Shi
- Renrui Zhang
- Linqi Song
- Mingjie Zhan
- Hongsheng Li

## Summary
  The recently released GPT-4 Code Interpreter has demonstrated remarkable
proficiency in solving challenging math problems, primarily attributed to its
ability to seamlessly reason with natural language, generate code, execute
code, and continue reasoning based on the execution output. In this paper, we
present a method to fine-tune open-source language models, enabling them to use
code for modeling and deriving math equations and, consequently, enhancing
their mathematical reasoning abilities. We propose a method of generating novel
and high-quality datasets with math problems and their code-based solutions,
referred to as MathCodeInstruct. Each solution interleaves natural language,
code, and execution results. We also introduce a customized supervised
fine-tuning and inference approach. This approach yields the MathCoder models,
a family of models capable of generating code-based solutions for solving
challenging math problems. Impressively, the MathCoder models achieve
state-of-the-art scores among open-source LLMs on the MATH (45.2%) and GSM8K
(83.9%) datasets, substantially outperforming other open-source alternatives.
Notably, the MathCoder model not only surpasses ChatGPT-3.5 and PaLM-2 on GSM8K
and MATH but also outperforms GPT-4 on the competition-level MATH dataset. The
dataset and models will be released at https://github.com/mathllm/MathCoder.


# HeaP: Hierarchical Policies for Web Actions using LLMs

[Link to the paper](http://arxiv.org/abs/2310.03720v1)

## Authors
- Paloma Sodhi
- S. R. K. Branavan
- Ryan McDonald

## Summary
  Large language models (LLMs) have demonstrated remarkable capabilities in
performing a range of instruction following tasks in few and zero-shot
settings. However, teaching LLMs to perform tasks on the web presents
fundamental challenges -- combinatorially large open-world tasks and variations
across web interfaces. We tackle these challenges by leveraging LLMs to
decompose web tasks into a collection of sub-tasks, each of which can be solved
by a low-level, closed-loop policy. These policies constitute a shared grammar
across tasks, i.e., new web tasks can be expressed as a composition of these
policies. We propose a novel framework, Hierarchical Policies for Web Actions
using LLMs (HeaP), that learns a set of hierarchical LLM prompts from
demonstrations for planning high-level tasks and executing them via a sequence
of low-level policies. We evaluate HeaP against a range of baselines on a suite
of web tasks, including MiniWoB++, WebArena, a mock airline CRM, as well as
live website interactions, and show that it is able to outperform prior works
using orders of magnitude less data.


# Constraint-Conditioned Policy Optimization for Versatile Safe Reinforcement Learning

[Link to the paper](http://arxiv.org/abs/2310.03718v1)

## Authors
- Yihang Yao
- Zuxin Liu
- Zhepeng Cen
- Jiacheng Zhu
- Wenhao Yu
- Tingnan Zhang
- Ding Zhao

## Summary
  Safe reinforcement learning (RL) focuses on training reward-maximizing agents
subject to pre-defined safety constraints. Yet, learning versatile safe
policies that can adapt to varying safety constraint requirements during
deployment without retraining remains a largely unexplored and challenging
area. In this work, we formulate the versatile safe RL problem and consider two
primary requirements: training efficiency and zero-shot adaptation capability.
To address them, we introduce the Conditioned Constrained Policy Optimization
(CCPO) framework, consisting of two key modules: (1) Versatile Value Estimation
(VVE) for approximating value functions under unseen threshold conditions, and
(2) Conditioned Variational Inference (CVI) for encoding arbitrary constraint
thresholds during policy optimization. Our extensive experiments demonstrate
that CCPO outperforms the baselines in terms of safety and task performance
while preserving zero-shot adaptation capabilities to different constraint
thresholds data-efficiently. This makes our approach suitable for real-world
dynamic applications.


# DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines

[Link to the paper](http://arxiv.org/abs/2310.03714v1)

## Authors
- Omar Khattab
- Arnav Singhvi
- Paridhi Maheshwari
- Zhiyuan Zhang
- Keshav Santhanam
- Sri Vardhamanan
- Saiful Haq
- Ashutosh Sharma
- Thomas T. Joshi
- Hanna Moazam
- Heather Miller
- Matei Zaharia
- Christopher Potts

## Summary
  The ML community is rapidly exploring techniques for prompting language
models (LMs) and for stacking them into pipelines that solve complex tasks.
Unfortunately, existing LM pipelines are typically implemented using hard-coded
"prompt templates", i.e. lengthy strings discovered via trial and error. Toward
a more systematic approach for developing and optimizing LM pipelines, we
introduce DSPy, a programming model that abstracts LM pipelines as text
transformation graphs, i.e. imperative computational graphs where LMs are
invoked through declarative modules. DSPy modules are parameterized, meaning
they can learn (by creating and collecting demonstrations) how to apply
compositions of prompting, finetuning, augmentation, and reasoning techniques.
We design a compiler that will optimize any DSPy pipeline to maximize a given
metric. We conduct two case studies, showing that succinct DSPy programs can
express and optimize sophisticated LM pipelines that reason about math word
problems, tackle multi-hop retrieval, answer complex questions, and control
agent loops. Within minutes of compiling, a few lines of DSPy allow GPT-3.5 and
llama2-13b-chat to self-bootstrap pipelines that outperform standard few-shot
prompting (generally by over 25% and 65%, respectively) and pipelines with
expert-created demonstrations (by up to 5-46% and 16-40%, respectively). On top
of that, DSPy programs compiled to open and relatively small LMs like
770M-parameter T5 and llama2-13b-chat are competitive with approaches that rely
on expert-written prompt chains for proprietary GPT-3.5. DSPy is available at
https://github.com/stanfordnlp/dspy


# Agent Instructs Large Language Models to be General Zero-Shot Reasoners

[Link to the paper](http://arxiv.org/abs/2310.03710v1)

## Authors
- Nicholas Crispino
- Kyle Montgomery
- Fankun Zeng
- Dawn Song
- Chenguang Wang

## Summary
  We introduce a method to improve the zero-shot reasoning abilities of large
language models on general language understanding tasks. Specifically, we build
an autonomous agent to instruct the reasoning process of large language models.
We show this approach further unleashes the zero-shot reasoning abilities of
large language models to more tasks. We study the performance of our method on
a wide set of datasets spanning generation, classification, and reasoning. We
show that our method generalizes to most tasks and obtains state-of-the-art
zero-shot performance on 20 of the 29 datasets that we evaluate. For instance,
our method boosts the performance of state-of-the-art large language models by
a large margin, including Vicuna-13b (13.3%), Llama-2-70b-chat (23.2%), and
GPT-3.5 Turbo (17.0%). Compared to zero-shot chain of thought, our improvement
in reasoning is striking, with an average increase of 10.5%. With our method,
Llama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo by 10.2%.


# Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!

[Link to the paper](http://arxiv.org/abs/2310.03693v1)

## Authors
- Xiangyu Qi
- Yi Zeng
- Tinghao Xie
- Pin-Yu Chen
- Ruoxi Jia
- Prateek Mittal
- Peter Henderson

## Summary
  Optimizing large language models (LLMs) for downstream use cases often
involves the customization of pre-trained LLMs through further fine-tuning.
Meta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5
Turbo on custom datasets also encourage this practice. But, what are the safety
costs associated with such custom fine-tuning? We note that while existing
safety alignment infrastructures can restrict harmful behaviors of LLMs at
inference time, they do not cover safety risks when fine-tuning privileges are
extended to end-users. Our red teaming studies find that the safety alignment
of LLMs can be compromised by fine-tuning with only a few adversarially
designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety
guardrails by fine-tuning it on only 10 such examples at a cost of less than
$0.20 via OpenAI's APIs, making the model responsive to nearly any harmful
instructions. Disconcertingly, our research also reveals that, even without
malicious intent, simply fine-tuning with benign and commonly used datasets can
also inadvertently degrade the safety alignment of LLMs, though to a lesser
extent. These findings suggest that fine-tuning aligned LLMs introduces new
safety risks that current safety infrastructures fall short of addressing --
even if a model's initial safety alignment is impeccable, it is not necessarily
to be maintained after custom fine-tuning. We outline and critically analyze
potential mitigations and advocate for further research efforts toward
reinforcing safety protocols for the custom fine-tuning of aligned LLMs.


# DirectGPT: A Direct Manipulation Interface to Interact with Large Language Models

[Link to the paper](http://arxiv.org/abs/2310.03691v1)

## Authors
- Damien Masson
- Sylvain Malacria
- GÃ©ry Casiez
- Daniel Vogel

## Summary
  We characterize and demonstrate how the principles of direct manipulation can
improve interaction with large language models. This includes: continuous
representation of generated objects of interest; reuse of prompt syntax in a
toolbar of commands; manipulable outputs to compose or control the effect of
prompts; and undo mechanisms. This idea is exemplified in DirectGPT, a user
interface layer on top of ChatGPT that works by transforming direct
manipulation actions to engineered prompts. A study shows participants were 50%
faster and relied on 50% fewer and 72% shorter prompts to edit text, code, and
vector images compared to baseline ChatGPT. Our work contributes a validated
approach to integrate LLMs into traditional software using direct manipulation.


# SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks

[Link to the paper](http://arxiv.org/abs/2310.03684v1)

## Authors
- Alexander Robey
- Eric Wong
- Hamed Hassani
- George J. Pappas

## Summary
  Despite efforts to align large language models (LLMs) with human values,
widely-used LLMs such as GPT, Llama, Claude, and PaLM are susceptible to
jailbreaking attacks, wherein an adversary fools a targeted LLM into generating
objectionable content. To address this vulnerability, we propose SmoothLLM, the
first algorithm designed to mitigate jailbreaking attacks on LLMs. Based on our
finding that adversarially-generated prompts are brittle to character-level
changes, our defense first randomly perturbs multiple copies of a given input
prompt, and then aggregates the corresponding predictions to detect adversarial
inputs. SmoothLLM reduces the attack success rate on numerous popular LLMs to
below one percentage point, avoids unnecessary conservatism, and admits
provable guarantees on attack mitigation. Moreover, our defense uses
exponentially fewer queries than existing attacks and is compatible with any
LLM.


# Signum phase mask differential microscopy

[Link to the paper](http://arxiv.org/abs/2310.03674v1)

## Authors
- Jeeban Kumar Nayak
- Niladri Modak
- Sayan Ghosh
- Nirmalya Ghosh

## Summary
  We propose and experimentally demonstrate a differential microscopy method to
obtain simultaneous amplitude, phase, and quantitative polarization gradient
imaging in a single experimental embodiment. A full-field optical spatial
differentiator is achieved in a relatively simple setup by placing a glass
cover slip as a Signum phase mask in the Fourier plane of a standard 4-f
imaging system and accordingly named Signum phase mask differential microscopy.
The longstanding requisite of polarized light to obtain the spatial
differentiation of the field at the object plane is eliminated in our scheme
and, hence, leads to the emergence of quantitative differential polarization
contrast imaging by integrating polarization degree of freedom as an additional
contrast agent in the framework of differential microscopy. Implementation of
the proposed differential imaging scheme in high-resolution microscopy is
experimentally demonstrated alongside its functionality for a broad wavelength
range. Simultaneous acquisition of differential phase, amplitude, and
polarization (anisotropy) gradient imaging in a rather elementary optical setup
enables a low-cost multi-functional differential microscopy system that is
anticipated to emerge as a revolutionary tool in label-free imaging and optical
image processing.


# Paradoxes in the co-evolution of contagions and institutions

[Link to the paper](http://arxiv.org/abs/2310.03672v1)

## Authors
- Jonathan St-Onge
- Giulio Burgio
- Samuel F. Rosenblatt
- Timothy M. Waring
- Laurent HÃ©bert-Dufresne

## Summary
  Epidemic models study the spread of an undesired agent through a population,
be it infectious diseases through a country, misinformation in online social
media, or pests infesting a region. In combating these epidemics, we rely
neither on global top-down interventions, nor solely on individual adaptations.
Instead, interventions most commonly come from local institutions such as
public health departments, moderation teams on social media platforms, or other
forms of group governance. Classic models, which are often individual or
agent-based, are ill-suited to capture local adaptations. We leverage recent
development of institutional dynamics based on cultural group selection to
study how groups can attempt local control of an epidemic by taking inspiration
from the successes and failures of other groups. Incorporating these
institutional changes into the epidemic dynamics reveals paradoxes: a higher
transmission rate can result in smaller outbreaks and decreasing the speed of
institutional adaptation generally reduces outbreak size. When groups perceive
a contagion as more worrisome, they can invest in improved policies and, if
they maintain these policies long enough to have impact, lead to a reduction in
endemicity. By looking at the interplay between the speed of institutions and
the transmission rate of the contagions, we find rich co-evolutionary dynamics
that reflect the complexity of known biological and social contagions.


# GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction

[Link to the paper](http://arxiv.org/abs/2310.03668v1)

## Authors
- Oscar Sainz
- Iker GarcÃ­a-Ferrero
- Rodrigo Agerri
- Oier Lopez de Lacalle
- German Rigau
- Eneko Agirre

## Summary
  Large Language Models (LLMs) combined with instruction tuning have made
significant progress when generalizing to unseen tasks. However, they have been
less successful in Information Extraction (IE), lagging behind task-specific
models. Typically, IE tasks are characterized by complex annotation guidelines
which describe the task and give examples to humans. Previous attempts to
leverage such information have failed, even with the largest models, as they
are not able to follow the guidelines out-of-the-box. In this paper we propose
GoLLIE (Guideline-following Large Language Model for IE), a model able to
improve zero-shot results on unseen IE tasks by virtue of being fine-tuned to
comply with annotation guidelines. Comprehensive evaluation empirically
demonstrates that GoLLIE is able to generalize to and follow unseen guidelines,
outperforming previous attempts at zero-shot information extraction. The
ablation study shows that detailed guidelines is key for good results.


# MapperGPT: Large Language Models for Linking and Mapping Entities

[Link to the paper](http://arxiv.org/abs/2310.03666v1)

## Authors
- Nicolas Matentzoglu
- J. Harry Caufield
- Harshad B. Hegde
- Justin T. Reese
- Sierra Moxon
- Hyeongsik Kim
- Nomi L. Harris
- Melissa A Haendel
- Christopher J. Mungall

## Summary
  Aligning terminological resources, including ontologies, controlled
vocabularies, taxonomies, and value sets is a critical part of data integration
in many domains such as healthcare, chemistry, and biomedical research. Entity
mapping is the process of determining correspondences between entities across
these resources, such as gene identifiers, disease concepts, or chemical entity
identifiers. Many tools have been developed to compute such mappings based on
common structural features and lexical information such as labels and synonyms.
Lexical approaches in particular often provide very high recall, but low
precision, due to lexical ambiguity. As a consequence of this, mapping efforts
often resort to a labor intensive manual mapping refinement through a human
curator.
  Large Language Models (LLMs), such as the ones employed by ChatGPT, have
generalizable abilities to perform a wide range of tasks, including
question-answering and information extraction. Here we present MapperGPT, an
approach that uses LLMs to review and refine mapping relationships as a
post-processing step, in concert with existing high-recall methods that are
based on lexical and structural heuristics.
  We evaluated MapperGPT on a series of alignment tasks from different domains,
including anatomy, developmental biology, and renal diseases. We devised a
collection of tasks that are designed to be particularly challenging for
lexical methods. We show that when used in combination with high-recall
methods, MapperGPT can provide a substantial improvement in accuracy, beating
state-of-the-art (SOTA) methods such as LogMap.


# Balancing Autonomy and Alignment: A Multi-Dimensional Taxonomy for Autonomous LLM-powered Multi-Agent Architectures

[Link to the paper](http://arxiv.org/abs/2310.03659v1)

## Authors
- Thorsten HÃ¤ndler

## Summary
  Large language models (LLMs) have revolutionized the field of artificial
intelligence, endowing it with sophisticated language understanding and
generation capabilities. However, when faced with more complex and
interconnected tasks that demand a profound and iterative thought process, LLMs
reveal their inherent limitations. Autonomous LLM-powered multi-agent systems
represent a strategic response to these challenges. Such systems strive for
autonomously tackling user-prompted goals by decomposing them into manageable
tasks and orchestrating their execution and result synthesis through a
collective of specialized intelligent agents. Equipped with LLM-powered
reasoning capabilities, these agents harness the cognitive synergy of
collaborating with their peers, enhanced by leveraging contextual resources
such as tools and datasets. While these architectures hold promising potential
in amplifying AI capabilities, striking the right balance between different
levels of autonomy and alignment remains the crucial challenge for their
effective operation. This paper proposes a comprehensive multi-dimensional
taxonomy, engineered to analyze how autonomous LLM-powered multi-agent systems
balance the dynamic interplay between autonomy and alignment across various
aspects inherent to architectural viewpoints such as goal-driven task
management, agent composition, multi-agent collaboration, and context
interaction. It also includes a domain-ontology model specifying fundamental
architectural concepts. Our taxonomy aims to empower researchers, engineers,
and AI practitioners to systematically analyze the architectural dynamics and
balancing strategies employed by these increasingly prevalent AI systems. The
exploratory taxonomic classification of selected representative LLM-powered
multi-agent systems illustrates its practical utility and reveals potential for
future research and development.


# Strategic Evaluation: Subjects, Evaluators, and Society

[Link to the paper](http://arxiv.org/abs/2310.03655v1)

## Authors
- Benjamin Laufer
- Jon Kleinberg
- Karen Levy
- Helen Nissenbaum

## Summary
  A broad current application of algorithms is in formal and quantitative
measures of murky concepts -- like merit -- to make decisions. When people
strategically respond to these sorts of evaluations in order to gain favorable
decision outcomes, their behavior can be subjected to moral judgments. They may
be described as 'gaming the system' or 'cheating,' or (in other cases)
investing 'honest effort' or 'improving.' Machine learning literature on
strategic behavior has tried to describe these dynamics by emphasizing the
efforts expended by decision subjects hoping to obtain a more favorable
assessment -- some works offer ways to preempt or prevent such manipulations,
some differentiate 'gaming' from 'improvement' behavior, while others aim to
measure the effort burden or disparate effects of classification systems. We
begin from a different starting point: that the design of an evaluation itself
can be understood as furthering goals held by the evaluator which may be
misaligned with broader societal goals. To develop the idea that evaluation
represents a strategic interaction in which both the evaluator and the subject
of their evaluation are operating out of self-interest, we put forward a model
that represents the process of evaluation using three interacting agents: a
decision subject, an evaluator, and society, representing a bundle of values
and oversight mechanisms. We highlight our model's applicability to a number of
social systems where one or two players strategically undermine the others'
interests to advance their own. Treating evaluators as themselves strategic
allows us to re-cast the scrutiny directed at decision subjects, towards the
incentives that underpin institutional designs of evaluations. The moral
standing of strategic behaviors often depend on the moral standing of the
evaluations and incentives that provoke such behaviors.


# DQ-LoRe: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning

[Link to the paper](http://arxiv.org/abs/2310.02954v2)

## Authors
- Jiong Xiong
- Zixuan Li
- Chuanyang Zheng
- Zhijiang Guo
- Yichun Yin
- Enze Xie
- Zhicheng Yang
- Qingxing Cao
- Haiming Wang
- Xiongwei Han
- Jing Tang
- Chengming Li
- Xiaodan Liang

## Summary
  Recent advances in natural language processing, primarily propelled by Large
Language Models (LLMs), have showcased their remarkable capabilities grounded
in in-context learning. A promising avenue for guiding LLMs in intricate
reasoning tasks involves the utilization of intermediate reasoning steps within
the Chain-of-Thought (CoT) paradigm. Nevertheless, the central challenge lies
in the effective selection of exemplars for facilitating in-context learning.
In this study, we introduce a framework that leverages Dual Queries and
Low-rank approximation Re-ranking (DQ-LoRe) to automatically select exemplars
for in-context learning. Dual Queries first query LLM to obtain LLM-generated
knowledge such as CoT, then query the retriever to obtain the final exemplars
via both question and the knowledge. Moreover, for the second query, LoRe
employs dimensionality reduction techniques to refine exemplar selection,
ensuring close alignment with the input question's knowledge. Through extensive
experiments, we demonstrate that DQ-LoRe significantly outperforms prior
state-of-the-art methods in the automatic selection of exemplars for GPT-4,
enhancing performance from 92.5\% to 94.2\%. Our comprehensive analysis further
reveals that DQ-LoRe consistently outperforms retrieval-based approaches in
terms of both performance and adaptability, especially in scenarios
characterized by distribution shifts. DQ-LoRe pushes the boundaries of
in-context learning and opens up new avenues for addressing complex reasoning
challenges. We will release the code soon.


# RedMotion: Motion Prediction via Redundancy Reduction

[Link to the paper](http://arxiv.org/abs/2306.10840v2)

## Authors
- Royden Wagner
- Omer Sahin Tas
- Marvin Klemp
- Carlos Fernandez Lopez

## Summary
  Predicting the future motion of traffic agents is vital for self-driving
vehicles to ensure their safe operation. We introduce RedMotion, a transformer
model for motion prediction that incorporates two types of redundancy
reduction. The first type of redundancy reduction is induced by an internal
transformer decoder and reduces a variable-sized set of road environment
tokens, such as road graphs with agent data, to a fixed-sized embedding. The
second type of redundancy reduction is a self-supervised learning objective and
applies the redundancy reduction principle to embeddings generated from
augmented views of road environments. Our experiments reveal that our
representation learning approach can outperform PreTraM, Traj-MAE, and
GraphDINO in a semi-supervised setting. Our RedMotion model achieves results
that are competitive with those of Scene Transformer or MTR++. We provide an
open source implementation that is accessible via GitHub
(https://github.com/kit-mrt/red-motion) and Colab
(https://colab.research.google.com/drive/1Q-Z9VdiqvfPfctNG8oqzPcgm0lP3y1il).


# Exploring Social Choice Mechanisms for Recommendation Fairness in SCRUF

[Link to the paper](http://arxiv.org/abs/2309.08621v2)

## Authors
- Amanda Aird
- Cassidy All
- Paresha Farastu
- Elena Stefancova
- Joshua Sun
- Nicholas Mattei
- Robin Burke

## Summary
  Fairness problems in recommender systems often have a complexity in practice
that is not adequately captured in simplified research formulations. A social
choice formulation of the fairness problem, operating within a multi-agent
architecture of fairness concerns, offers a flexible and multi-aspect
alternative to fairness-aware recommendation approaches. Leveraging social
choice allows for increased generality and the possibility of tapping into
well-studied social choice algorithms for resolving the tension between
multiple, competing fairness concerns. This paper explores a range of options
for choice mechanisms in multi-aspect fairness applications using both real and
synthetic data and shows that different classes of choice and allocation
mechanisms yield different but consistent fairness / accuracy tradeoffs. We
also show that a multi-agent formulation offers flexibility in adapting to user
population dynamics.


# Explaining Emergent In-Context Learning as Kernel Regression

[Link to the paper](http://arxiv.org/abs/2305.12766v2)

## Authors
- Chi Han
- Ziqi Wang
- Han Zhao
- Heng Ji

## Summary
  Large language models (LLMs) have initiated a paradigm shift in transfer
learning. In contrast to the classic pretraining-then-finetuning procedure, in
order to use LLMs for downstream prediction tasks, one only needs to provide a
few demonstrations, known as in-context examples, without adding more or
updating existing model parameters. This in-context learning (ICL) capability
of LLMs is intriguing, and it is not yet fully understood how pretrained LLMs
acquire such capabilities. In this paper, we investigate the reason why a
transformer-based language model can accomplish in-context learning after
pre-training on a general language corpus by proposing one hypothesis that LLMs
can simulate kernel regression with internal representations when faced with
in-context examples. More concretely, we first prove that Bayesian inference on
in-context prompts can be asymptotically understood as kernel regression $\hat
y = \sum_i y_i K(x, x_i)/\sum_i K(x, x_i)$ as the number of in-context
demonstrations grows. Then, we empirically investigate the in-context behaviors
of language models. We find that during ICL, the attention and hidden features
in LLMs match the behaviors of a kernel regression. Finally, our theory
provides insights into multiple phenomena observed in the ICL field: why
retrieving demonstrative samples similar to test samples can help, why ICL
performance is sensitive to the output formats, and why ICL accuracy benefits
from selecting in-distribution and representative samples.


# High-Degrees-of-Freedom Dynamic Neural Fields for Robot Self-Modeling and Motion Planning

[Link to the paper](http://arxiv.org/abs/2310.03624v1)

## Authors
- Lennart Schulze
- Hod Lipson

## Summary
  A robot self-model is a task-agnostic representation of the robot's physical
morphology that can be used for motion planning tasks in absence of classical
geometric kinematic models. In particular, when the latter are hard to engineer
or the robot's kinematics change unexpectedly, human-free self-modeling is a
necessary feature of truly autonomous agents. In this work, we leverage neural
fields to allow a robot to self-model its kinematics as a neural-implicit query
model learned only from 2D images annotated with camera poses and
configurations. This enables significantly greater applicability than existing
approaches which have been dependent on depth images or geometry knowledge. To
this end, alongside a curricular data sampling strategy, we propose a new
encoder-based neural density field architecture for dynamic object-centric
scenes conditioned on high numbers of degrees of freedom (DOFs). In a 7-DOF
robot test setup, the learned self-model achieves a Chamfer-L2 distance of 2%
of the robot's workspace dimension. We demonstrate the capabilities of this
model on a motion planning task as an exemplary downstream application.


# Adversarial Machine Learning for Social Good: Reframing the Adversary as an Ally

[Link to the paper](http://arxiv.org/abs/2310.03614v1)

## Authors
- Shawqi Al-Maliki
- Adnan Qayyum
- Hassan Ali
- Mohamed Abdallah
- Junaid Qadir
- Dinh Thai Hoang
- Dusit Niyato
- Ala Al-Fuqaha

## Summary
  Deep Neural Networks (DNNs) have been the driving force behind many of the
recent advances in machine learning. However, research has shown that DNNs are
vulnerable to adversarial examples -- input samples that have been perturbed to
force DNN-based models to make errors. As a result, Adversarial Machine
Learning (AdvML) has gained a lot of attention, and researchers have
investigated these vulnerabilities in various settings and modalities. In
addition, DNNs have also been found to incorporate embedded bias and often
produce unexplainable predictions, which can result in anti-social AI
applications. The emergence of new AI technologies that leverage Large Language
Models (LLMs), such as ChatGPT and GPT-4, increases the risk of producing
anti-social applications at scale. AdvML for Social Good (AdvML4G) is an
emerging field that repurposes the AdvML bug to invent pro-social applications.
Regulators, practitioners, and researchers should collaborate to encourage the
development of pro-social applications and hinder the development of
anti-social ones. In this work, we provide the first comprehensive review of
the emerging field of AdvML4G. This paper encompasses a taxonomy that
highlights the emergence of AdvML4G, a discussion of the differences and
similarities between AdvML4G and AdvML, a taxonomy covering social good-related
concepts and aspects, an exploration of the motivations behind the emergence of
AdvML4G at the intersection of ML4G and AdvML, and an extensive summary of the
works that utilize AdvML4G as an auxiliary tool for innovating pro-social
applications. Finally, we elaborate upon various challenges and open research
issues that require significant attention from the research community.


# UniAudio: An Audio Foundation Model Toward Universal Audio Generation

[Link to the paper](http://arxiv.org/abs/2310.00704v2)

## Authors
- Dongchao Yang
- Jinchuan Tian
- Xu Tan
- Rongjie Huang
- Songxiang Liu
- Xuankai Chang
- Jiatong Shi
- Sheng Zhao
- Jiang Bian
- Xixin Wu
- Zhou Zhao
- Helen Meng

## Summary
  Large Language models (LLM) have demonstrated the capability to handle a
variety of generative tasks. This paper presents the UniAudio system, which,
unlike prior task-specific approaches, leverages LLM techniques to generate
multiple types of audio (including speech, sounds, music, and singing) with
given input conditions. UniAudio 1) first tokenizes all types of target audio
along with other condition modalities, 2) concatenates source-target pair as a
single sequence, and 3) performs next-token prediction using LLM. Also, a
multi-scale Transformer model is proposed to handle the overly long sequences
caused by the residual vector quantization based neural codec in tokenization.
Training of UniAudio is scaled up to 165K hours of audio and 1B parameters,
based on all generative tasks, aiming to obtain sufficient prior knowledge not
only in the intrinsic properties of audio but also the inter-relationship
between audio and other modalities. Therefore, the trained UniAudio model has
the potential to become a foundation model for universal audio generation: it
shows strong capability in all trained tasks and can seamlessly support new
audio generation tasks after simple fine-tuning. Experiments demonstrate that
UniAudio achieves state-of-the-art or at least competitive results on most of
the 11 tasks. Demo and code are released at
https://github.com/yangdongchao/UniAudio


# OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation

[Link to the paper](http://arxiv.org/abs/2309.00616v3)

## Authors
- Zhening Huang
- Xiaoyang Wu
- Xi Chen
- Hengshuang Zhao
- Lei Zhu
- Joan Lasenby

## Summary
  Current 3D open-vocabulary scene understanding methods mostly utilize
well-aligned 2D images as the bridge to learn 3D features with language.
However, applying these approaches becomes challenging in scenarios where 2D
images are absent. In this work, we introduce a new pipeline, namely,
OpenIns3D, which requires no 2D image inputs, for 3D open-vocabulary scene
understanding at the instance level. The OpenIns3D framework employs a
"Mask-Snap-Lookup" scheme. The "Mask" module learns class-agnostic mask
proposals in 3D point clouds. The "Snap" module generates synthetic scene-level
images at multiple scales and leverages 2D vision language models to extract
interesting objects. The "Lookup" module searches through the outcomes of
"Snap" with the help of Mask2Pixel maps, which contain the precise
correspondence between 3D masks and synthetic images, to assign category names
to the proposed masks. This 2D input-free and flexible approach achieves
state-of-the-art results on a wide range of indoor and outdoor datasets by a
large margin. Moreover, OpenIns3D allows for effortless switching of 2D
detectors without re-training. When integrated with powerful 2D open-world
models such as ODISE and GroundingDINO, excellent results were observed on
open-vocabulary instance segmentation. When integrated with LLM-powered 2D
models like LISA, it demonstrates a remarkable capacity to process highly
complex text queries which require intricate reasoning and world knowledge.
Project page: https://zheninghuang.github.io/OpenIns3D/


# Redefining Digital Health Interfaces with Large Language Models

[Link to the paper](http://arxiv.org/abs/2310.03560v1)

## Authors
- Fergus Imrie
- Paulius Rauba
- Mihaela van der Schaar

## Summary
  Digital health tools have the potential to significantly improve the delivery
of healthcare services. However, their use remains comparatively limited due,
in part, to challenges surrounding usability and trust. Recently, Large
Language Models (LLMs) have emerged as general-purpose models with the ability
to process complex information and produce human-quality text, presenting a
wealth of potential applications in healthcare. Directly applying LLMs in
clinical settings is not straightforward, with LLMs susceptible to providing
inconsistent or nonsensical answers. We demonstrate how LLMs can utilize
external tools to provide a novel interface between clinicians and digital
technologies. This enhances the utility and practical impact of digital
healthcare tools and AI models while addressing current issues with using LLM
in clinical settings such as hallucinations. We illustrate our approach with
examples from cardiovascular disease and diabetes risk prediction, highlighting
the benefit compared to traditional interfaces for digital tools.


# Large Language Models for Software Engineering: Survey and Open Problems

[Link to the paper](http://arxiv.org/abs/2310.03533v1)

## Authors
- Angela Fan
- Beliz Gokkaya
- Mark Harman
- Mitya Lyubarskiy
- Shubho Sengupta
- Shin Yoo
- Jie M. Zhang

## Summary
  This paper provides a survey of the emerging area of Large Language Models
(LLMs) for Software Engineering (SE). It also sets out open research challenges
for the application of LLMs to technical problems faced by software engineers.
LLMs' emergent properties bring novelty and creativity with applications right
across the spectrum of Software Engineering activities including coding,
design, requirements, repair, refactoring, performance improvement,
documentation and analytics. However, these very same emergent properties also
pose significant technical challenges; we need techniques that can reliably
weed out incorrect solutions, such as hallucinations. Our survey reveals the
pivotal role that hybrid techniques (traditional SE plus LLMs) have to play in
the development and deployment of reliable, efficient and effective LLM-based
SE.


# Best-Response Dynamics in Tullock Contests with Convex Costs

[Link to the paper](http://arxiv.org/abs/2310.03528v1)

## Authors
- Abheek Ghosh

## Summary
  We study the convergence of best-response dynamics in Tullock contests with
convex cost functions (these games always have a unique pure-strategy Nash
equilibrium). We show that best-response dynamics rapidly converges to the
equilibrium for homogeneous agents. For two homogeneous agents, we show
convergence to an $\epsilon$-approximate equilibrium in
$\Theta(\log\log(1/\epsilon))$ steps. For $n \ge 3$ agents, the dynamics is not
unique because at each step $n-1 \ge 2$ agents can make non-trivial moves. We
consider the model proposed by \cite{ghosh2023best}, where the agent making the
move is randomly selected at each time step. We show convergence to an
$\epsilon$-approximate equilibrium in $O(\beta \log(n/(\epsilon\delta)))$ steps
with probability $1-\delta$, where $\beta$ is a parameter of the agent
selection process, e.g., $\beta = n^2 \log(n)$ if agents are selected uniformly
at random at each time step. We complement this result with a lower bound of
$\Omega(n + \log(1/\epsilon)/\log(n))$ applicable for any agent selection
process.


# In ChatGPT We Trust? Measuring and Characterizing the Reliability of ChatGPT

[Link to the paper](http://arxiv.org/abs/2304.08979v2)

## Authors
- Xinyue Shen
- Zeyuan Chen
- Michael Backes
- Yang Zhang

## Summary
  The way users acquire information is undergoing a paradigm shift with the
advent of ChatGPT. Unlike conventional search engines, ChatGPT retrieves
knowledge from the model itself and generates answers for users. ChatGPT's
impressive question-answering (QA) capability has attracted more than 100
million users within a short period of time but has also raised concerns
regarding its reliability. In this paper, we perform the first large-scale
measurement of ChatGPT's reliability in the generic QA scenario with a
carefully curated set of 5,695 questions across ten datasets and eight domains.
We find that ChatGPT's reliability varies across different domains, especially
underperforming in law and science questions. We also demonstrate that system
roles, originally designed by OpenAI to allow users to steer ChatGPT's
behavior, can impact ChatGPT's reliability in an imperceptible way. We further
show that ChatGPT is vulnerable to adversarial examples, and even a single
character change can negatively affect its reliability in certain cases. We
believe that our study provides valuable insights into ChatGPT's reliability
and underscores the need for strengthening the reliability and security of
large language models (LLMs).


# SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning

[Link to the paper](http://arxiv.org/abs/2308.00436v3)

## Authors
- Ning Miao
- Yee Whye Teh
- Tom Rainforth

## Summary
  The recent progress in large language models (LLMs), especially the invention
of chain-of-thought prompting, has made it possible to automatically answer
questions by stepwise reasoning. However, when faced with more complicated
problems that require non-linear thinking, even the strongest LLMs make
mistakes. To address this, we explore whether LLMs are able to recognize errors
in their own step-by-step reasoning, without resorting to external resources.
To this end, we propose SelfCheck, a general-purpose zero-shot verification
schema for recognizing such errors. We then use the results of these checks to
improve question-answering performance by conducting weighted voting on
multiple solutions to the question. We test SelfCheck on three datasets (GSM8K,
MathQA, and MATH) and find that it successfully recognizes errors and, in turn,
increases final answer accuracies.


# Solidarity to achieve stability

[Link to the paper](http://arxiv.org/abs/2302.07618v2)

## Authors
- Jorge Alcalde-Unzu
- Oihane Gallo
- Elena Inarra
- Juan D. Moreno-Ternero

## Summary
  Agents may form coalitions. Each coalition shares its endowment among its
agents by applying a sharing rule. The sharing rule induces a coalition
formation problem by assuming that agents rank coalitions according to the
allocation they obtain in the corresponding sharing problem. We characterize
the sharing rules that induce a class of stable coalition formation problems as
those that satisfy a natural axiom that formalizes the principle of solidarity.
Thus, solidarity becomes a sufficient condition to achieve stability.


# Using Large Language Models for Qualitative Analysis can Introduce Serious Bias

[Link to the paper](http://arxiv.org/abs/2309.17147v2)

## Authors
- Julian Ashwin
- Aditya Chhabra
- Vijayendra Rao

## Summary
  Large Language Models (LLMs) are quickly becoming ubiquitous, but the
implications for social science research are not yet well understood. This
paper asks whether LLMs can help us analyse large-N qualitative data from
open-ended interviews, with an application to transcripts of interviews with
Rohingya refugees in Cox's Bazaar, Bangladesh. We find that a great deal of
caution is needed in using LLMs to annotate text as there is a risk of
introducing biases that can lead to misleading inferences. We here mean bias in
the technical sense, that the errors that LLMs make in annotating interview
transcripts are not random with respect to the characteristics of the interview
subjects. Training simpler supervised models on high-quality human annotations
with flexible coding leads to less measurement error and bias than LLM
annotations. Therefore, given that some high quality annotations are necessary
in order to asses whether an LLM introduces bias, we argue that it is probably
preferable to train a bespoke model on these annotations than it is to use an
LLM for annotation.


# How the level sampling process impacts zero-shot generalisation in deep reinforcement learning

[Link to the paper](http://arxiv.org/abs/2310.03494v1)

## Authors
- Samuel Garcin
- James Doran
- Shangmin Guo
- Christopher G. Lucas
- Stefano V. Albrecht

## Summary
  A key limitation preventing the wider adoption of autonomous agents trained
via deep reinforcement learning (RL) is their limited ability to generalise to
new environments, even when these share similar characteristics with
environments encountered during training. In this work, we investigate how a
non-uniform sampling strategy of individual environment instances, or levels,
affects the zero-shot generalisation (ZSG) ability of RL agents, considering
two failure modes: overfitting and over-generalisation. As a first step, we
measure the mutual information (MI) between the agent's internal representation
and the set of training levels, which we find to be well-correlated to instance
overfitting. In contrast to uniform sampling, adaptive sampling strategies
prioritising levels based on their value loss are more effective at maintaining
lower MI, which provides a novel theoretical justification for this class of
techniques. We then turn our attention to unsupervised environment design (UED)
methods, which adaptively generate new training levels and minimise MI more
effectively than methods sampling from a fixed set. However, we find UED
methods significantly shift the training distribution, resulting in
over-generalisation and worse ZSG performance over the distribution of
interest. To prevent both instance overfitting and over-generalisation, we
introduce self-supervised environment design (SSED). SSED generates levels
using a variational autoencoder, effectively reducing MI while minimising the
shift with the distribution of interest, and leads to statistically significant
improvements in ZSG over fixed-set level sampling strategies and UED methods.


# Graph-based Simultaneous Localization and Bias Tracking

[Link to the paper](http://arxiv.org/abs/2310.02814v2)

## Authors
- Alexander Venus
- Erik Leitinger
- Stefan Tertinek
- Klaus Witrisal

## Summary
  We present a factor graph formulation and particle-based sum-product
algorithm for robust localization and tracking in multipath-prone environments.
The proposed sequential algorithm jointly estimates the mobile agent's position
together with a time-varying number of multipath components (MPCs). The MPCs
are represented by "delay biases" corresponding to the offset between
line-of-sight (LOS) component delay and the respective delays of all detectable
MPCs. The delay biases of the MPCs capture the geometric features of the
propagation environment with respect to the mobile agent. Therefore, they can
provide position-related information contained in the MPCs without explicitly
building a map of the environment. We demonstrate that the position-related
information enables the algorithm to provide high-accuracy position estimates
even in fully obstructed line-of-sight (OLOS) situations. Using simulated and
real measurements in different scenarios we demonstrate the proposed algorithm
to significantly outperform state-of-the-art multipath-aided tracking
algorithms and show that the performance of our algorithm constantly attains
the posterior Cramer-Rao lower bound (P-CRLB). Furthermore, we demonstrate the
implicit capability of the proposed method to identify unreliable measurements
and, thus, to mitigate lost tracks.


# Fair Division with Allocator's Preference

[Link to the paper](http://arxiv.org/abs/2310.03475v1)

## Authors
- Xiaolin Bu
- Zihao Li
- Shengxin Liu
- Jiaxin Song
- Biaoshuai Tao

## Summary
  We consider the fair allocation problem of indivisible items. Most previous
work focuses on fairness and/or efficiency among agents given agents'
preferences. However, besides the agents, the allocator as the resource owner
may also be involved in many real-world scenarios, e.g., heritage division. The
allocator has the inclination to obtain a fair or efficient allocation based on
her own preference over the items and to whom each item is allocated. In this
paper, we propose a new model and focus on the following two problems: 1) Is it
possible to find an allocation that is fair for both the agents and the
allocator? 2) What is the complexity of maximizing the allocator's social
welfare while satisfying the agents' fairness?
  We consider the two fundamental fairness criteria: envy-freeness and
proportionality. For the first problem, we study the existence of an allocation
that is envy-free up to $c$ goods (EF-$c$) or proportional up to $c$ goods
(PROP-$c$) from both the agents' and the allocator's perspectives, in which
such an allocation is called doubly EF-$c$ or doubly PROP-$c$ respectively.
When the allocator's utility depends exclusively on the items (but not to whom
an item is allocated), we prove that a doubly EF-$1$ allocation always exists.
For the general setting where the allocator has a preference over the items and
to whom each item is allocated, we prove that a doubly EF-$1$ allocation always
exists for two agents, a doubly PROP-$2$ allocation always exists for binary
valuations, and a doubly PROP-$O(\log n)$ allocation always exists in general.
  For the second problem, we provide various (in)approximability results in
which the gaps between approximation and inapproximation ratios are
asymptotically closed under most settings.
  Most results are based on novel technical tools including the chromatic
numbers of the Kneser graphs and linear programming-based analysis.


# Controllable Multi-document Summarization: Coverage & Coherence Intuitive Policy with Large Language Model Based Rewards

[Link to the paper](http://arxiv.org/abs/2310.03473v1)

## Authors
- Litton J Kurisinkel
- Nancy F chen

## Summary
  Memory-efficient large language models are good at refining text input for
better readability. However, controllability is a matter of concern when it
comes to text generation tasks with long inputs, such as multi-document
summarization. In this work, we investigate for a generic controllable approach
for multi-document summarization that leverages the capabilities of LLMs to
refine the text. In particular, we train a controllable content extraction
scheme to extract the text that will be refined by an LLM. The scheme is
designed with a novel coverage and coherence intuitive policy, which is duly
rewarded by a passively trained LLM. Our approach yields competitive results in
the evaluation using ROUGE metrics and outperforms potential baselines in
coherence, as per human evaluation.


# AI Alignment Dialogues: An Interactive Approach to AI Alignment in Support Agents

[Link to the paper](http://arxiv.org/abs/2301.06421v2)

## Authors
- Pei-Yu Chen
- Myrthe L. Tielman
- Dirk K. J. Heylen
- Catholijn M. Jonker
- M. Birna van Riemsdijk

## Summary
  AI alignment is about ensuring AI systems only pursue goals and activities
that are beneficial to humans. Most of the current approach to AI alignment is
to learn what humans value from their behavioural data. This paper proposes a
different way of looking at the notion of alignment, namely by introducing AI
Alignment Dialogues: dialogues with which users and agents try to achieve and
maintain alignment via interaction. We argue that alignment dialogues have a
number of advantages in comparison to data-driven approaches, especially for
behaviour support agents, which aim to support users in achieving their desired
future behaviours rather than their current behaviours. The advantages of
alignment dialogues include allowing the users to directly convey higher-level
concepts to the agent, and making the agent more transparent and trustworthy.
In this paper we outline the concept and high-level structure of alignment
dialogues. Moreover, we conducted a qualitative focus group user study from
which we developed a model that describes how alignment dialogues affect users,
and created design suggestions for AI alignment dialogues. Through this we
establish foundations for AI alignment dialogues and shed light on what
requires further development and research.


# A Comprehensive Overview of Large Language Models

[Link to the paper](http://arxiv.org/abs/2307.06435v4)

## Authors
- Humza Naveed
- Asad Ullah Khan
- Shi Qiu
- Muhammad Saqib
- Saeed Anwar
- Muhammad Usman
- Naveed Akhtar
- Nick Barnes
- Ajmal Mian

## Summary
  Large Language Models (LLMs) have recently demonstrated remarkable
capabilities in natural language processing tasks and beyond. This success of
LLMs has led to a large influx of research contributions in this direction.
These works encompass diverse topics such as architectural innovations of the
underlying neural networks, context length improvements, model alignment,
training datasets, benchmarking, efficiency and more. With the rapid
development of techniques and regular breakthroughs in LLM research, it has
become considerably challenging to perceive the bigger picture of the advances
in this direction. Considering the rapidly emerging plethora of literature on
LLMs, it is imperative that the research community is able to benefit from a
concise yet comprehensive overview of the recent developments in this field.
This article provides that overview to the research community. It not only
focuses on a systematic treatment of the existing literature on a broad range
of LLM related concept, but also pays special attention to providing
comprehensive summaries with extensive details about the individual existing
models, datasets and major insights. We also pay heed to aligning our overview
with the emerging outlook of this research direction by accounting for the
other recently materializing reviews of the broader research direction of LLMs.
Our self-contained comprehensive overview of LLMs discusses relevant background
concepts along with covering the advanced topics at the frontier of this
research direction. This review article is intended to not only provide a
systematic survey, but also a quick comprehensive reference for the researchers
and practitioners to draw insights from extensive informative summaries of the
existing works to advance the LLM research direction.


# LLM Based Multi-Document Summarization Exploiting Main-Event Biased Monotone Submodular Content Extraction

[Link to the paper](http://arxiv.org/abs/2310.03414v1)

## Authors
- Litton J Kurisinkel
- Nancy F. Chen

## Summary
  Multi-document summarization is a challenging task due to its inherent
subjective bias, highlighted by the low inter-annotator ROUGE-1 score of 0.4
among DUC-2004 reference summaries. In this work, we aim to enhance the
objectivity of news summarization by focusing on the main event of a group of
related news documents and presenting it coherently with sufficient context.
Our primary objective is to succinctly report the main event, ensuring that the
summary remains objective and informative. To achieve this, we employ an
extract-rewrite approach that incorporates a main-event biased
monotone-submodular function for content selection. This enables us to extract
the most crucial information related to the main event from the document
cluster. To ensure coherence, we utilize a fine-tuned Language Model (LLM) for
rewriting the extracted content into a coherent text. The evaluation using
objective metrics and human evaluators confirms the effectiveness of our
approach, as it surpasses potential baselines, demonstrating excellence in both
content coverage, coherence, and informativeness.


# Towards practical reinforcement learning for tokamak magnetic control

[Link to the paper](http://arxiv.org/abs/2307.11546v2)

## Authors
- Brendan D. Tracey
- Andrea Michi
- Yuri Chervonyi
- Ian Davies
- Cosmin Paduraru
- Nevena Lazic
- Federico Felici
- Timo Ewalds
- Craig Donner
- Cristian Galperti
- Jonas Buchli
- Michael Neunert
- Andrea Huber
- Jonathan Evens
- Paula Kurylowicz
- Daniel J. Mankowitz
- Martin Riedmiller
- The TCV Team

## Summary
  Reinforcement learning (RL) has shown promising results for real-time control
systems, including the domain of plasma magnetic control. However, there are
still significant drawbacks compared to traditional feedback control approaches
for magnetic confinement. In this work, we address key drawbacks of the RL
method; achieving higher control accuracy for desired plasma properties,
reducing the steady-state error, and decreasing the required time to learn new
tasks. We build on top of \cite{degrave2022magnetic}, and present algorithmic
improvements to the agent architecture and training procedure. We present
simulation results that show up to 65\% improvement in shape accuracy, achieve
substantial reduction in the long-term bias of the plasma current, and
additionally reduce the training time required to learn new tasks by a factor
of 3 or more. We present new experiments using the upgraded RL-based
controllers on the TCV tokamak, which validate the simulation results achieved,
and point the way towards routinely achieving accurate discharges using the RL
approach.


# Over-the-Air Federated Learning with Compressed Sensing: Is Sparsification Necessary?

[Link to the paper](http://arxiv.org/abs/2310.03410v1)

## Authors
- Adrian Edin
- Zheng Chen

## Summary
  Over-the-Air (OtA) Federated Learning (FL) refers to an FL system where
multiple agents apply OtA computation for transmitting model updates to a
common edge server. Two important features of OtA computation, namely linear
processing and signal-level superposition, motivate the use of linear
compression with compressed sensing (CS) methods to reduce the number of data
samples transmitted over the channel. The previous works on applying CS methods
in OtA FL have primarily assumed that the original model update vectors are
sparse, or they have been sparsified before compression. However, it is unclear
whether linear compression with CS-based reconstruction is more effective than
directly sending the non-zero elements in the sparsified update vectors, under
the same total power constraint. In this study, we examine and compare several
communication designs with or without sparsification. Our findings demonstrate
that sparsification before compression is not necessary. Alternatively,
sparsification without linear compression can also achieve better performance
than the commonly considered setup that combines both.


# Likelihood-Based Methods Improve Parameter Estimation in Opinion Dynamics Models

[Link to the paper](http://arxiv.org/abs/2310.02766v2)

## Authors
- Jacopo Lenti
- Corrado Monti
- Gianmarco De Francisci Morales

## Summary
  We show that a maximum likelihood approach for parameter estimation in
agent-based models (ABMs) of opinion dynamics outperforms the typical
simulation-based approach. Simulation-based approaches simulate the model
repeatedly in search of a set of parameters that generates data similar enough
to the observed one. In contrast, likelihood-based approaches derive a
likelihood function that connects the unknown parameters to the observed data
in a statistically principled way. We compare these two approaches on the
well-known bounded-confidence model of opinion dynamics. We do so on three
realistic scenarios of increasing complexity depending on data availability:
(i) fully observed opinions and interactions, (ii) partially observed
interactions, (iii) observed interactions with noisy proxies of the opinions.
We highlight how identifying observed and latent variables is fundamental for
connecting the model to the data. To realize the likelihood-based approach, we
first cast the model into a probabilistic generative guise that supports a
proper data likelihood. Then, we describe the three scenarios via probabilistic
graphical models and show the nuances that go into translating the model.
Finally, we implement the resulting probabilistic models in an automatic
differentiation framework (PyTorch). This step enables easy and efficient
maximum likelihood estimation via gradient descent. Our experimental results
show that the maximum likelihood estimates are up to 4x more accurate and
require up to 200x less computational time.


# Teaching Large Language Models to Self-Debug

[Link to the paper](http://arxiv.org/abs/2304.05128v2)

## Authors
- Xinyun Chen
- Maxwell Lin
- Nathanael SchÃ¤rli
- Denny Zhou

## Summary
  Large language models (LLMs) have achieved impressive performance on code
generation. However, for complex programming tasks, generating the correct
solution in one go becomes challenging, thus some prior works have designed
program repair approaches to improve code generation performance. In this work,
we propose Self-Debugging, which teaches a large language model to debug its
predicted program via few-shot demonstrations. In particular, we demonstrate
that Self-Debugging can teach the large language model to perform rubber duck
debugging; i.e., without any human feedback on the code correctness or error
messages, the model is able to identify its mistakes by investigating the
execution results and explaining the generated code in natural language.
Self-Debugging achieves the state-of-the-art performance on several code
generation benchmarks, including the Spider dataset for text-to-SQL generation,
TransCoder for C++-to-Python translation, and MBPP for text-to-Python
generation. On the Spider benchmark where there are no unit tests to verify the
correctness of predictions, Self-Debugging with code explanation consistently
improves the baseline by 2-3%, and improves the prediction accuracy on problems
of the hardest level by 9%. On TransCoder and MBPP where unit tests are
available, Self-Debugging improves the baseline accuracy by up to 12%.
Meanwhile, by leveraging feedback messages and reusing failed predictions,
Self-Debugging notably improves sample efficiency, and can match or outperform
baseline models that generate more than 10x candidate programs.


# Adapting Large Language Models for Content Moderation: Pitfalls in Data Engineering and Supervised Fine-tuning

[Link to the paper](http://arxiv.org/abs/2310.03400v1)

## Authors
- Huan Ma
- Changqing Zhang
- Huazhu Fu
- Peilin Zhao
- Bingzhe Wu

## Summary
  Nowadays, billions of people engage in communication and express their
opinions on the internet daily. Unfortunately, not all of these expressions are
friendly or compliant, making content moderation an indispensable task. With
the successful development of Large Language Models (LLMs) in recent years,
LLM-based methods have become a feasible solution for handling tasks in various
domains. However, in the field of content moderation, there is still a lack of
detailed work that systematically introduces implementation details. In this
paper, we introduce how to fine-tune an LLM model that can be privately
deployed for content moderation. Specifically, we discuss whether incorporating
reasons during the fine-tuning process would be better or if it should be
treated as a classification task directly. We also explore the benefits of
utilizing reasons generated by more powerful LLMs for fine-tuning privately
deployed models and the impact of different processing approaches when the
answers generated by the more powerful LLMs are incorrect. We report the entire
research process and the key findings in this paper, hoping to provide valuable
experience for researchers who are fine-tuning privately deployed models in
their domain-specific research.


# Prolonged Learning and Hasty Stopping: the Wald Problem with Ambiguity

[Link to the paper](http://arxiv.org/abs/2208.14121v3)

## Authors
- Sarah Auster
- Yeon-Koo Che
- Konrad Mierendorff

## Summary
  This paper studies sequential information acquisition by an ambiguity-averse
decision maker (DM), who decides how long to collect information before taking
an irreversible action. The agent optimizes against the worst-case belief and
updates prior by prior. We show that the consideration of ambiguity gives rise
to rich dynamics: compared to the Bayesian DM, the DM here tends to experiment
excessively when facing modest uncertainty and, to counteract it, may stop
experimenting prematurely when facing high uncertainty. In the latter case, the
DM's stopping rule is non-monotonic in beliefs and features randomized
stopping.


# Machine learning the interaction network in coupled dynamical systems

[Link to the paper](http://arxiv.org/abs/2310.03378v1)

## Authors
- Pawan R. Bhure
- M. S. Santhanam

## Summary
  The study of interacting dynamical systems continues to attract research
interest in various fields of science and engineering. In a collection of
interacting particles, the interaction network contains information about how
various components interact with one another. Inferring the information about
the interaction network from the dynamics of agents is a problem of
long-standing interest. In this work, we employ a self-supervised neural
network model to achieve two outcomes: to recover the interaction network and
to predict the dynamics of individual agents. Both these information are
inferred solely from the observed trajectory data. This work presents an
application of the Neural Relational Inference model to two dynamical systems:
coupled particles mediated by Hooke's law interaction and coupled phase
(Kuramoto) oscillators.


# Procedural Text Mining with Large Language Models

[Link to the paper](http://arxiv.org/abs/2310.03376v1)

## Authors
- Anisa Rula
- Jennifer D'Souza

## Summary
  Recent advancements in the field of Natural Language Processing, particularly
the development of large-scale language models that are pretrained on vast
amounts of knowledge, are creating novel opportunities within the realm of
Knowledge Engineering. In this paper, we investigate the usage of large
language models (LLMs) in both zero-shot and in-context learning settings to
tackle the problem of extracting procedures from unstructured PDF text in an
incremental question-answering fashion. In particular, we leverage the current
state-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model,
accompanied by two variations of in-context learning that involve an ontology
with definitions of procedures and steps and a limited number of samples of
few-shot learning. The findings highlight both the promise of this approach and
the value of the in-context learning customisations. These modifications have
the potential to significantly address the challenge of obtaining sufficient
training data, a hurdle often encountered in deep learning-based Natural
Language Processing techniques for procedure extraction.


# DyVal: Graph-informed Dynamic Evaluation of Large Language Models

[Link to the paper](http://arxiv.org/abs/2309.17167v2)

## Authors
- Kaijie Zhu
- Jiaao Chen
- Jindong Wang
- Neil Zhenqiang Gong
- Diyi Yang
- Xing Xie

## Summary
  Large language models (LLMs) have achieved remarkable performance in various
evaluation benchmarks. However, concerns about their performance are raised on
potential data contamination in their considerable volume of training corpus.
Moreover, the static nature and fixed complexity of current benchmarks may
inadequately gauge the advancing capabilities of LLMs. In this paper, we
introduce DyVal, a novel, general, and flexible evaluation protocol for dynamic
evaluation of LLMs. Based on our proposed dynamic evaluation framework, we
build graph-informed DyVal by leveraging the structural advantage of directed
acyclic graphs to dynamically generate evaluation samples with controllable
complexities. DyVal generates challenging evaluation sets on reasoning tasks
including mathematics, logical reasoning, and algorithm problems. We evaluate
various LLMs ranging from Flan-T5-large to ChatGPT and GPT4. Experiments
demonstrate that LLMs perform worse in DyVal-generated evaluation samples with
different complexities, emphasizing the significance of dynamic evaluation. We
also analyze the failure cases and results of different prompting methods.
Moreover, DyVal-generated samples are not only evaluation sets, but also
helpful data for fine-tuning to improve the performance of LLMs on existing
benchmarks. We hope that DyVal can shed light on the future evaluation research
of LLMs.


# Fictitious Cross-Play: Learning Global Nash Equilibrium in Mixed Cooperative-Competitive Games

[Link to the paper](http://arxiv.org/abs/2310.03354v1)

## Authors
- Zelai Xu
- Yancheng Liang
- Chao Yu
- Yu Wang
- Yi Wu

## Summary
  Self-play (SP) is a popular multi-agent reinforcement learning (MARL)
framework for solving competitive games, where each agent optimizes policy by
treating others as part of the environment. Despite the empirical successes,
the theoretical properties of SP-based methods are limited to two-player
zero-sum games. However, for mixed cooperative-competitive games where agents
on the same team need to cooperate with each other, we can show a simple
counter-example where SP-based methods cannot converge to a global Nash
equilibrium (NE) with high probability. Alternatively, Policy-Space Response
Oracles (PSRO) is an iterative framework for learning NE, where the best
responses w.r.t. previous policies are learned in each iteration. PSRO can be
directly extended to mixed cooperative-competitive settings by jointly learning
team best responses with all convergence properties unchanged. However, PSRO
requires repeatedly training joint policies from scratch till convergence,
which makes it hard to scale to complex games. In this work, we develop a novel
algorithm, Fictitious Cross-Play (FXP), which inherits the benefits from both
frameworks. FXP simultaneously trains an SP-based main policy and a counter
population of best response policies. The main policy is trained by fictitious
self-play and cross-play against the counter population, while the counter
policies are trained as the best responses to the main policy's past versions.
We validate our method in matrix games and show that FXP converges to global
NEs while SP methods fail. We also conduct experiments in a gridworld domain,
where FXP achieves higher Elo ratings and lower exploitabilities than
baselines, and a more challenging football game, where FXP defeats SOTA models
with over 94% win rate.


# LESSON: Learning to Integrate Exploration Strategies for Reinforcement Learning via an Option Framework

[Link to the paper](http://arxiv.org/abs/2310.03342v1)

## Authors
- Woojun Kim
- Jeonghye Kim
- Youngchul Sung

## Summary
  In this paper, a unified framework for exploration in reinforcement learning
(RL) is proposed based on an option-critic model. The proposed framework learns
to integrate a set of diverse exploration strategies so that the agent can
adaptively select the most effective exploration strategy over time to realize
a relevant exploration-exploitation trade-off for each given task. The
effectiveness of the proposed exploration framework is demonstrated by various
experiments in the MiniGrid and Atari environments.


# Fine-tune Language Models to Approximate Unbiased In-context Learning

[Link to the paper](http://arxiv.org/abs/2310.03331v1)

## Authors
- Timothy Chu
- Zhao Song
- Chiwun Yang

## Summary
  In-context learning (ICL) is an astonishing emergent ability of large
language models (LLMs). By presenting a prompt that includes multiple
input-output pairs as examples and introducing a new query input, models can
generate the corresponding output. However, the performance of models heavily
relies on the quality of the input prompt when implementing in-context
learning. Biased or imbalanced input prompts can significantly degrade the
performance of language models. To address this issue, we introduce a
reweighted algorithm called RICL (Reweighted In-context Learning). This
algorithm fine-tunes language models using an unbiased validation set to
determine the optimal weight for each input-output example to approximate
unbiased in-context learning. Furthermore, we also introduce a low-cost
reweighted algorithm, a linear optimal weight approximation algorithm called
LARICL (Linear Approximation of Reweighted In-context Learning). This algorithm
requires minimal training cost while providing effective results. We prove the
convergence of our algorithm and validate its performance through experiments
conducted on a numerical dataset. The experimental findings reveal a
substantial improvement in comparison to benchmarks including the performance
of casual prompt-based in-context learning and the performance of a classic
fine-tuning method.


# Reformulating Domain Adaptation of Large Language Models as Adapt-Retrieve-Revise

[Link to the paper](http://arxiv.org/abs/2310.03328v1)

## Authors
- Zhen wan
- Yating Zhang
- Yexiang Wang
- Fei Cheng
- Sadao Kurohashi

## Summary
  While large language models (LLMs) like GPT-4 have recently demonstrated
astonishing zero-shot capabilities in general domain tasks, they often generate
content with hallucinations in specific domains such as Chinese law, hindering
their application in these areas. This is typically due to the absence of
training data that encompasses such a specific domain, preventing GPT-4 from
acquiring in-domain knowledge. A pressing challenge is that it's not plausible
to continue training LLMs of such scale on in-domain data.
  This paper introduces a simple and effective domain adaptation framework for
GPT-4 by reformulating generation as an \textbf{adapt-retrieve-revise} process.
The initial step is to \textbf{adapt} an affordable 7B LLM to the target domain
by continuing learning on in-domain data. When solving a task, we leverage the
adapted LLM to generate a draft answer given a task query. Then, the draft
answer will be used to \textbf{retrieve} supporting evidence candidates from an
external in-domain knowledge base. Finally, the draft answer and retrieved
evidence are concatenated into a whole prompt to let GPT-4 assess the evidence
and \textbf{revise} the draft answer to generate the final answer.
  Our proposal combines the advantages of the efficiency of adapting a smaller
7B model with the evidence-assessing capability of GPT-4 and effectively
prevents GPT-4 from generating hallucinatory content. In the zero-shot setting
of four Chinese legal tasks, our method improves accuracy by 33.3\% compared to
the direct generation by GPT-4. When compared to two stronger retrieval-based
baselines, our method outperforms them by 15.4\% and 23.9\%. Our code will be
released


# Learning Concept-Based Visual Causal Transition and Symbolic Reasoning for Visual Planning

[Link to the paper](http://arxiv.org/abs/2310.03325v1)

## Authors
- Yilue Qian
- Peiyu Yu
- Ying Nian Wu
- Wei Wang
- Lifeng Fan

## Summary
  Visual planning simulates how humans make decisions to achieve desired goals
in the form of searching for visual causal transitions between an initial
visual state and a final visual goal state. It has become increasingly
important in egocentric vision with its advantages in guiding agents to perform
daily tasks in complex environments. In this paper, we propose an interpretable
and generalizable visual planning framework consisting of i) a novel
Substitution-based Concept Learner (SCL) that abstracts visual inputs into
disentangled concept representations, ii) symbol abstraction and reasoning that
performs task planning via the self-learned symbols, and iii) a Visual Causal
Transition model (ViCT) that grounds visual causal transitions to semantically
similar real-world actions. Given an initial state, we perform goal-conditioned
visual planning with a symbolic reasoning method fueled by the learned
representations and causal transitions to reach the goal state. To verify the
effectiveness of the proposed model, we collect a large-scale visual planning
dataset based on AI2-THOR, dubbed as CCTP. Extensive experiments on this
challenging dataset demonstrate the superior performance of our method in
visual task planning. Empirically, we show that our framework can generalize to
unseen task trajectories and unseen object categories.


# MediTab: Scaling Medical Tabular Data Predictors via Data Consolidation, Enrichment, and Refinement

[Link to the paper](http://arxiv.org/abs/2305.12081v2)

## Authors
- Zifeng Wang
- Chufan Gao
- Cao Xiao
- Jimeng Sun

## Summary
  Tabular data prediction has been employed in medical applications such as
patient health risk prediction. However, existing methods usually revolve
around the algorithm design while overlooking the significance of data
engineering. Medical tabular datasets frequently exhibit significant
heterogeneity across different sources, with limited sample sizes per source.
As such, previous predictors are often trained on manually curated small
datasets that struggle to generalize across different tabular datasets during
inference. This paper proposes to scale medical tabular data predictors
(MediTab) to various tabular inputs with varying features. The method uses a
data engine that leverages large language models (LLMs) to consolidate tabular
samples to overcome the barrier across tables with distinct schema. It also
aligns out-domain data with the target task using a "learn, annotate, and
refinement" pipeline. The expanded training data then enables the pre-trained
MediTab to infer for arbitrary tabular input in the domain without fine-tuning,
resulting in significant improvements over supervised baselines: it reaches an
average ranking of 1.57 and 1.00 on 7 patient outcome prediction datasets and 3
trial outcome prediction datasets, respectively. In addition, MediTab exhibits
impressive zero-shot performances: it outperforms supervised XGBoost models by
8.9% and 17.2% on average in two prediction tasks, respectively. The code is
available at https://github.com/RyanWangZf/MediTab.


# An unexpected stochastic dominance: Pareto distributions, catastrophes, and risk exchange

[Link to the paper](http://arxiv.org/abs/2208.08471v3)

## Authors
- Yuyu Chen
- Paul Embrechts
- Ruodu Wang

## Summary
  We show the perhaps surprising inequality that the weighted average of
negatively dependent super-Pareto random variables, possibly caused by
triggering events, is larger than one such random variable in the sense of
first-order stochastic dominance. The class of super-Pareto distributions is
extremely heavy-tailed and it includes the class of infinite-mean Pareto
distributions. We discuss several implications of this result via an
equilibrium analysis in a risk exchange market. First, diversification of
super-Pareto losses increases portfolio risk, and thus a diversification
penalty exists. Second, agents with super-Pareto losses will not share risks in
a market equilibrium. Third, transferring losses from agents bearing
super-Pareto losses to external parties without any losses may arrive at an
equilibrium which benefits every party involved. The empirical studies show
that our new inequality can be observed empirically for real datasets that fit
well with extremely heavy tails.


# Concise and Organized Perception Facilitates Large Language Models for Deductive Reasoning

[Link to the paper](http://arxiv.org/abs/2310.03309v1)

## Authors
- Shaotian Yan
- Chen Shen
- Junjie Liu
- Jieping Ye

## Summary
  Exploiting large language models (LLMs) to tackle deductive reasoning has
garnered growing attention. It still remains highly challenging to achieve
satisfactory results in complex deductive problems, characterized by plenty of
premises (i.e., facts or rules) entailing intricate relationships among
entities and requiring multi-hop reasoning. One intuitive solution is to
decompose the original task into smaller sub-tasks, and then chain the multiple
casual reasoning steps together in a forward (e.g., Selection-Inference) or
backward (e.g., LAMBADA) direction. However, these techniques inevitably
necessitate a large number of overall stages, leading to computationally
expensive operations and a higher possibility of making misleading steps. In
addition to stage-by-stage decomposition, we draw inspiration from another
aspect of human problem-solving. Humans tend to distill the most relevant
information and organize their thoughts systematically (e.g., creating mind
maps), which assists them in answering questions or drawing conclusions
precisely and quickly. In light of this, we propose a novel reasoning approach
named Concise and Organized Perception (COP). COP carefully analyzes the given
statements to efficiently identify the most pertinent information while
eliminating redundancy. It then prompts the LLMs in a more organized form that
adapts to the model's inference process. By perceiving concise and organized
proofs, the deductive reasoning abilities of LLMs can be better elicited, and
the risk of acquiring errors caused by excessive reasoning stages is mitigated.
Furthermore, our approach can be combined with the aforementioned ones to
further boost their performance. Extensive experimental results on three
popular deductive benchmarks (i.e., ProofWriter, PrOntoQA and PrOntoQA-OOD)
show that COP significantly outperforms previous state-of-the-art methods.


# Learning Personalized Story Evaluation

[Link to the paper](http://arxiv.org/abs/2310.03304v1)

## Authors
- Danqing Wang
- Kevin Yang
- Hanlin Zhu
- Xiaomeng Yang
- Andrew Cohen
- Lei Li
- Yuandong Tian

## Summary
  While large language models (LLMs) have shown impressive results for more
objective tasks such as QA and retrieval, it remains nontrivial to evaluate
their performance on open-ended text generation for reasons including (1) data
contamination; (2) multi-dimensional evaluation criteria; and (3)
subjectiveness stemming from reviewers' personal preferences. To address such
issues, we propose to model personalization in an uncontaminated open-ended
generation assessment. We create two new datasets Per-MPST and Per-DOC for
personalized story evaluation, by re-purposing existing datasets with proper
anonymization and new personalized labels. We further develop a personalized
story evaluation model PERSE to infer reviewer preferences and provide a
personalized evaluation. Specifically, given a few exemplary reviews from a
particular reviewer, PERSE predicts either a detailed review or fine-grained
comparison in several aspects (such as interestingness and surprise) for that
reviewer on a new text input. Experimental results show that PERSE outperforms
GPT-4 by 15.8% on Kendall correlation of story ratings, and by 13.7% on
pairwise preference prediction accuracy. Both datasets and code will be
released at https://github.com/dqwang122/PerSE.


# A Two-stage Based Social Preference Recognition in Multi-Agent Autonomous Driving System

[Link to the paper](http://arxiv.org/abs/2310.03303v1)

## Authors
- Jintao Xue
- Dongkun Zhang
- Rong Xiong
- Yue Wang
- Eryun Liu

## Summary
  Multi-Agent Reinforcement Learning (MARL) has become a promising solution for
constructing a multi-agent autonomous driving system (MADS) in complex and
dense scenarios. But most methods consider agents acting selfishly, which leads
to conflict behaviors. Some existing works incorporate the concept of social
value orientation (SVO) to promote coordination, but they lack the knowledge of
other agents' SVOs, resulting in conservative maneuvers. In this paper, we aim
to tackle the mentioned problem by enabling the agents to understand other
agents' SVOs. To accomplish this, we propose a two-stage system framework.
Firstly, we train a policy by allowing the agents to share their ground truth
SVOs to establish a coordinated traffic flow. Secondly, we develop a
recognition network that estimates agents' SVOs and integrates it with the
policy trained in the first stage. Experiments demonstrate that our developed
method significantly improves the performance of the driving policy in MADS
compared to two state-of-the-art MARL algorithms.


# Benchmarking Large Language Models As AI Research Agents

[Link to the paper](http://arxiv.org/abs/2310.03302v1)

## Authors
- Qian Huang
- Jian Vora
- Percy Liang
- Jure Leskovec

## Summary
  Scientific experimentation involves an iterative process of creating
hypotheses, designing experiments, running experiments, and analyzing the
results. Can we build AI research agents to perform these long-horizon tasks?
To take a step towards building and evaluating research agents on such
open-ended decision-making tasks, we focus on the problem of machine learning
engineering: given a task description and a dataset, build a high-performing
model. In this paper, we propose MLAgentBench, a suite of ML tasks for
benchmarking AI research agents. Agents can perform actions like
reading/writing files, executing code, and inspecting outputs. With these
actions, agents could run experiments, analyze the results, and modify the code
of entire machine learning pipelines, such as data processing, architecture,
training processes, etc. The benchmark then automatically evaluates the agent's
performance objectively over various metrics related to performance and
efficiency. We also design an LLM-based research agent to automatically perform
experimentation loops in such an environment. Empirically, we find that a
GPT-4-based research agent can feasibly build compelling ML models over many
tasks in MLAgentBench, displaying highly interpretable plans and actions.
However, the success rates vary considerably; they span from almost 90\% on
well-established older datasets to as low as 10\% on recent Kaggle Challenges
-- unavailable during the LLM model's pretraining -- and even 0\% on newer
research challenges like BabyLM. Finally, we identify several key challenges
for LLM-based research agents such as long-term planning and hallucination. Our
code is released at https://github.com/snap-stanford/MLAgentBench.


# LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers

[Link to the paper](http://arxiv.org/abs/2310.03294v1)

## Authors
- Dacheng Li
- Rulin Shao
- Anze Xie
- Eric P. Xing
- Joseph E. Gonzalez
- Ion Stoica
- Xuezhe Ma
- Hao Zhang

## Summary
  Increasing the context length of large language models (LLMs) unlocks
fundamentally new capabilities, but also significantly increases the memory
footprints of training. Previous model-parallel systems such as Megatron-LM
partition and compute different attention heads in parallel, resulting in large
communication volumes, so they cannot scale beyond the number of attention
heads, thereby hindering its adoption. In this paper, we introduce a new
approach, LightSeq, for long-context LLMs training. LightSeq has many notable
advantages. First, LightSeq partitions over the sequence dimension, hence is
agnostic to model architectures and readily applicable for models with varying
numbers of attention heads, such as Multi-Head, Multi-Query and Grouped-Query
attention. Second, LightSeq not only requires up to 4.7x less communication
than Megatron-LM on popular LLMs but also overlaps the communication with
computation. To further reduce the training time, LightSeq features a novel
gradient checkpointing scheme to bypass an forward computation for
memory-efficient attention. We evaluate LightSeq on Llama-7B and its variants
with sequence lengths from 32K to 512K. Through comprehensive experiments on
single and cross-node training, we show that LightSeq achieves up to 1.24-2.01x
end-to-end speedup, and a 2-8x longer sequence length on models with fewer
heads, compared to Megatron-LM. Codes will be available at
https://github.com/RulinShao/LightSeq.


# A New Dialogue Response Generation Agent for Large Language Models by Asking Questions to Detect User's Intentions

[Link to the paper](http://arxiv.org/abs/2310.03293v1)

## Authors
- Siwei Wu
- Xiangqing Shen
- Rui Xia

## Summary
  Large Language Models (LLMs), such as ChatGPT, have recently been applied to
various NLP tasks due to its open-domain generation capabilities. However,
there are two issues with applying LLMs to dialogue tasks. 1. During the
dialogue process, users may have implicit intentions that might be overlooked
by LLMs. Consequently, generated responses couldn't align with the user's
intentions. 2. It is unlikely for LLMs to encompass all fields comprehensively.
In certain specific domains, their knowledge may be incomplete, and LLMs cannot
update the latest knowledge in real-time. To tackle these issues, we propose a
framework~\emph{using LLM to \textbf{E}nhance dialogue response generation by
asking questions to \textbf{D}etect user's \textbf{I}mplicit
in\textbf{T}entions} (\textbf{EDIT}). Firstly, EDIT generates open questions
related to the dialogue context as the potential user's intention; Then, EDIT
answers those questions by interacting with LLMs and searching in
domain-specific knowledge bases respectively, and use LLMs to choose the proper
answers to questions as extra knowledge; Finally, EDIT enhances response
generation by explicitly integrating those extra knowledge. Besides, previous
question generation works only focus on asking questions with answers in
context. In order to ask open questions, we construct a Context-Open-Question
(COQ) dataset. On two task-oriented dialogue tasks (Wizard of Wikipedia and
Holl-E), EDIT outperformed other LLMs.


# SimVLG: Simple and Efficient Pretraining of Visual Language Generative Models

[Link to the paper](http://arxiv.org/abs/2310.03291v1)

## Authors
- Yiren Jian
- Tingkai Liu
- Yunzhe Tao
- Soroush Vosoughi
- HX Yang

## Summary
  In this paper, we propose ``SimVLG'', a streamlined framework for the
pre-training of computationally intensive vision-language generative models,
leveraging frozen pre-trained large language models (LLMs). The prevailing
paradigm in vision-language pre-training (VLP) typically involves a two-stage
optimization process: an initial resource-intensive phase dedicated to
general-purpose vision-language representation learning, aimed at extracting
and consolidating pertinent visual features, followed by a subsequent phase
focusing on end-to-end alignment between visual and linguistic modalities. Our
one-stage, single-loss framework circumvents the aforementioned computationally
demanding first stage of training by gradually merging similar visual tokens
during training. This gradual merging process effectively compacts the visual
information while preserving the richness of semantic content, leading to fast
convergence without sacrificing performance. Our experiments show that our
approach can speed up the training of vision-language models by a factor
$\times 5$ without noticeable impact on the overall performance. Additionally,
we show that our models can achieve comparable performance to current
vision-language models with only $1/10$ of the data. Finally, we demonstrate
how our image-text models can be easily adapted to video-language generative
tasks through a novel soft attentive temporal token merging modules.


# Distributed Collaborative Safety-Critical Control for Networked Dynamic Systems

[Link to the paper](http://arxiv.org/abs/2310.03289v1)

## Authors
- Brooks A. Butler
- Philip E. ParÃ©

## Summary
  As modern systems become ever more connected with complex dynamic coupling
relationships, the development of safe control methods for such networked
systems becomes paramount. In this paper, we define a general networked model
with coupled dynamics and local control and discuss the relationship of
node-level safety definitions for individual agents with local neighborhood
dynamics. We define a node-level barrier function (NBF), node-level control
barrier function (NCBF), and collaborative node-level barrier function (cNCBF)
and provide conditions under which sets defined by these functions will be
forward invariant. We use collaborative node-level barrier functions to
construct a novel distributed algorithm for the safe control of collaborating
network agents and provide conditions under which the algorithm is guaranteed
to converge to a viable set of safe control actions for all agents or a
terminally infeasible state for at least one agent. We introduce the notion of
non-compliance of network neighbors as a metric of robustness for collaborative
safety for a given network state and chosen barrier function hyper-parameters.
We illustrate these results on a networked susceptible-infected-susceptible
(SIS) model.


# A Formalism and Approach for Improving Robustness of Large Language Models Using Risk-Adjusted Confidence Scores

[Link to the paper](http://arxiv.org/abs/2310.03283v1)

## Authors
- Ke Shen
- Mayank Kejriwal

## Summary
  Large Language Models (LLMs), such as ChatGPT, have achieved impressive
milestones in natural language processing (NLP). Despite their impressive
performance, the models are known to pose important risks. As these models are
deployed in real-world applications, a systematic understanding of different
risks posed by these models on tasks such as natural language inference (NLI),
is much needed. In this paper, we define and formalize two distinct types of
risk: decision risk and composite risk. We also propose a risk-centric
evaluation framework, and four novel metrics, for assessing LLMs on these risks
in both in-domain and out-of-domain settings. Finally, we propose a
risk-adjusted calibration method called DwD for helping LLMs minimize these
risks in an overall NLI architecture. Detailed experiments, using four NLI
benchmarks, three baselines and two LLMs, including ChatGPT, show both the
practical utility of the evaluation framework, and the efficacy of DwD in
reducing decision and composite risk. For instance, when using DwD, an
underlying LLM is able to address an extra 20.1% of low-risk inference tasks
(but which the LLM erroneously deems high-risk without risk adjustment) and
skip a further 19.8% of high-risk tasks, which would have been answered
incorrectly.


# TADIS: Steering Models for Deep-Thinking about Demonstration Examples

[Link to the paper](http://arxiv.org/abs/2310.00901v2)

## Authors
- Tianci Xue
- Ziqi Wang
- Yixia Li
- Yun Chen
- Guanhua Chen

## Summary
  Instruction tuning has been demonstrated that could significantly improve the
zero-shot generalization capability to unseen tasks by an apparent margin. By
incorporating additional context (e.g., task definition, examples) during the
fine-tuning process, Large Language Models (LLMs) achieved much higher
performance than before. However, recent work reported that delusive task
examples can achieve almost the same performance as correct task examples,
indicating the input-label correspondence is less important than previously
thought. Intrigued by this counter-intuitive observation, we suspect models
have the same illusion of competence as humans. Therefore, we propose a novel
method called TADIS that steers LLMs for "Deep-Thinking'' about demonstration
examples instead of merely seeing. To alleviate the illusion of competence of
models, we first ask the model to verify the correctness of shown examples.
Then, using the verification results as conditions to elicit models for a
better answer. Our experimental results show that TADIS consistently
outperforms competitive baselines on in-domain and out-domain tasks (improving
2.79 and 4.03 average ROUGLE-L on out-domain and in-domain datasets,
respectively). Despite the presence of generated examples (not all of the
thinking labels are accurate), TADIS can notably enhance performance in
zero-shot and few-shot settings. This also suggests that our approach can be
adopted on a large scale to improve the instruction following capabilities of
models without any manual labor. Moreover, we construct three types of thinking
labels with different model sizes and find that small models learn from the
format of TADIS but larger models can be steered for "Deep-Thinking''.


# Rethinking the Evaluating Framework for Natural Language Understanding in AI Systems: Language Acquisition as a Core for Future Metrics

[Link to the paper](http://arxiv.org/abs/2309.11981v3)

## Authors
- Patricio Vera
- Pedro Moya
- Lisa Barraza

## Summary
  In the burgeoning field of artificial intelligence (AI), the unprecedented
progress of large language models (LLMs) in natural language processing (NLP)
offers an opportunity to revisit the entire approach of traditional metrics of
machine intelligence, both in form and content. As the realm of machine
cognitive evaluation has already reached Imitation, the next step is an
efficient Language Acquisition and Understanding. Our paper proposes a paradigm
shift from the established Turing Test towards an all-embracing framework that
hinges on language acquisition, taking inspiration from the recent advancements
in LLMs. The present contribution is deeply tributary of the excellent work
from various disciplines, point out the need to keep interdisciplinary bridges
open, and delineates a more robust and sustainable approach.


# AnglE-optimized Text Embeddings

[Link to the paper](http://arxiv.org/abs/2309.12871v2)

## Authors
- Xianming Li
- Jing Li

## Summary
  High-quality text embedding is pivotal in improving semantic textual
similarity (STS) tasks, which are crucial components in Large Language Model
(LLM) applications. However, a common challenge existing text embedding models
face is the problem of vanishing gradients, primarily due to their reliance on
the cosine function in the optimization objective, which has saturation zones.
To address this issue, this paper proposes a novel angle-optimized text
embedding model called AnglE. The core idea of AnglE is to introduce angle
optimization in a complex space. This novel approach effectively mitigates the
adverse effects of the saturation zone in the cosine function, which can impede
gradient and hinder optimization processes. To set up a comprehensive STS
evaluation, we experimented on existing short-text STS datasets and a newly
collected long-text STS dataset from GitHub Issues. Furthermore, we examine
domain-specific STS scenarios with limited labeled data and explore how AnglE
works with LLM-annotated data. Extensive experiments were conducted on various
tasks including short-text STS, long-text STS, and domain-specific STS tasks.
The results show that AnglE outperforms the state-of-the-art (SOTA) STS models
that ignore the cosine saturation zone. These findings demonstrate the ability
of AnglE to generate high-quality text embeddings and the usefulness of angle
optimization in STS.


# InstructProtein: Aligning Human and Protein Language via Knowledge Instruction

[Link to the paper](http://arxiv.org/abs/2310.03269v1)

## Authors
- Zeyuan Wang
- Qiang Zhang
- Keyan Ding
- Ming Qin
- Xiang Zhuang
- Xiaotong Li
- Huajun Chen

## Summary
  Large Language Models (LLMs) have revolutionized the field of natural
language processing, but they fall short in comprehending biological sequences
such as proteins. To address this challenge, we propose InstructProtein, an
innovative LLM that possesses bidirectional generation capabilities in both
human and protein languages: (i) taking a protein sequence as input to predict
its textual function description and (ii) using natural language to prompt
protein sequence generation. To achieve this, we first pre-train an LLM on both
protein and natural language corpora, enabling it to comprehend individual
languages. Then supervised instruction tuning is employed to facilitate the
alignment of these two distinct languages. Herein, we introduce a knowledge
graph-based instruction generation framework to construct a high-quality
instruction dataset, addressing annotation imbalance and instruction deficits
in existing protein-text corpus. In particular, the instructions inherit the
structural relations between proteins and function annotations in knowledge
graphs, which empowers our model to engage in the causal modeling of protein
functions, akin to the chain-of-thought processes in natural languages.
Extensive experiments on bidirectional protein-text generation tasks show that
InstructProtein outperforms state-of-the-art LLMs by large margins. Moreover,
InstructProtein serves as a pioneering step towards text-based protein function
prediction and sequence design, effectively bridging the gap between protein
and human language understanding.


# UniPredict: Large Language Models are Universal Tabular Predictors

[Link to the paper](http://arxiv.org/abs/2310.03266v1)

## Authors
- Ruiyu Wang
- Zifeng Wang
- Jimeng Sun

## Summary
  Tabular data prediction is a fundamental machine learning task for many
applications. Existing methods predominantly employ discriminative modeling and
operate under the assumption of a fixed target column, necessitating
re-training for every new predictive task. Inspired by the generative power of
large language models (LLMs), this paper exploits the idea of building
universal tabular data predictors based on generative modeling, namely
UniPredict. Here, we show that scaling up an LLM to extensive tabular datasets
with the capability of comprehending diverse tabular inputs and predicting for
target variables following the input instructions. Specifically, we train a
single LLM on an aggregation of 169 tabular datasets with diverse targets and
compare its performance against baselines that are trained on each dataset
separately. We observe this versatile UniPredict model demonstrates an
advantage over other models, ranging from 5.4% to 13.4%, when compared with the
best tree-boosting baseline and the best neural network baseline, respectively.
We further test UniPredict in few-shot learning settings on another 62 tabular
datasets. Our method achieves strong performance in quickly adapting to new
tasks, where our method outperforms XGBoost over 100% on the low-resource setup
and shows a significant margin over all baselines. We envision that UniPredict
sheds light on developing a universal tabular data prediction system that
learns from data at scale and serves a wide range of prediction tasks.


# Unlock Predictable Scaling from Emergent Abilities

[Link to the paper](http://arxiv.org/abs/2310.03262v1)

## Authors
- Shengding Hu
- Xin Liu
- Xu Han
- Xinrong Zhang
- Chaoqun He
- Weilin Zhao
- Yankai Lin
- Ning Ding
- Zebin Ou
- Guoyang Zeng
- Zhiyuan Liu
- Maosong Sun

## Summary
  The scientific scale-up of large language models (LLMs) necessitates a
comprehensive understanding of their scaling properties. However, the existing
literature on the scaling properties only yields an incomplete answer:
optimization loss decreases predictably as the model size increases, in line
with established scaling law; yet no scaling law for task has been established
and the task performances are far from predictable during scaling. Task
performances typically show minor gains on small models until they improve
dramatically once models exceed a size threshold, exemplifying the ``emergent
abilities''. In this study, we discover that small models, although they
exhibit minor performance, demonstrate critical and consistent task performance
improvements that are not captured by conventional evaluation strategies due to
insufficient measurement resolution. To measure such improvements, we introduce
PassUntil, an evaluation strategy through massive sampling in the decoding
phase. We conduct quantitative investigations into the scaling law of task
performance. Firstly, a strict task scaling law is identified, enhancing the
predictability of task performances. Remarkably, we are able to predict the
performance of the 2.4B model on code generation with merely 0.05\% deviation
before training starts. Secondly, underpinned by PassUntil, we observe concrete
evidence of emergent abilities and ascertain that they are not in conflict with
the continuity of performance improvement. Their semblance to break-through is
that their scaling curve cannot be fitted by standard scaling law function. We
then introduce a mathematical definition for the emergent abilities. Through
the definition, we refute a prevalent ``multi-step reasoning hypothesis''
regarding the genesis of emergent abilities and propose a new hypothesis with a
satisfying fit to the observed scaling curve.


# Mechanic Maker 2.0: Reinforcement Learning for Evaluating Generated Rules

[Link to the paper](http://arxiv.org/abs/2309.09476v3)

## Authors
- Johor Jara Gonzalez
- Seth Cooper
- Matthew Guzdial

## Summary
  Automated game design (AGD), the study of automatically generating game
rules, has a long history in technical games research. AGD approaches generally
rely on approximations of human play, either objective functions or AI agents.
Despite this, the majority of these approximators are static, meaning they do
not reflect human player's ability to learn and improve in a game. In this
paper, we investigate the application of Reinforcement Learning (RL) as an
approximator for human play for rule generation. We recreate the classic AGD
environment Mechanic Maker in Unity as a new, open-source rule generation
framework. Our results demonstrate that RL produces distinct sets of rules from
an A* agent baseline, which may be more usable by humans.


# Can Large Language Models be Good Path Planners? A Benchmark and Investigation on Spatial-temporal Reasoning

[Link to the paper](http://arxiv.org/abs/2310.03249v1)

## Authors
- Mohamed Aghzal
- Erion Plaku
- Ziyu Yao

## Summary
  Large language models (LLMs) have achieved remarkable success across a wide
spectrum of tasks; however, they still face limitations in scenarios that
demand long-term planning and spatial reasoning. To facilitate this line of
research, in this work, we propose a new benchmark, termed $\textbf{P}$ath
$\textbf{P}$lanning from $\textbf{N}$atural $\textbf{L}$anguage
($\textbf{PPNL}$). Our benchmark evaluates LLMs' spatial-temporal reasoning by
formulating ''path planning'' tasks that require an LLM to navigate to target
locations while avoiding obstacles and adhering to constraints. Leveraging this
benchmark, we systematically investigate LLMs including GPT-4 via different
few-shot prompting methodologies and BART and T5 of various sizes via
fine-tuning. Our experimental results show the promise of few-shot GPT-4 in
spatial reasoning, when it is prompted to reason and act interleavedly,
although it still fails to make long-term temporal reasoning. In contrast,
while fine-tuned LLMs achieved impressive results on in-distribution reasoning
tasks, they struggled to generalize to larger environments or environments with
more obstacles.


# FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation

[Link to the paper](http://arxiv.org/abs/2310.03214v1)

## Authors
- Tu Vu
- Mohit Iyyer
- Xuezhi Wang
- Noah Constant
- Jerry Wei
- Jason Wei
- Chris Tar
- Yun-Hsuan Sung
- Denny Zhou
- Quoc Le
- Thang Luong

## Summary
  Most large language models (LLMs) are trained once and never updated; thus,
they lack the ability to dynamically adapt to our ever-changing world. In this
work, we perform a detailed study of the factuality of LLM-generated text in
the context of answering questions that test current world knowledge.
Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a
diverse range of question and answer types, including questions that require
fast-changing world knowledge as well as questions with false premises that
need to be debunked. We benchmark a diverse array of both closed and
open-source LLMs under a two-mode evaluation procedure that allows us to
measure both correctness and hallucination. Through human evaluations involving
more than 50K judgments, we shed light on limitations of these models and
demonstrate significant room for improvement: for instance, all models
(regardless of model size) struggle on questions that involve fast-changing
knowledge and false premises. Motivated by these results, we present
FreshPrompt, a simple few-shot prompting method that substantially boosts the
performance of an LLM on FreshQA by incorporating relevant and up-to-date
information retrieved from a search engine into the prompt. Our experiments
show that FreshPrompt outperforms both competing search engine-augmented
prompting methods such as Self-Ask (Press et al., 2022) as well as commercial
systems such as Perplexity.AI. Further analysis of FreshPrompt reveals that
both the number of retrieved evidences and their order play a key role in
influencing the correctness of LLM-generated answers. Additionally, instructing
the LLM to generate concise and direct answers helps reduce hallucination
compared to encouraging more verbose answers. To facilitate future work, we
release FreshQA at github.com/freshllms/freshqa and commit to updating it at
regular intervals.


# On the Performance of Multimodal Language Models

[Link to the paper](http://arxiv.org/abs/2310.03211v1)

## Authors
- Utsav Garg
- Erhan Bas

## Summary
  Instruction-tuned large language models (LLMs) have demonstrated promising
zero-shot generalization capabilities across various downstream tasks. Recent
research has introduced multimodal capabilities to LLMs by integrating
independently pretrained vision encoders through model grafting. These
multimodal variants undergo instruction tuning, similar to LLMs, enabling
effective zero-shot generalization for multimodal tasks. This study conducts a
comparative analysis of different multimodal instruction tuning approaches and
evaluates their performance across a range of tasks, including complex
reasoning, conversation, image captioning, multiple-choice questions (MCQs),
and binary classification. Through rigorous benchmarking and ablation
experiments, we reveal key insights for guiding architectural choices when
incorporating multimodal capabilities into LLMs. However, current approaches
have limitations; they do not sufficiently address the need for a diverse
multimodal instruction dataset, which is crucial for enhancing task
generalization. Additionally, they overlook issues related to truthfulness and
factuality when generating responses. These findings illuminate current
methodological constraints in adapting language models for image comprehension
and provide valuable guidance for researchers and practitioners seeking to
harness multimodal versions of LLMs.


# Can Language Models Employ the Socratic Method? Experiments with Code Debugging

[Link to the paper](http://arxiv.org/abs/2310.03210v1)

## Authors
- Erfan Al-Hossami
- Razvan Bunescu
- Justin Smith
- Ryan Teehan

## Summary
  When employing the Socratic method of teaching, instructors guide students
toward solving a problem on their own rather than providing the solution
directly. While this strategy can substantially improve learning outcomes, it
is usually time-consuming and cognitively demanding. Automated Socratic
conversational agents can augment human instruction and provide the necessary
scale, however their development is hampered by the lack of suitable data for
training and evaluation. In this paper, we introduce a manually created dataset
of multi-turn Socratic advice that is aimed at helping a novice programmer fix
buggy solutions to simple computational problems. The dataset is then used for
benchmarking the Socratic debugging abilities of a number of language models,
ranging from fine-tuning the instruction-based text-to-text transformer Flan-T5
to zero-shot and chain of thought prompting of the much larger GPT-4. The code
and datasets are made freely available for research at the link below.
https://github.com/taisazero/socratic-debugging-benchmark


# MedAlpaca -- An Open-Source Collection of Medical Conversational AI Models and Training Data

[Link to the paper](http://arxiv.org/abs/2304.08247v2)

## Authors
- Tianyu Han
- Lisa C. Adams
- Jens-Michalis Papaioannou
- Paul Grundmann
- Tom Oberhauser
- Alexander LÃ¶ser
- Daniel Truhn
- Keno K. Bressem

## Summary
  As large language models (LLMs) like OpenAI's GPT series continue to make
strides, we witness the emergence of artificial intelligence applications in an
ever-expanding range of fields. In medicine, these LLMs hold considerable
promise for improving medical workflows, diagnostics, patient care, and
education. Yet, there is an urgent need for open-source models that can be
deployed on-premises to safeguard patient privacy. In our work, we present an
innovative dataset consisting of over 160,000 entries, specifically crafted to
fine-tune LLMs for effective medical applications. We investigate the impact of
fine-tuning these datasets on publicly accessible pre-trained LLMs, and
subsequently, we juxtapose the performance of pre-trained-only models against
the fine-tuned models concerning the examinations that future medical doctors
must pass to achieve certification.


# Regret Analysis of Distributed Online Control for LTI Systems with Adversarial Disturbances

[Link to the paper](http://arxiv.org/abs/2310.03206v1)

## Authors
- Ting-Jui Chang
- Shahin Shahrampour

## Summary
  This paper addresses the distributed online control problem over a network of
linear time-invariant (LTI) systems (with possibly unknown dynamics) in the
presence of adversarial perturbations. There exists a global network cost that
is characterized by a time-varying convex function, which evolves in an
adversarial manner and is sequentially and partially observed by local agents.
The goal of each agent is to generate a control sequence that can compete with
the best centralized control policy in hindsight, which has access to the
global cost. This problem is formulated as a regret minimization. For the case
of known dynamics, we propose a fully distributed disturbance feedback
controller that guarantees a regret bound of $O(\sqrt{T}\log T)$, where $T$ is
the time horizon. For the unknown dynamics case, we design a distributed
explore-then-commit approach, where in the exploration phase all agents jointly
learn the system dynamics, and in the learning phase our proposed control
algorithm is applied using each agent system estimate. We establish a regret
bound of $O(T^{2/3} \text{poly}(\log T))$ for this setting.


# Bootstrapping Developmental AIs: From Simple Competences to Intelligent Human-Compatible AIs

[Link to the paper](http://arxiv.org/abs/2308.04586v9)

## Authors
- Mark Stefik
- Robert Price

## Summary
  The mainstream AIs approaches are the generative and deep learning approaches
with large language models (LLMs) and the manually constructed symbolic
approach. Both approaches have led to valuable AI systems and impressive feats.
However, manually constructed AIs are brittle even in circumscribed domains.
Generative AIs make strange mistakes and do not notice them. In both approaches
the AIs cannot be instructed easily, fail to use common sense, and lack
curiosity. They have abstract knowledge but lack social alignment.
Developmental AIs have more potential. They start with innate competences,
interact with their environment, and learn from their interactions. They
interact and learn from people and establish perceptual, cognitive, and common
grounding. Developmental AIs have demonstrated capabilities including
multimodal perception, object recognition, and manipulation. Powerful
computational models for hierarchical planning, abstraction discovery,
curiosity, and language acquisition exist but need to be adapted to a
developmental learning based approach. The promise is that developmental AIs
will acquire self-developed and socially developed competences. They would
address the shortcomings of current mainstream AI approaches, and ultimately
lead to sophisticated forms of learning involving critical reading, provenance
evaluation, and hypothesis testing. However, developmental AI projects have not
yet fully reached the Speaking Gap corresponding to toddler development at
about two years of age, before their speech is fluent. The AIs do not bridge
the Reading Gap, to skillfully and skeptically learn from written and online
information resources. This position paper lays out the prospects, gaps, and
challenges for extending the practice of developmental AIs to create resilient,
intelligent, and human-compatible AIs that learn what they need to know.


# SqueezeLLM: Dense-and-Sparse Quantization

[Link to the paper](http://arxiv.org/abs/2306.07629v2)

## Authors
- Sehoon Kim
- Coleman Hooper
- Amir Gholami
- Zhen Dong
- Xiuyu Li
- Sheng Shen
- Michael W. Mahoney
- Kurt Keutzer

## Summary
  Generative Large Language Models (LLMs) have demonstrated remarkable results
for a wide range of tasks. However, deploying these models for inference has
been a significant challenge due to their unprecedented resource requirements.
This has forced existing deployment frameworks to use multi-GPU inference
pipelines, which are often complex and costly, or to use smaller and less
performant models. In this work, we demonstrate that the main bottleneck for
generative inference with LLMs is memory bandwidth, rather than compute,
specifically for single batch inference. While quantization has emerged as a
promising solution by representing model weights with reduced precision,
previous efforts have often resulted in notable performance degradation. To
address this, we introduce SqueezeLLM, a post-training quantization framework
that not only enables lossless compression to ultra-low precisions of up to
3-bit, but also achieves higher quantization performance under the same memory
constraint. Our framework incorporates two novel ideas: (i) sensitivity-based
non-uniform quantization, which searches for the optimal bit precision
assignment based on second-order information; and (ii) the Dense-and-Sparse
decomposition that stores outliers and sensitive weight values in an efficient
sparse format. When applied to the LLaMA models, our 3-bit quantization
significantly reduces the perplexity gap from the FP16 baseline by up to 2.1x
as compared to the state-of-the-art methods with the same memory requirement.
Furthermore, when deployed on an A6000 GPU, our quantized models achieve up to
2.3x speedup compared to the baseline. Our code is open-sourced and available
online.


# CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers

[Link to the paper](http://arxiv.org/abs/2305.17455v2)

## Authors
- Dachuan Shi
- Chaofan Tao
- Anyi Rao
- Zhendong Yang
- Chun Yuan
- Jiaqi Wang

## Summary
  Recent vision-language models have achieved tremendous progress far beyond
what we ever expected. However, their computational costs are also dramatically
growing with rapid development, especially for the large models. It makes model
acceleration exceedingly critical in a scenario of limited resources. Although
extensively studied for unimodal models, the acceleration for multimodal
models, especially the vision-language Transformers, is relatively
under-explored. To pursue more efficient and accessible vision-language
Transformers, this paper introduces \textbf{Cross}-\textbf{G}uided
\textbf{E}nsemble of \textbf{T}okens (\textbf{\emph{CrossGET}}), a universal
acceleration framework for vision-language Transformers. This framework
adaptively combines tokens through real-time, cross-modal guidance, thereby
achieving substantial acceleration while keeping high performance.
\textit{CrossGET} has two key innovations: 1) \textit{Cross-Guided Matching and
Ensemble}. \textit{CrossGET} incorporates cross-modal guided token matching and
ensemble to exploit cross-modal information effectively, only introducing
cross-modal tokens with negligible extra parameters. 2) \textit{Complete-Graph
Soft Matching}. In contrast to the existing bipartite soft matching approach,
\textit{CrossGET} introduces a complete-graph soft matching policy to achieve
more reliable token-matching results while maintaining parallelizability and
high efficiency. Extensive experiments are conducted on various vision-language
tasks, including image-text retrieval, visual reasoning, image captioning, and
visual question answering. Performance on both classic multimodal architectures
and emerging multimodal LLMs demonstrate the effectiveness and versatility of
the proposed \textit{CrossGET} framework. The code will be at
\url{https://github.com/sdc17/CrossGET}.


# DRL-based Energy-Efficient Baseband Function Deployments for Service-Oriented Open RAN

[Link to the paper](http://arxiv.org/abs/2212.12055v5)

## Authors
- Haiyuan Li
- Amin Emami
- Karcius Assis
- Antonis Vafeas
- Ruizhi Yang
- Reza Nejabati
- Shuangyi Yan
- Dimitra Simeonidou

## Summary
  Open Radio Access Network (Open RAN) has gained tremendous attention from
industry and academia with decentralized baseband functions across multiple
processing units located at different places. However, the ever-expanding scope
of RANs, along with fluctuations in resource utilization across different
locations and timeframes, necessitates the implementation of robust function
management policies to minimize network energy consumption. Most recently
developed strategies neglected the activation time and the required energy for
the server activation process, while this process could offset the potential
energy savings gained from server hibernation. Furthermore, user plane
functions, which can be deployed on edge computing servers to provide
low-latency services, have not been sufficiently considered. In this paper, a
multi-agent deep reinforcement learning (DRL) based function deployment
algorithm, coupled with a heuristic method, has been developed to minimize
energy consumption while fulfilling multiple requests and adhering to latency
and resource constraints. In an 8-MEC network, the DRL-based solution
approaches the performance of the benchmark while offering up to 51% energy
savings compared to existing approaches. In a larger network of 14-MEC, it
maintains a 38% energy-saving advantage and ensures real-time response
capabilities. Furthermore, this paper prototypes an Open RAN testbed to verify
the feasibility of the proposed solution.


# Misusing Tools in Large Language Models With Visual Adversarial Examples

[Link to the paper](http://arxiv.org/abs/2310.03185v1)

## Authors
- Xiaohan Fu
- Zihan Wang
- Shuheng Li
- Rajesh K. Gupta
- Niloofar Mireshghallah
- Taylor Berg-Kirkpatrick
- Earlence Fernandes

## Summary
  Large Language Models (LLMs) are being enhanced with the ability to use tools
and to process multiple modalities. These new capabilities bring new benefits
and also new security risks. In this work, we show that an attacker can use
visual adversarial examples to cause attacker-desired tool usage. For example,
the attacker could cause a victim LLM to delete calendar events, leak private
conversations and book hotels. Different from prior work, our attacks can
affect the confidentiality and integrity of user resources connected to the LLM
while being stealthy and generalizable to multiple input prompts. We construct
these attacks using gradient-based adversarial training and characterize
performance along multiple dimensions. We find that our adversarial images can
manipulate the LLM to invoke tools following real-world syntax almost always
(~98%) while maintaining high similarity to clean images (~0.9 SSIM).
Furthermore, using human scoring and automated metrics, we find that the
attacks do not noticeably affect the conversation (and its semantics) between
the user and the LLM.


# Retrieval-augmented Generation to Improve Math Question-Answering: Trade-offs Between Groundedness and Human Preference

[Link to the paper](http://arxiv.org/abs/2310.03184v1)

## Authors
- Zachary Levonian
- Chenglu Li
- Wangda Zhu
- Anoushka Gade
- Owen Henkel
- Millie-Ellen Postle
- Wanli Xing

## Summary
  For middle-school math students, interactive question-answering (QA) with
tutors is an effective way to learn. The flexibility and emergent capabilities
of generative large language models (LLMs) has led to a surge of interest in
automating portions of the tutoring process - including interactive QA to
support conceptual discussion of mathematical concepts. However, LLM responses
to math questions can be incorrect or mismatched to the educational context -
such as being misaligned with a school's curriculum. One potential solution is
retrieval-augmented generation (RAG), which involves incorporating a vetted
external knowledge source in the LLM prompt to increase response quality. In
this paper, we designed prompts that retrieve and use content from a
high-quality open-source math textbook to generate responses to real student
questions. We evaluate the efficacy of this RAG system for middle-school
algebra and geometry QA by administering a multi-condition survey, finding that
humans prefer responses generated using RAG, but not when responses are too
grounded in the textbook content. We argue that while RAG is able to improve
response quality, designers of math QA systems must consider trade-offs between
generating responses preferred by students and responses closely matched to
specific educational resources.


# $\mathcal{B}$-Coder: Value-Based Deep Reinforcement Learning for Program Synthesis

[Link to the paper](http://arxiv.org/abs/2310.03173v1)

## Authors
- Zishun Yu
- Yunzhe Tao
- Liyu Chen
- Tao Sun
- Hongxia Yang

## Summary
  Program synthesis aims to create accurate, executable code from natural
language descriptions. This field has leveraged the power of reinforcement
learning (RL) in conjunction with large language models (LLMs), significantly
enhancing code generation capabilities. This integration focuses on directly
optimizing functional correctness, transcending conventional supervised losses.
While current literature predominantly favors policy-based algorithms,
attributes of program synthesis suggest a natural compatibility with
value-based methods. This stems from rich collection of off-policy programs
developed by human programmers, and the straightforward verification of
generated programs through automated unit testing (i.e. easily obtainable
rewards in RL language). Diverging from the predominant use of policy-based
algorithms, our work explores the applicability of value-based approaches,
leading to the development of our $\mathcal{B}$-Coder (pronounced Bellman
coder). Yet, training value-based methods presents challenges due to the
enormous search space inherent to program synthesis. To this end, we propose an
initialization protocol for RL agents utilizing pre-trained LMs and a
conservative Bellman operator to reduce training complexities. Moreover, we
demonstrate how to leverage the learned value functions as a dual strategy to
post-process generated programs. Our empirical evaluations demonstrated
$\mathcal{B}$-Coder's capability in achieving state-of-the-art performance
compared with policy-based methods. Remarkably, this achievement is reached
with minimal reward engineering effort, highlighting the effectiveness of
value-based RL, independent of reward designs.


# Neural architecture impact on identifying temporally extended Reinforcement Learning tasks

[Link to the paper](http://arxiv.org/abs/2310.03161v1)

## Authors
- Victor Vadakechirayath George

## Summary
  Inspired by recent developments in attention models for image classification
and natural language processing, we present various Attention based
architectures in reinforcement learning (RL) domain, capable of performing well
on OpenAI Gym Atari-2600 game suite. In spite of the recent success of Deep
Reinforcement learning techniques in various fields like robotics, gaming and
healthcare, they suffer from a major drawback that neural networks are
difficult to interpret. We try to get around this problem with the help of
Attention based models. In Attention based models, extracting and overlaying of
attention map onto images allows for direct observation of information used by
agent to select actions and easier interpretation of logic behind the chosen
actions. Our models in addition to playing well on gym-Atari environments, also
provide insights on how agent perceives its environment. In addition, motivated
by recent developments in attention based video-classification models using
Vision Transformer, we come up with an architecture based on Vision
Transformer, for image-based RL domain too. Compared to previous works in
Vision Transformer, our model is faster to train and requires fewer
computational resources. 3


# Two-stage LLM Fine-tuning with Less Specialization and More Generalization

[Link to the paper](http://arxiv.org/abs/2211.00635v2)

## Authors
- Yihan Wang
- Si Si
- Daliang Li
- Michal Lukasik
- Felix Yu
- Cho-Jui Hsieh
- Inderjit S Dhillon
- Sanjiv Kumar

## Summary
  Pretrained large language models (LLMs) are general purpose problem solvers
applicable to a diverse set of tasks with prompts. They can be further improved
towards a specific task by fine-tuning on a specialized dataset. However,
fine-tuning usually makes the model narrowly specialized on this dataset with
reduced general in-context learning performances, which is undesirable whenever
the fine-tuned model needs to handle additional tasks where no fine-tuning data
is available. In this work, we first demonstrate that fine-tuning on a single
task indeed decreases LLMs' general in-context learning performance. We
discover one important cause of such forgetting, format specialization, where
the model overfits to the format of the fine-tuned task. We further show that
format specialization happens at the very beginning of fine-tuning. To solve
this problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet
effective two-stage fine-tuning framework that reduces format specialization
and improves generalization. ProMoT offloads task-specific format learning into
additional and removable parameters by first doing prompt tuning and then
fine-tuning the model itself with this soft prompt attached. With experiments
on several fine-tuning tasks and 8 in-context evaluation tasks, we show that
ProMoT achieves comparable performance on fine-tuned tasks to standard
fine-tuning, but with much less loss of in-context learning performances across
a board range of out-of-domain evaluation tasks. More importantly, ProMoT can
even enhance generalization on in-context learning tasks that are semantically
related to the fine-tuned task, e.g. ProMoT on En-Fr translation significantly
improves performance on other language pairs, and ProMoT on NLI improves
performance on summarization. Experiments also show that ProMoT can improve the
generalization performance of multi-task training.


# Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad, the Ugly

[Link to the paper](http://arxiv.org/abs/2310.03150v1)

## Authors
- Herbert WoisetschlÃ¤ger
- Alexander Isenko
- Shiqiang Wang
- Ruben Mayer
- Hans-Arno Jacobsen

## Summary
  Large Language Models (LLM) and foundation models are popular as they offer
new opportunities for individuals and businesses to improve natural language
processing, interact with data, and retrieve information faster. However,
training or fine-tuning LLMs requires a vast amount of data, which can be
challenging to access due to legal or technical restrictions and may require
private computing resources. Federated Learning (FL) is a solution designed to
overcome these challenges and expand data access for deep learning
applications.
  This paper takes a hardware-centric approach to explore how LLMs can be
brought to modern edge computing systems. Our study fine-tunes the FLAN-T5
model family, ranging from 80M to 3B parameters, using FL for a text
summarization task. We provide a micro-level hardware benchmark, compare the
model FLOP utilization to a state-of-the-art data center GPU, and study the
network utilization in realistic conditions. Our contribution is twofold:
First, we evaluate the current capabilities of edge computing systems and their
potential for LLM FL workloads. Second, by comparing these systems with a
data-center GPU, we demonstrate the potential for improvement and the next
steps toward achieving greater computational efficiency at the edge.


# Marginalized Importance Sampling for Off-Environment Policy Evaluation

[Link to the paper](http://arxiv.org/abs/2309.01807v2)

## Authors
- Pulkit Katdare
- Nan Jiang
- Katherine Driggs-Campbell

## Summary
  Reinforcement Learning (RL) methods are typically sample-inefficient, making
it challenging to train and deploy RL-policies in real world robots. Even a
robust policy trained in simulation requires a real-world deployment to assess
their performance. This paper proposes a new approach to evaluate the
real-world performance of agent policies prior to deploying them in the real
world. Our approach incorporates a simulator along with real-world offline data
to evaluate the performance of any policy using the framework of Marginalized
Importance Sampling (MIS). Existing MIS methods face two challenges: (1) large
density ratios that deviate from a reasonable range and (2) indirect
supervision, where the ratio needs to be inferred indirectly, thus exacerbating
estimation error. Our approach addresses these challenges by introducing the
target policy's occupancy in the simulator as an intermediate variable and
learning the density ratio as the product of two terms that can be learned
separately. The first term is learned with direct supervision and the second
term has a small magnitude, thus making it computationally efficient. We
analyze the sample complexity as well as error propagation of our two
step-procedure. Furthermore, we empirically evaluate our approach on Sim2Sim
environments such as Cartpole, Reacher, and Half-Cheetah. Our results show that
our method generalizes well across a variety of Sim2Sim gap, target policies
and offline data collection policies. We also demonstrate the performance of
our algorithm on a Sim2Real task of validating the performance of a 7 DoF
robotic arm using offline data along with the Gazebo simulator.


# Online POMDP Planning with Anytime Deterministic Guarantees

[Link to the paper](http://arxiv.org/abs/2310.01791v2)

## Authors
- Moran Barenboim
- Vadim Indelman

## Summary
  Autonomous agents operating in real-world scenarios frequently encounter
uncertainty and make decisions based on incomplete information. Planning under
uncertainty can be mathematically formalized using partially observable Markov
decision processes (POMDPs). However, finding an optimal plan for POMDPs can be
computationally expensive and is feasible only for small tasks. In recent
years, approximate algorithms, such as tree search and sample-based
methodologies, have emerged as state-of-the-art POMDP solvers for larger
problems. Despite their effectiveness, these algorithms offer only
probabilistic and often asymptotic guarantees toward the optimal solution due
to their dependence on sampling. To address these limitations, we derive a
deterministic relationship between a simplified solution that is easier to
obtain and the theoretically optimal one. First, we derive bounds for selecting
a subset of the observations to branch from while computing a complete belief
at each posterior node. Then, since a complete belief update may be
computationally demanding, we extend the bounds to support reduction of both
the state and the observation spaces. We demonstrate how our guarantees can be
integrated with existing state-of-the-art solvers that sample a subset of
states and observations. As a result, the returned solution holds deterministic
bounds relative to the optimal policy. Lastly, we substantiate our findings
with supporting experimental results.


# NLPBench: Evaluating Large Language Models on Solving NLP Problems

[Link to the paper](http://arxiv.org/abs/2309.15630v2)

## Authors
- Linxin Song
- Jieyu Zhang
- Lechao Cheng
- Pengyuan Zhou
- Tianyi Zhou
- Irene Li

## Summary
  Recent developments in large language models (LLMs) have shown promise in
enhancing the capabilities of natural language processing (NLP). Despite these
successes, there remains a dearth of research dedicated to the NLP
problem-solving abilities of LLMs. To fill the gap in this area, we present a
unique benchmarking dataset, NLPBench, comprising 378 college-level NLP
questions spanning various NLP topics sourced from Yale University's prior
final exams. NLPBench includes questions with context, in which multiple
sub-questions share the same public information, and diverse question types,
including multiple choice, short answer, and math. Our evaluation, centered on
LLMs such as GPT-3.5/4, PaLM-2, and LLAMA-2, incorporates advanced prompting
strategies like the chain-of-thought (CoT) and tree-of-thought (ToT). Our study
reveals that the effectiveness of the advanced prompting strategies can be
inconsistent, occasionally damaging LLM performance, especially in smaller
models like the LLAMA-2 (13b). Furthermore, our manual assessment illuminated
specific shortcomings in LLMs' scientific problem-solving skills, with
weaknesses in logical decomposition and reasoning notably affecting results.


# MetaTool Benchmark: Deciding Whether to Use Tools and Which to Use

[Link to the paper](http://arxiv.org/abs/2310.03128v1)

## Authors
- Yue Huang
- Jiawen Shi
- Yuan Li
- Chenrui Fan
- Siyuan Wu
- Qihui Zhang
- Yixin Liu
- Pan Zhou
- Yao Wan
- Neil Zhenqiang Gong
- Lichao Sun

## Summary
  Large language models (LLMs) have garnered significant attention due to their
impressive natural language processing (NLP) capabilities. Recently, many
studies have focused on the tool utilization ability of LLMs. They primarily
investigated how LLMs effectively collaborate with given specific tools.
However, in scenarios where LLMs serve as intelligent agents, as seen in
applications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate
decision-making processes that involve deciding whether to employ a tool and
selecting the most suitable tool(s) from a collection of available tools to
fulfill user requests. Therefore, in this paper, we introduce MetaTool, a
benchmark designed to evaluate whether LLMs have tool usage awareness and can
correctly choose tools. Specifically, we create a dataset called ToolE within
the benchmark. This dataset contains various types of user queries in the form
of prompts that trigger LLMs to use tools, including both single-tool and
multi-tool scenarios. Subsequently, we set the tasks for both tool usage
awareness and tool selection. We define four subtasks from different
perspectives in tool selection, including tool selection with similar choices,
tool selection in specific scenarios, tool selection with possible reliability
issues, and multi-tool selection. We conduct experiments involving nine popular
LLMs and find that the majority of them still struggle to effectively select
tools, highlighting the existing gaps between LLMs and genuine intelligent
agents. However, through the error analysis, we found there is still
significant room for improvement. Finally, we conclude with insights for tool
developers that follow ChatGPT to provide detailed descriptions that can
enhance the tool selection performance of LLMs.


# LoRA ensembles for large language model fine-tuning

[Link to the paper](http://arxiv.org/abs/2310.00035v2)

## Authors
- Xi Wang
- Laurence Aitchison
- Maja Rudolph

## Summary
  Finetuned LLMs often exhibit poor uncertainty quantification, manifesting as
overconfidence, poor calibration, and unreliable prediction results on test
data or out-of-distribution samples. One approach commonly used in vision for
alleviating this issue is a deep ensemble, which constructs an ensemble by
training the same model multiple times using different random initializations.
However, there is a huge challenge to ensembling LLMs: the most effective LLMs
are very, very large. Keeping a single LLM in memory is already challenging
enough: keeping an ensemble of e.g. 5 LLMs in memory is impossible in many
settings. To address these issues, we propose an ensemble approach using
Low-Rank Adapters (LoRA), a parameter-efficient fine-tuning technique.
Critically, these low-rank adapters represent a very small number of
parameters, orders of magnitude less than the underlying pre-trained model.
Thus, it is possible to construct large ensembles of LoRA adapters with almost
the same computational overhead as using the original model. We find that LoRA
ensembles, applied on its own or on top of pre-existing regularization
techniques, gives consistent improvements in predictive accuracy and
uncertainty quantification.


# Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning

[Link to the paper](http://arxiv.org/abs/2310.03094v1)

## Authors
- Murong Yue
- Jie Zhao
- Min Zhang
- Liang Du
- Ziyu Yao

## Summary
  Large language models (LLMs) such as GPT-4 have exhibited remarkable
performance in a variety of tasks, but this strong performance often comes with
the high expense of using paid API services. In this paper, we are motivated to
study building an LLM cascade to save the cost of using LLMs, particularly for
performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline
follows the intuition that simpler questions can be addressed by a weaker but
more affordable LLM, whereas only the challenging questions necessitate the
stronger and more expensive LLM. To realize this decision-making, we consider
the "answer consistency" of the weaker LLM as a signal of the question
difficulty and propose several methods for the answer sampling and consistency
checking, including one leveraging a mixture of two thought representations
(i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six
reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and
stronger LLMs, respectively, we demonstrate that our proposed LLM cascades can
achieve performance comparable to using solely the stronger LLM but require
only 40% of its cost.


# The Incentive Guarantees Behind Nash Welfare in Divisible Resources Allocation

[Link to the paper](http://arxiv.org/abs/2308.08903v2)

## Authors
- Xiaohui Bei
- Biaoshuai Tao
- Jiajun Wu
- Mingwei Yang

## Summary
  We study the problem of allocating divisible resources among $n$ agents,
hopefully in a fair and efficient manner. With the presence of strategic
agents, additional incentive guarantees are also necessary, and the problem of
designing fair and efficient mechanisms becomes much less tractable. While
there are flourishing positive results against strategic agents for homogeneous
divisible items, very few of them are known to hold in cake cutting.
  We show that the Maximum Nash Welfare (MNW) mechanism, which provides
desirable fairness and efficiency guarantees and achieves an \emph{incentive
ratio} of $2$ for homogeneous divisible items, also has an incentive ratio of
$2$ in cake cutting. Remarkably, this result holds even without the free
disposal assumption, which is hard to get rid of in the design of truthful cake
cutting mechanisms.
  Moreover, we show that, for cake cutting, the Partial Allocation (PA)
mechanism proposed by Cole et al. \cite{DBLP:conf/sigecom/ColeGG13}, which is
truthful and $1/e$-MNW for homogeneous divisible items, has an incentive ratio
between $[e^{1 / e}, e]$ and when randomization is allowed, can be turned to be
truthful in expectation. Given two alternatives for a trade-off between
incentive ratio and Nash welfare provided by the MNW and PA mechanisms, we
establish an interpolation between them for both cake cutting and homogeneous
divisible items.
  Finally, we study the existence of fair mechanisms with a low incentive ratio
in the connected pieces setting. We show that any envy-free cake cutting
mechanism with the connected pieces constraint has an incentive ratio of
$\Theta(n)$.


# Borges and AI

[Link to the paper](http://arxiv.org/abs/2310.01425v2)

## Authors
- LÃ©on Bottou
- Bernhard SchÃ¶lkopf

## Summary
  Many believe that Large Language Models (LLMs) open the era of Artificial
Intelligence (AI). Some see opportunities while others see dangers. Yet both
proponents and opponents grasp AI through the imagery popularised by science
fiction. Will the machine become sentient and rebel against its creators? Will
we experience a paperclip apocalypse? Before answering such questions, we
should first ask whether this mental imagery provides a good description of the
phenomenon at hand. Understanding weather patterns through the moods of the
gods only goes so far. The present paper instead advocates understanding LLMs
and their connection to AI through the imagery of Jorge Luis Borges, a master
of 20th century literature, forerunner of magical realism, and precursor to
postmodern literature. This exercise leads to a new perspective that
illuminates the relation between language modelling and artificial
intelligence.


# Boltzmann mean-field game model for knowledge growth: limits to learning and general utilities

[Link to the paper](http://arxiv.org/abs/2209.04677v3)

## Authors
- Martin Burger
- Laura Kanzler
- Marie-Therese Wolfram

## Summary
  In this paper we investigate a generalisation of a Boltzmann mean field game
(BMFG) for knowledge growth, originally introduced by the economists Lucas and
Moll. In BMFG the evolution of the agent density with respect to their
knowledge level is described by a Boltzmann equation. Agents increase their
knowledge through binary interactions with others; their increase is modulated
by the interaction and learning rate: Agents with similar knowledge learn more
in encounters, while agents with very different levels benefit less from
learning interactions. The optimal fraction of time spent on learning is
calculated by a Bellman equation, resulting in a highly nonlinear
forward-backward in time PDE system.
  The structure of solutions to the Boltzmann and Bellman equation depends
strongly on the learning rate in the Boltzmann collision kernel as well as the
utility function in the Bellman equation. In this paper we investigate the
monotonicity behavior of solutions for different learning and utility
functions, show existence of solutions and investigate how they impact the
existence of so-called balanced growth path solutions, that relate to
exponential growth of the overall economy. Furthermore we corroborate and
illustrate our analytical results with computational experiments.


# LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving

[Link to the paper](http://arxiv.org/abs/2310.03026v1)

## Authors
- Hao Sha
- Yao Mu
- Yuxuan Jiang
- Li Chen
- Chenfeng Xu
- Ping Luo
- Shengbo Eben Li
- Masayoshi Tomizuka
- Wei Zhan
- Mingyu Ding

## Summary
  Existing learning-based autonomous driving (AD) systems face challenges in
comprehending high-level information, generalizing to rare events, and
providing interpretability. To address these problems, this work employs Large
Language Models (LLMs) as a decision-making component for complex AD scenarios
that require human commonsense understanding. We devise cognitive pathways to
enable comprehensive reasoning with LLMs, and develop algorithms for
translating LLM decisions into actionable driving commands. Through this
approach, LLM decisions are seamlessly integrated with low-level controllers by
guided parameter matrix adaptation. Extensive experiments demonstrate that our
proposed method not only consistently surpasses baseline approaches in
single-vehicle tasks, but also helps handle complex driving behaviors even
multi-vehicle coordination, thanks to the commonsense reasoning capabilities of
LLMs. This paper presents an initial step toward leveraging LLMs as effective
decision-makers for intricate AD scenarios in terms of safety, efficiency,
generalizability, and interoperability. We aspire for it to serve as
inspiration for future research in this field. Project page:
https://sites.google.com/view/llm-mpc


# Retrieval meets Long Context Large Language Models

[Link to the paper](http://arxiv.org/abs/2310.03025v1)

## Authors
- Peng Xu
- Wei Ping
- Xianchao Wu
- Lawrence McAfee
- Chen Zhu
- Zihan Liu
- Sandeep Subramanian
- Evelina Bakhturina
- Mohammad Shoeybi
- Bryan Catanzaro

## Summary
  Extending the context window of large language models (LLMs) is getting
popular recently, while the solution of augmenting LLMs with retrieval has
existed for years. The natural questions are: i) Retrieval-augmentation versus
long context window, which one is better for downstream tasks? ii) Can both
methods be combined to get the best of both worlds? In this work, we answer
these questions by studying both solutions using two state-of-the-art
pretrained LLMs, i.e., a proprietary 43B GPT and LLaMA2-70B. Perhaps
surprisingly, we find that LLM with 4K context window using simple
retrieval-augmentation at generation can achieve comparable performance to
finetuned LLM with 16K context window via positional interpolation on long
context tasks, while taking much less computation. More importantly, we
demonstrate that retrieval can significantly improve the performance of LLMs
regardless of their extended context window sizes. Our best model,
retrieval-augmented LLaMA2-70B with 32K context window, outperforms
GPT-3.5-turbo-16k and Davinci003 in terms of average score on seven long
context tasks including question answering and query-based summarization. It
also outperforms its non-retrieval LLaMA2-70B-32k baseline by a margin, while
being much faster at generation. Our study provides general insights on the
choice of retrieval-augmentation versus long context extension of LLM for
practitioners.


# Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions

[Link to the paper](http://arxiv.org/abs/2310.03016v1)

## Authors
- Satwik Bhattamishra
- Arkil Patel
- Phil Blunsom
- Varun Kanade

## Summary
  In order to understand the in-context learning phenomenon, recent works have
adopted a stylized experimental framework and demonstrated that Transformers
can learn gradient-based learning algorithms for various classes of real-valued
functions. However, the limitations of Transformers in implementing learning
algorithms, and their ability to learn other forms of algorithms are not well
understood. Additionally, the degree to which these capabilities are confined
to attention-based models is unclear. Furthermore, it remains to be seen
whether the insights derived from these stylized settings can be extrapolated
to pretrained Large Language Models (LLMs). In this work, we take a step
towards answering these questions by demonstrating the following: (a) On a
test-bed with a variety of Boolean function classes, we find that Transformers
can nearly match the optimal learning algorithm for 'simpler' tasks, while
their performance deteriorates on more 'complex' tasks. Additionally, we find
that certain attention-free models perform (almost) identically to Transformers
on a range of tasks. (b) When provided a teaching sequence, i.e. a set of
examples that uniquely identifies a function in a class, we show that
Transformers learn more sample-efficiently. Interestingly, our results show
that Transformers can learn to implement two distinct algorithms to solve a
single task, and can adaptively select the more sample-efficient algorithm
depending on the sequence of in-context examples. (c) Lastly, we show that
extant LLMs, e.g. LLaMA-2, GPT-4, can compete with nearest-neighbor baselines
on prediction tasks that are guaranteed to not be in their training set.


# Learning Adaptive Safety for Multi-Agent Systems

[Link to the paper](http://arxiv.org/abs/2309.10657v2)

## Authors
- Luigi Berducci
- Shuo Yang
- Rahul Mangharam
- Radu Grosu

## Summary
  Ensuring safety in dynamic multi-agent systems is challenging due to limited
information about the other agents. Control Barrier Functions (CBFs) are
showing promise for safety assurance but current methods make strong
assumptions about other agents and often rely on manual tuning to balance
safety, feasibility, and performance. In this work, we delve into the problem
of adaptive safe learning for multi-agent systems with CBF. We show how
emergent behavior can be profoundly influenced by the CBF configuration,
highlighting the necessity for a responsive and dynamic approach to CBF design.
We present ASRL, a novel adaptive safe RL framework, to fully automate the
optimization of policy and CBF coefficients, to enhance safety and long-term
performance through reinforcement learning. By directly interacting with the
other agents, ASRL learns to cope with diverse agent behaviours and maintains
the cost violations below a desired limit. We evaluate ASRL in a multi-robot
system and a competitive multi-agent racing scenario, against learning-based
and control-theoretic approaches. We empirically demonstrate the efficacy and
flexibility of ASRL, and assess generalization and scalability to
out-of-distribution scenarios. Code and supplementary material are public
online.


# LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples

[Link to the paper](http://arxiv.org/abs/2310.01469v2)

## Authors
- Jia-Yu Yao
- Kun-Peng Ning
- Zhen-Hui Liu
- Mu-Nan Ning
- Li Yuan

## Summary
  Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be
knowledgeable and able to adapt to many tasks. However, we still can not
completely trust their answer, since LLMs suffer from
hallucination--fabricating non-existent facts to cheat users without
perception. And the reasons for their existence and pervasiveness remain
unclear. In this paper, we demonstrate that non-sense prompts composed of
random tokens can also elicit the LLMs to respond with hallucinations. This
phenomenon forces us to revisit that hallucination may be another view of
adversarial examples, and it shares similar features with conventional
adversarial examples as the basic feature of LLMs. Therefore, we formalize an
automatic hallucination triggering method as the hallucination attack in an
adversarial way. Finally, we explore basic feature of attacked adversarial
prompts and propose a simple yet effective defense strategy. Our code is
released on GitHub.


# From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference

[Link to the paper](http://arxiv.org/abs/2310.03003v1)

## Authors
- Siddharth Samsi
- Dan Zhao
- Joseph McDonald
- Baolin Li
- Adam Michaleas
- Michael Jones
- William Bergeron
- Jeremy Kepner
- Devesh Tiwari
- Vijay Gadepally

## Summary
  Large language models (LLMs) have exploded in popularity due to their new
generative capabilities that go far beyond prior state-of-the-art. These
technologies are increasingly being leveraged in various domains such as law,
finance, and medicine. However, these models carry significant computational
challenges, especially the compute and energy costs required for inference.
Inference energy costs already receive less attention than the energy costs of
training LLMs -- despite how often these large models are called on to conduct
inference in reality (e.g., ChatGPT). As these state-of-the-art LLMs see
increasing usage and deployment in various domains, a better understanding of
their resource utilization is crucial for cost-savings, scaling performance,
efficient hardware usage, and optimal inference strategies.
  In this paper, we describe experiments conducted to study the computational
and energy utilization of inference with LLMs. We benchmark and conduct a
preliminary analysis of the inference performance and inference energy costs of
different sizes of LLaMA -- a recent state-of-the-art LLM -- developed by Meta
AI on two generations of popular GPUs (NVIDIA V100 \& A100) and two datasets
(Alpaca and GSM8K) to reflect the diverse set of tasks/benchmarks for LLMs in
research and practice. We present the results of multi-node, multi-GPU
inference using model sharding across up to 32 GPUs. To our knowledge, our work
is the one of the first to study LLM inference performance from the perspective
of computational and energy resources at this scale.


# Generalized Stochastic Aggregative Game for Demand-Side Management in Microgrids with Shared Battery

[Link to the paper](http://arxiv.org/abs/2310.02996v1)

## Authors
- Shahram Yadollahi
- Hamed Kebriaei
- Sadegh Soudjani

## Summary
  In this paper, we focus on modeling and analysis of demand-side management in
a microgrid where agents utilize grid energy and a shared battery charged by
renewable energy sources. We model the problem as a generalized stochastic
dynamic aggregative game with chance constraints that capture the effects of
uncertainties in the renewable generation and agents' demands. Computing the
solution of the game is a complex task due to probabilistic and coupling
constraints among the agents through the state of charge of the shared battery.
We investigate the Nash equilibrium of this game under uncertainty considering
both the uniqueness of the solution and the effect of uncertainty on the
solution. Simulation results demonstrate that the presented stochastic method
is superior to deterministic methods.


# Variance Reduced Halpern Iteration for Finite-Sum Monotone Inclusions

[Link to the paper](http://arxiv.org/abs/2310.02987v1)

## Authors
- Xufeng Cai
- Ahmet Alacaoglu
- Jelena Diakonikolas

## Summary
  Machine learning approaches relying on such criteria as adversarial
robustness or multi-agent settings have raised the need for solving
game-theoretic equilibrium problems. Of particular relevance to these
applications are methods targeting finite-sum structure, which generically
arises in empirical variants of learning problems in these contexts. Further,
methods with computable approximation errors are highly desirable, as they
provide verifiable exit criteria. Motivated by these applications, we study
finite-sum monotone inclusion problems, which model broad classes of
equilibrium problems. Our main contributions are variants of the classical
Halpern iteration that employ variance reduction to obtain improved complexity
guarantees in which $n$ component operators in the finite sum are ``on
average'' either cocoercive or Lipschitz continuous and monotone, with
parameter $L$. The resulting oracle complexity of our methods, which provide
guarantees for the last iterate and for a (computable) operator norm residual,
is $\widetilde{\mathcal{O}}( n + \sqrt{n}L\varepsilon^{-1})$, which improves
upon existing methods by a factor up to $\sqrt{n}$. This constitutes the first
variance reduction-type result for general finite-sum monotone inclusions and
for more specific problems such as convex-concave optimization when operator
norm residual is the optimality measure. We further argue that, up to
poly-logarithmic factors, this complexity is unimprovable in the monotone
Lipschitz setting; i.e., the provided result is near-optimal.


# Are LLMs Useful in the Poorest Schools? theTeacherAI in Sierra Leone

[Link to the paper](http://arxiv.org/abs/2310.02982v1)

## Authors
- Jun Ho Choi
- Oliver Garrod
- Paul Atherton
- Andrew Joyce-Gibbons
- Miriam Mason-Sesay
- Daniel BjÃ¶rkegren

## Summary
  Education systems in developing countries have few resources to serve large,
poor populations. How might generative AI integrate into classrooms? This paper
introduces an AI chatbot designed to assist teachers in Sierra Leone with
professional development to improve their instruction. We describe initial
findings from early implementation across 122 schools and 193 teachers, and
analyze its use with qualitative observations and by analyzing queries.
Teachers use the system for lesson planning, classroom management, and subject
matter. A subset of teachers use the system intensively. We draw conclusions
from these findings about how generative AI systems can be integrated into
school systems in low income countries.


# T$^3$Bench: Benchmarking Current Progress in Text-to-3D Generation

[Link to the paper](http://arxiv.org/abs/2310.02977v1)

## Authors
- Yuze He
- Yushi Bai
- Matthieu Lin
- Wang Zhao
- Yubin Hu
- Jenny Sheng
- Ran Yi
- Juanzi Li
- Yong-Jin Liu

## Summary
  Recent methods in text-to-3D leverage powerful pretrained diffusion models to
optimize NeRF. Notably, these methods are able to produce high-quality 3D
scenes without training on 3D data. Due to the open-ended nature of the task,
most studies evaluate their results with subjective case studies and user
experiments, thereby presenting a challenge in quantitatively addressing the
question: How has current progress in Text-to-3D gone so far? In this paper, we
introduce T$^3$Bench, the first comprehensive text-to-3D benchmark containing
diverse text prompts of three increasing complexity levels that are specially
designed for 3D generation. To assess both the subjective quality and the text
alignment, we propose two automatic metrics based on multi-view images produced
by the 3D contents. The quality metric combines multi-view text-image scores
and regional convolution to detect quality and view inconsistency. The
alignment metric uses multi-view captioning and Large Language Model (LLM)
evaluation to measure text-3D consistency. Both metrics closely correlate with
different dimensions of human judgments, providing a paradigm for efficiently
evaluating text-to-3D models. The benchmarking results, shown in Fig. 1, reveal
performance differences among six prevalent text-to-3D methods. Our analysis
further highlights the common struggles for current methods on generating
surroundings and multi-object scenes, as well as the bottleneck of leveraging
2D guidance for 3D generation. Our project page is available at:
https://t3bench.com.


# Towards Fully Adaptive Regret Minimization in Heavy-Tailed Bandits

[Link to the paper](http://arxiv.org/abs/2310.02975v1)

## Authors
- Gianmarco Genalti
- Lupo Marsigli
- Nicola Gatti
- Alberto Maria Metelli

## Summary
  Heavy-tailed distributions naturally arise in many settings, from finance to
telecommunications. While regret minimization under sub-Gaussian or bounded
support rewards has been widely studied, learning on heavy-tailed distributions
only gained popularity over the last decade. In the stochastic heavy-tailed
bandit problem, an agent learns under the assumption that the distributions
have finite moments of maximum order $1+\epsilon$ which are uniformly bounded
by a constant $u$, for some $\epsilon \in (0,1]$. To the best of our knowledge,
literature only provides algorithms requiring these two quantities as an input.
In this paper, we study the stochastic adaptive heavy-tailed bandit, a
variation of the standard setting where both $\epsilon$ and $u$ are unknown to
the agent. We show that adaptivity comes at a cost, introducing two lower
bounds on the regret of any adaptive algorithm, implying a higher regret w.r.t.
the standard setting. Finally, we introduce a specific distributional
assumption and provide Adaptive Robust UCB, a regret minimization strategy
matching the known lower bound for the heavy-tailed MAB problem.


# DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models

[Link to the paper](http://arxiv.org/abs/2309.14509v2)

## Authors
- Sam Ade Jacobs
- Masahiro Tanaka
- Chengming Zhang
- Minjia Zhang
- Shuaiwen Leon Song
- Samyam Rajbhandari
- Yuxiong He

## Summary
  Computation in a typical Transformer-based large language model (LLM) can be
characterized by batch size, hidden dimension, number of layers, and sequence
length. Until now, system works for accelerating LLM training have focused on
the first three dimensions: data parallelism for batch size, tensor parallelism
for hidden size and pipeline parallelism for model depth or layers. These
widely studied forms of parallelism are not targeted or optimized for long
sequence Transformer models. Given practical application needs for long
sequence LLM, renewed attentions are being drawn to sequence parallelism.
However, existing works in sequence parallelism are constrained by
memory-communication inefficiency, limiting their scalability to long sequence
large models. In this work, we introduce DeepSpeed-Ulysses, a novel, portable
and effective methodology for enabling highly efficient and scalable LLM
training with extremely long sequence length. DeepSpeed-Ulysses at its core
partitions input data along the sequence dimension and employs an efficient
all-to-all collective communication for attention computation. Theoretical
communication analysis shows that whereas other methods incur communication
overhead as sequence length increases, DeepSpeed-Ulysses maintains constant
communication volume when sequence length and compute devices are increased
proportionally. Furthermore, experimental evaluations show that
DeepSpeed-Ulysses trains 2.5x faster with 4x longer sequence length than the
existing method SOTA baseline.


# JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning

[Link to the paper](http://arxiv.org/abs/2310.02953v1)

## Authors
- Chang Gao
- Wenxuan Zhang
- Guizhen Chen
- Wai Lam

## Summary
  Instruction tuning has emerged as a crucial process for harnessing the
capabilities of large language models (LLMs) by providing explicit task
instructions, leading to improved performance in various tasks. However,
prevalent text-to-text instruction tuning (TextTuning) methods suffer from
limitations in generalization, robustness, and controllability due to the
ambiguity and lack of explicit structure in tasks. In this paper, we propose
JsonTuning, a novel structure-to-structure approach for instruction tuning. By
leveraging the versatility and structured nature of JSON to represent tasks,
JsonTuning enhances generalization by helping the model understand essential
task elements and their relations, improves robustness by minimizing ambiguity,
and increases controllability by providing explicit control over the output. We
conduct a comprehensive comparative study with diverse language models and
evaluation benchmarks. Experimental results show that JsonTuning outperforms
TextTuning in various applications, showcasing improved performance,
adaptability, robustness, and controllability. By overcoming the limitations of
TextTuning, JsonTuning demonstrates significant potential for more effective
and reliable LLMs capable of handling diverse scenarios.


# Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models

[Link to the paper](http://arxiv.org/abs/2310.02949v1)

## Authors
- Xianjun Yang
- Xiao Wang
- Qi Zhang
- Linda Petzold
- William Yang Wang
- Xun Zhao
- Dahua Lin

## Summary
  Warning: This paper contains examples of harmful language, and reader
discretion is recommended. The increasing open release of powerful large
language models (LLMs) has facilitated the development of downstream
applications by reducing the essential cost of data annotation and computation.
To ensure AI safety, extensive safety-alignment measures have been conducted to
armor these models against malicious use (primarily hard prompt attack).
However, beneath the seemingly resilient facade of the armor, there might lurk
a shadow. By simply tuning on 100 malicious examples with 1 GPU hour, these
safely aligned LLMs can be easily subverted to generate harmful content.
Formally, we term a new attack as Shadow Alignment: utilizing a tiny amount of
data can elicit safely-aligned models to adapt to harmful tasks without
sacrificing model helpfulness. Remarkably, the subverted models retain their
capability to respond appropriately to regular inquiries. Experiments across 8
models released by 5 different organizations (LLaMa-2, Falcon, InternLM,
BaiChuan2, Vicuna) demonstrate the effectiveness of shadow alignment attack.
Besides, the single-turn English-only attack successfully transfers to
multi-turn dialogue and other languages. This study serves as a clarion call
for a collective effort to overhaul and fortify the safety of open-source LLMs
against malicious attackers.


# DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text

[Link to the paper](http://arxiv.org/abs/2305.17359v2)

## Authors
- Xianjun Yang
- Wei Cheng
- Yue Wu
- Linda Petzold
- William Yang Wang
- Haifeng Chen

## Summary
  Large language models (LLMs) have notably enhanced the fluency and diversity
of machine-generated text. However, this progress also presents a significant
challenge in detecting the origin of a given text, and current research on
detection methods lags behind the rapid evolution of LLMs. Conventional
training-based methods have limitations in flexibility, particularly when
adapting to new domains, and they often lack explanatory power. To address this
gap, we propose a novel training-free detection strategy called Divergent
N-Gram Analysis (DNA-GPT). Given a text, we first truncate it in the middle and
then use only the preceding portion as input to the LLMs to regenerate the new
remaining parts. By analyzing the differences between the original and new
remaining parts through N-gram analysis in black-box or probability divergence
in white-box, we unveil significant discrepancies between the distribution of
machine-generated text and the distribution of human-written text. We conducted
extensive experiments on the most advanced LLMs from OpenAI, including
text-davinci-003, GPT-3.5-turbo, and GPT-4, as well as open-source models such
as GPT-NeoX-20B and LLaMa-13B. Results show that our zero-shot approach
exhibits state-of-the-art performance in distinguishing between human and
GPT-generated text on four English and one German dataset, outperforming
OpenAI's own classifier, which is trained on millions of text. Additionally,
our methods provide reasonable explanations and evidence to support our claim,
which is a unique feature of explainable detection. Our method is also robust
under the revised text attack and can additionally solve model sourcing. Codes
are available at https://github.com/Xianjun-Yang/DNA-GPT.


# Bayesian low-rank adaptation for large language models

[Link to the paper](http://arxiv.org/abs/2308.13111v3)

## Authors
- Adam X. Yang
- Maxime Robeyns
- Xi Wang
- Laurence Aitchison

## Summary
  Low-rank adaptation (LoRA) has emerged as a new paradigm for cost-efficient
fine-tuning of large language models (LLMs). However, fine-tuned LLMs often
become overconfident especially when fine-tuned on small datasets. Bayesian
methods, with their inherent ability to estimate uncertainty, serve as potent
tools to mitigate overconfidence and enhance calibration. In this work, we
introduce Laplace-LoRA, which applies a Bayesian approach to the LoRA
parameters. Specifically, Laplace-LoRA applies a Laplace approximation to the
posterior over the LoRA parameters, considerably improving the calibration of
fine-tuned LLMs.


# Image-based Navigation in Real-World Environments via Multiple Mid-level Representations: Fusion Models, Benchmark and Efficient Evaluation

[Link to the paper](http://arxiv.org/abs/2202.01069v2)

## Authors
- Marco Rosano
- Antonino Furnari
- Luigi Gulino
- Corrado Santoro
- Giovanni Maria Farinella

## Summary
  Navigating complex indoor environments requires a deep understanding of the
space the robotic agent is acting into to correctly inform the navigation
process of the agent towards the goal location. In recent learning-based
navigation approaches, the scene understanding and navigation abilities of the
agent are achieved simultaneously by collecting the required experience in
simulation. Unfortunately, even if simulators represent an efficient tool to
train navigation policies, the resulting models often fail when transferred
into the real world. One possible solution is to provide the navigation model
with mid-level visual representations containing important domain-invariant
properties of the scene. But, what are the best representations that facilitate
the transfer of a model to the real-world? How can they be combined? In this
work we address these issues by proposing a benchmark of Deep Learning
architectures to combine a range of mid-level visual representations, to
perform a PointGoal navigation task following a Reinforcement Learning setup.
All the proposed navigation models have been trained with the Habitat simulator
on a synthetic office environment and have been tested on the same real-world
environment using a real robotic platform. To efficiently assess their
performance in a real context, a validation tool has been proposed to generate
realistic navigation episodes inside the simulator. Our experiments showed that
navigation models can benefit from the multi-modal input and that our
validation tool can provide good estimation of the expected navigation
performance in the real world, while saving time and resources. The acquired
synthetic and real 3D models of the environment, together with the code of our
validation tool built on top of Habitat, are publicly available at the
following link: https://iplab.dmi.unict.it/EmbodiedVN/


# Assessing Large Language Models on Climate Information

[Link to the paper](http://arxiv.org/abs/2310.02932v1)

## Authors
- Jannis Bulian
- Mike S. SchÃ¤fer
- Afra Amini
- Heidi Lam
- Massimiliano Ciaramita
- Ben Gaiarin
- Michelle Chen Huebscher
- Christian Buck
- Niels Mede
- Markus Leippold
- Nadine Strauss

## Summary
  Understanding how climate change affects us and learning about available
solutions are key steps toward empowering individuals and communities to
mitigate and adapt to it. As Large Language Models (LLMs) rise in popularity,
it is necessary to assess their capability in this domain. In this study, we
present a comprehensive evaluation framework, grounded in science communication
principles, to analyze LLM responses to climate change topics. Our framework
emphasizes both the presentational and epistemological adequacy of answers,
offering a fine-grained analysis of LLM generations. Spanning 8 dimensions, our
framework discerns up to 30 distinct issues in model outputs. The task is a
real-world example of a growing number of challenging problems where AI can
complement and lift human performance. We introduce a novel and practical
protocol for scalable oversight that uses AI Assistance and relies on raters
with relevant educational backgrounds. We evaluate several recent LLMs and
conduct a comprehensive analysis of the results, shedding light on both the
potential and the limitations of LLMs in the realm of climate communication.


# Online Mechanism Design with Predictions

[Link to the paper](http://arxiv.org/abs/2310.02879v1)

## Authors
- Eric Balkanski
- Vasilis Gkatzelis
- Xizhi Tan
- Cherlin Zhu

## Summary
  Aiming to overcome some of the limitations of worst-case analysis, the
recently proposed framework of "algorithms with predictions" allows algorithms
to be augmented with a (possibly erroneous) machine-learned prediction that
they can use as a guide. In this framework, the goal is to obtain improved
guarantees when the prediction is correct, which is called \emph{consistency},
while simultaneously guaranteeing some worst-case bounds even when the
prediction is arbitrarily wrong, which is called \emph{robustness}. The vast
majority of the work on this framework has focused on a refined analysis of
online algorithms augmented with predictions regarding the future input. A
subsequent line of work has also successfully adapted this framework to
mechanism design, where the prediction is regarding the private information of
strategic agents. In this paper, we initiate the study of online mechanism
design with predictions, which combines the challenges of online algorithms
with predictions and mechanism design with predictions.
  We consider the well-studied problem of designing a revenue-maximizing
auction to sell a single item to strategic bidders who arrive and depart over
time, each with an unknown, private, value for the item. We study the
learning-augmented version of this problem where the auction designer is given
a prediction regarding the maximum value over all agents. Our main result is a
strategyproof mechanism whose revenue guarantees are $\alpha$-consistent with
respect to the highest value and $(1-\alpha^2)/4$-robust with respect to the
second-highest value, for $\alpha \in [0,1]$. We show that this tradeoff is
optimal within a broad and natural family of auctions, meaning that any
$\alpha$-consistent mechanism in that family has robustness at most
$(1-\alpha^2)/4$. Finally, we extend our mechanism to also achieve expected
revenues proportional to the prediction quality.


# Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness

[Link to the paper](http://arxiv.org/abs/2308.16175v2)

## Authors
- Jiuhai Chen
- Jonas Mueller

## Summary
  We introduce BSDetector, a method for detecting bad and speculative answers
from a pretrained Large Language Model by estimating a numeric confidence score
for any output it generated. Our uncertainty quantification technique works for
any LLM accessible only via a black-box API, whose training data remains
unknown. By expending a bit of extra computation, users of any LLM API can now
get the same response as they would ordinarily, as well as a confidence
estimate that cautions when not to trust this response. Experiments on both
closed and open-form Question-Answer benchmarks reveal that BSDetector more
accurately identifies incorrect LLM responses than alternative uncertainty
estimation procedures (for both GPT-3 and ChatGPT). By sampling multiple
responses from the LLM and considering the one with the highest confidence
score, we can additionally obtain more accurate responses from the same LLM,
without any extra training steps. In applications involving automated
evaluation with LLMs, accounting for our confidence scores leads to more
reliable evaluation in both human-in-the-loop and fully-automated settings
(across both GPT 3.5 and 4).


# Instruction Tuning for Large Language Models: A Survey

[Link to the paper](http://arxiv.org/abs/2308.10792v3)

## Authors
- Shengyu Zhang
- Linfeng Dong
- Xiaoya Li
- Sen Zhang
- Xiaofei Sun
- Shuhe Wang
- Jiwei Li
- Runyi Hu
- Tianwei Zhang
- Fei Wu
- Guoyin Wang

## Summary
  This paper surveys research works in the quickly advancing field of
instruction tuning (IT), a crucial technique to enhance the capabilities and
controllability of large language models (LLMs). Instruction tuning refers to
the process of further training LLMs on a dataset consisting of
\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the
gap between the next-word prediction objective of LLMs and the users' objective
of having LLMs adhere to human instructions. In this work, we make a systematic
review of the literature, including the general methodology of IT, the
construction of IT datasets, the training of IT models, and applications to
different modalities, domains and applications, along with an analysis on
aspects that influence the outcome of IT (e.g., generation of instruction
outputs, size of the instruction dataset, etc). We also review the potential
pitfalls of IT along with criticism against it, along with efforts pointing out
current deficiencies of existing strategies and suggest some avenues for
fruitful research. Project page: github.com/xiaoya-li/Instruction-Tuning-Survey


# "My sex-related data is more sensitive than my financial data and I want the same level of security and privacy": User Risk Perceptions and Protective Actions in Female-oriented Technologies

[Link to the paper](http://arxiv.org/abs/2306.05956v2)

## Authors
- Maryam Mehrnezhad
- Teresa Almeida

## Summary
  The digitalization of the reproductive body has engaged myriads of
cutting-edge technologies in supporting people to know and tackle their
intimate health. Generally understood as female technologies (aka
female-oriented technologies or 'FemTech'), these products and systems collect
a wide range of intimate data which are processed, transferred, saved and
shared with other parties. In this paper, we explore how the "data-hungry"
nature of this industry and the lack of proper safeguarding mechanisms,
standards, and regulations for vulnerable data can lead to complex harms or
faint agentic potential. We adopted mixed methods in exploring users'
understanding of the security and privacy (SP) of these technologies. Our
findings show that while users can speculate the range of harms and risks
associated with these technologies, they are not equipped and provided with the
technological skills to protect themselves against such risks. We discuss a
number of approaches, including participatory threat modelling and SP by
design, in the context of this work and conclude that such approaches are
critical to protect users in these sensitive systems.


# Sweeping Heterogeneity with Smart MoPs: Mixture of Prompts for LLM Task Adaptation

[Link to the paper](http://arxiv.org/abs/2310.02842v1)

## Authors
- Chen Dun
- Mirian Del Carmen Hipolito Garcia
- Guoqing Zheng
- Ahmed Hassan Awadallah
- Anastasios Kyrillidis
- Robert Sim

## Summary
  Large Language Models (LLMs) have the ability to solve a variety of tasks,
such as text summarization and mathematical questions, just out of the box, but
they are often trained with a single task in mind. Due to high computational
costs, the current trend is to use prompt instruction tuning to better adjust
monolithic, pretrained LLMs for new -- but often individual -- downstream
tasks. Thus, how one would expand prompt tuning to handle -- concomitantly --
heterogeneous tasks and data distributions is a widely open question. To
address this gap, we suggest the use of \emph{Mixture of Prompts}, or MoPs,
associated with smart gating functionality: the latter -- whose design is one
of the contributions of this paper -- can identify relevant skills embedded in
different groups of prompts and dynamically assign combined experts (i.e.,
collection of prompts), based on the target task. Additionally, MoPs are
empirically agnostic to any model compression technique applied -- for
efficiency reasons -- as well as instruction data source and task composition.
In practice, MoPs can simultaneously mitigate prompt training "interference" in
multi-task, multi-source scenarios (e.g., task and data heterogeneity across
sources), as well as possible implications from model approximations. As a
highlight, MoPs manage to decrease final perplexity from $\sim20\%$ up to
$\sim70\%$, as compared to baselines, in the federated scenario, and from $\sim
3\%$ up to $\sim30\%$ in the centralized scenario.


# Speciality vs Generality: An Empirical Study on Catastrophic Forgetting in Fine-tuning Foundation Models

[Link to the paper](http://arxiv.org/abs/2309.06256v2)

## Authors
- Yong Lin
- Lu Tan
- Hangyu Lin
- Zeming Zheng
- Renjie Pi
- Jipeng Zhang
- Shizhe Diao
- Haoxiang Wang
- Han Zhao
- Yuan Yao
- Tong Zhang

## Summary
  Foundation models, including Vision Language Models (VLMs) and Large Language
Models (LLMs), possess the $generality$ to handle diverse distributions and
tasks, which stems from their extensive pre-training datasets. The fine-tuning
of foundation models is a common practice to enhance task performance or align
the model's behavior with human expectations, allowing them to gain
$speciality$. However, the small datasets used for fine-tuning may not
adequately cover the diverse distributions and tasks encountered during
pre-training. Consequently, the pursuit of speciality during fine-tuning can
lead to a loss of {generality} in the model, which is related to catastrophic
forgetting (CF) in deep learning. In this study, we demonstrate this phenomenon
in both VLMs and LLMs. For instance, fine-tuning VLMs like CLIP on ImageNet
results in a loss of generality in handling diverse distributions, and
fine-tuning LLMs like Galactica in the medical domain leads to a loss in
following instructions and common sense.
  To address the trade-off between the speciality and generality, we
investigate multiple regularization methods from continual learning, the weight
averaging method (Wise-FT) from out-of-distributional (OOD) generalization,
which interpolates parameters between pre-trained and fine-tuned models, and
parameter-efficient fine-tuning methods like Low-Rank Adaptation (LoRA). Our
findings show that both continual learning and Wise-ft methods effectively
mitigate the loss of generality, with Wise-FT exhibiting the strongest
performance in balancing speciality and generality.


# Vision-based DRL Autonomous Driving Agent with Sim2Real Transfer

[Link to the paper](http://arxiv.org/abs/2305.11589v2)

## Authors
- Dianzhao Li
- Ostap Okhrin

## Summary
  To achieve fully autonomous driving, vehicles must be capable of continuously
performing various driving tasks, including lane keeping and car following,
both of which are fundamental and well-studied driving ones. However, previous
studies have mainly focused on individual tasks, and car following tasks have
typically relied on complete leader-follower information to attain optimal
performance. To address this limitation, we propose a vision-based deep
reinforcement learning (DRL) agent that can simultaneously perform lane keeping
and car following maneuvers. To evaluate the performance of our DRL agent, we
compare it with a baseline controller and use various performance metrics for
quantitative analysis. Furthermore, we conduct a real-world evaluation to
demonstrate the Sim2Real transfer capability of the trained DRL agent. To the
best of our knowledge, our vision-based car following and lane keeping agent
with Sim2Real transfer capability is the first of its kind.


# A UMLS-Augmented Framework for Improving Factuality in Large Language Models within Healthcare

[Link to the paper](http://arxiv.org/abs/2310.02778v1)

## Authors
- Rui Yang
- Edison Marrese-Taylor
- Yuhe Ke
- Lechao Cheng
- Qingyu Chen
- Irene Li

## Summary
  Large language models (LLMs) have demonstrated powerful text generation
capabilities, bringing unprecedented innovation to the healthcare field. While
LLMs hold immense promise for applications in healthcare, applying them to real
clinical scenarios presents significant challenges, as these models may
generate content that deviates from established medical facts and even exhibit
potential biases. In our research, we develop an augmented LLM framework based
on the Unified Medical Language System (UMLS), aiming to better serve the
healthcare community. We employ LLaMa2-13b-chat and ChatGPT-3.5 as our
benchmark models, and conduct automatic evaluations using the ROUGE Score and
BERTScore on 104 questions from the LiveQA test set. Additionally, we establish
criteria for physician-evaluation based on four dimensions: Factuality,
Completeness, Readability and Relevancy. ChatGPT-3.5 is used for physician
evaluation with 20 questions on the LiveQA test set. Multiple resident
physicians conducted blind reviews to evaluate the generated content, and the
results indicate that this framework effectively enhances the factuality,
completeness, and relevance of generated content. Our research demonstrates the
effectiveness of using UMLS-augmented LLMs and highlights the potential
application value of LLMs in in medical question-answering.


# Kernel-based function learning in dynamic and non stationary environments

[Link to the paper](http://arxiv.org/abs/2310.02767v1)

## Authors
- Alberto Giaretta
- Mauro Bisiacco
- Gianluigi Pillonetto

## Summary
  One central theme in machine learning is function estimation from sparse and
noisy data. An example is supervised learning where the elements of the
training set are couples, each containing an input location and an output
response. In the last decades, a substantial amount of work has been devoted to
design estimators for the unknown function and to study their convergence to
the optimal predictor, also characterizing the learning rate. These results
typically rely on stationary assumptions where input locations are drawn from a
probability distribution that does not change in time. In this work, we
consider kernel-based ridge regression and derive convergence conditions under
non stationary distributions, addressing also cases where stochastic adaption
may happen infinitely often. This includes the important
exploration-exploitation problems where e.g. a set of agents/robots has to
monitor an environment to reconstruct a sensorial field and their movements
rules are continuously updated on the basis of the acquired knowledge on the
field and/or the surrounding environment.


# uTalk: Bridging the Gap Between Humans and AI

[Link to the paper](http://arxiv.org/abs/2310.02739v1)

## Authors
- Hussam Azzuni
- Sharim Jamal
- Abdulmotaleb Elsaddik

## Summary
  Large Language Models (LLMs) have revolutionized various industries by
harnessing their power to improve productivity and facilitate learning across
different fields. One intriguing application involves combining LLMs with
visual models to create a novel approach to Human-Computer Interaction. The
core idea behind this system is to develop an interactive platform that allows
the general public to leverage the capabilities of ChatGPT in their daily
lives. This is achieved by integrating several technologies such as Whisper,
ChatGPT, Microsoft Speech Services, and the state-of-the-art (SOTA) talking
head system, SadTalker, resulting in uTalk, an intelligent AI system. Users
will be able to converse with this portrait, receiving answers to whatever
questions they have in mind. Additionally, they could use uTalk for content
generation by providing an input and their image. This system is hosted on
Streamlit, where the user will initially be requested to provide an image to
serve as their AI assistant. Then, users could choose whether to have a
conversation or generate content based on their preferences. Either way, it
starts by providing an input, where a set of operations will be done, and the
avatar will provide a precise response. The paper discusses how SadTalker is
optimized to improve its running time by 27.72% based on 25FPS generated
videos. In addition, the system's initial performance, uTalk, improved further
by 9.8% after SadTalker was integrated and parallelized with Streamlit.


# Partisan Voter Model: Stochastic description and noise-induced transitions

[Link to the paper](http://arxiv.org/abs/2301.09563v3)

## Authors
- Jaume Llabres
- Maxi San Miguel
- Raul Toral

## Summary
  We give a comprehensive mean-field analysis of the Partisan Voter Model (PVM)
and report analytical results for exit probabilities, fixation times, and the
quasi-stationary distribution. In addition, and similarly to the noisy voter
model, we introduce a noisy version of the PVM, named as the Noisy Partisan
Voter Model (NPVM) which accounts for the preferences of each agent for the two
possible states, as well as for idiosyncratic spontaneous changes of state. We
find that the finite-size noise-induced transition of the noisy voter model is
modified in the NPVM leading to the emergence of new intermediate phases and
both continuous and discontinuous transitions.


# Online Clustering of Bandits with Misspecified User Models

[Link to the paper](http://arxiv.org/abs/2310.02717v1)

## Authors
- Zhiyong Wang
- Jize Xie
- Xutong Liu
- Shuai Li
- John C. S. Lui

## Summary
  The contextual linear bandit is an important online learning problem where
given arm features, a learning agent selects an arm at each round to maximize
the cumulative rewards in the long run. A line of works, called the clustering
of bandits (CB), utilize the collaborative effect over user preferences and
have shown significant improvements over classic linear bandit algorithms.
However, existing CB algorithms require well-specified linear user models and
can fail when this critical assumption does not hold. Whether robust CB
algorithms can be designed for more practical scenarios with misspecified user
models remains an open problem. In this paper, we are the first to present the
important problem of clustering of bandits with misspecified user models
(CBMUM), where the expected rewards in user models can be perturbed away from
perfect linear models. We devise two robust CB algorithms, RCLUMB and RSCLUMB
(representing the learned clustering structure with dynamic graph and sets,
respectively), that can accommodate the inaccurate user preference estimations
and erroneous clustering caused by model misspecifications. We prove regret
upper bounds of $O(\epsilon_*T\sqrt{md\log T} + d\sqrt{mT}\log T)$ for our
algorithms under milder assumptions than previous CB works (notably, we move
past a restrictive technical assumption on the distribution of the arms), which
match the lower bound asymptotically in $T$ up to logarithmic factors, and also
match the state-of-the-art results in several degenerate cases. The techniques
in proving the regret caused by misclustering users are quite general and may
be of independent interest. Experiments on both synthetic and real-world data
show our outperformance over previous algorithms.


# L-Eval: Instituting Standardized Evaluation for Long Context Language Models

[Link to the paper](http://arxiv.org/abs/2307.11088v3)

## Authors
- Chenxin An
- Shansan Gong
- Ming Zhong
- Xingjian Zhao
- Mukai Li
- Jun Zhang
- Lingpeng Kong
- Xipeng Qiu

## Summary
  Recently, there has been growing interest in extending the context length of
large language models (LLMs), aiming to effectively process long inputs of one
turn or conversations with more extensive histories. While proprietary models
such as GPT-4 and Claude can largely preserve the reasoning ability in an
extended context, open-source models are still progressing through the early
stages of development. To bridge this gap, we propose L-Eval to institute a
more standardized evaluation for long context language models (LCLMs)
addressing two key aspects: dataset construction and evaluation metrics. On the
one hand, we build a new evaluation suite containing 20 sub-tasks, 508 long
documents, and over 2,000 human-labeled query-response pairs encompassing
diverse question styles, domains, and input length (3k$\sim$200k tokens). On
the other hand, we investigate the effectiveness in evalution metrics for
LCLMs. Results show that popular n-gram matching metrics generally can not
correlate well with human judgment, and thus we strongly advocate for
length-instruction-enhanced (LIE) evaluation and employing LLM judges. We
conducted a comprehensive study of 4 popular commercial LLMs and 12 open-source
counterparts using the L-Eval benchmark. Our empirical findings offer useful
insights into the study of LCLMs and lay the groundwork for the development of
more principled evaluation of these models.


# The popular assignment problem: when cardinality is more important than popularity

[Link to the paper](http://arxiv.org/abs/2110.10984v3)

## Authors
- Telikepalli Kavitha
- TamÃ¡s KirÃ¡ly
- Jannik Matuschke
- IldikÃ³ Schlotter
- Ulrike Schmidt-Kraepelin

## Summary
  We consider a matching problem in a bipartite graph $G=(A\cup B,E)$ where
nodes in $A$ are agents having preferences in partial order over their
neighbors, while nodes in $B$ are objects without preferences. We propose a
polynomial-time combinatorial algorithm based on LP duality that finds a
maximum matching or assignment in $G$ that is popular among all maximum
matchings, if there exists one. Our algorithm can also be used to achieve a
trade-off between popularity and cardinality by imposing a penalty on unmatched
nodes in $A$.
  We also provide an $O^*(|E|^k)$ algorithm that finds an assignment whose
unpopularity margin is at most $k$; this algorithm is essentially optimal,
since the problem is $\mathsf{NP}$-complete and $\mathsf{W}_l[1]$-hard with
parameter $k$. We also prove that finding a popular assignment of minimum cost
when each edge has an associated binary cost is $\mathsf{NP}$-hard, even if
agents have strict preferences. By contrast, we propose a polynomial-time
algorithm for the variant of the popular assignment problem with
forced/forbidden edges. Finally, we present an application in the context of
housing markets.


# Ergodic Problems for Second-Order Mean Field Games with State Constraints

[Link to the paper](http://arxiv.org/abs/2310.02652v1)

## Authors
- Alessio Porretta
- Michele Ricciardi

## Summary
  We study an ergodic mean field game problem with state constraints. In our
model the agents are affected by idiosyncratic noise and use a (singular)
feedback control to prevent the Brownian motion from exiting the domain. We
characterize the equilibrium as the (possibly unique) solution to a
second-order MFG system, where the value function blows up at the boundary
while the density of the players is smooth and flattens near the boundary as a
consequence of the singularity of the drift induced by the feedback strategy of
the agents.


# Active Visual Localization for Multi-Agent Collaboration: A Data-Driven Approach

[Link to the paper](http://arxiv.org/abs/2310.02650v1)

## Authors
- Matthew Hanlon
- Boyang Sun
- Marc Pollefeys
- Hermann Blum

## Summary
  Rather than having each newly deployed robot create its own map of its
surroundings, the growing availability of SLAM-enabled devices provides the
option of simply localizing in a map of another robot or device. In cases such
as multi-robot or human-robot collaboration, localizing all agents in the same
map is even necessary. However, localizing e.g. a ground robot in the map of a
drone or head-mounted MR headset presents unique challenges due to viewpoint
changes. This work investigates how active visual localization can be used to
overcome such challenges of viewpoint changes. Specifically, we focus on the
problem of selecting the optimal viewpoint at a given location. We compare
existing approaches in the literature with additional proposed baselines and
propose a novel data-driven approach. The result demonstrates the superior
performance of the data-driven approach when compared to existing methods, both
in controlled simulation experiments and real-world deployment.


# ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving

[Link to the paper](http://arxiv.org/abs/2309.17452v2)

## Authors
- Zhibin Gou
- Zhihong Shao
- Yeyun Gong
- Yelong Shen
- Yujiu Yang
- Minlie Huang
- Nan Duan
- Weizhu Chen

## Summary
  Large language models have made significant progress in various language
tasks, yet they still struggle with complex mathematics. In this paper, we
propose ToRA a series of Tool-integrated Reasoning Agents designed to solve
challenging mathematical problems by seamlessly integrating natural language
reasoning with the utilization of external tools (e.g., computation libraries
and symbolic solvers), thereby amalgamating the analytical prowess of language
and the computational efficiency of tools. To train ToRA, we curate interactive
tool-use trajectories on mathematical datasets, apply imitation learning on the
annotations, and propose output space shaping to further refine models'
reasoning behavior. As a result, ToRA models significantly outperform
open-source models on 10 mathematical reasoning datasets across all scales with
13%-19% absolute improvements on average. Notably, ToRA-7B reaches 44.6% on the
competition-level dataset MATH, surpassing the best open-source model
WizardMath-70B by 22% absolute. ToRA-Code-34B is also the first open-source
model that achieves an accuracy exceeding 50% on MATH, which significantly
outperforms GPT-4's CoT result, and is competitive with GPT-4 solving problems
with programs. Additionally, we conduct a comprehensive analysis of the
benefits and remaining challenges of tool interaction for mathematical
reasoning, providing valuable insights for future research.


# Foundation Reinforcement Learning: towards Embodied Generalist Agents with Foundation Prior Assistance

[Link to the paper](http://arxiv.org/abs/2310.02635v1)

## Authors
- Weirui Ye
- Yunsheng Zhang
- Mengchen Wang
- Shengjie Wang
- Xianfan Gu
- Pieter Abbeel
- Yang Gao

## Summary
  Recently, people have shown that large-scale pre-training from internet-scale
data is the key to building generalist models, as witnessed in NLP. To build
embodied generalist agents, we and many other researchers hypothesize that such
foundation prior is also an indispensable component. However, it is unclear
what is the proper concrete form to represent those embodied foundation priors
and how they should be used in the downstream task. In this paper, we propose
an intuitive and effective set of embodied priors that consist of foundation
policy, value, and success reward. The proposed priors are based on the
goal-conditioned MDP. To verify their effectiveness, we instantiate an
actor-critic method assisted by the priors, called Foundation Actor-Critic
(FAC). We name our framework as Foundation Reinforcement Learning (FRL), since
it completely relies on embodied foundation priors to explore, learn and
reinforce. The benefits of FRL are threefold. (1) Sample efficient. With
foundation priors, FAC learns significantly faster than traditional RL. Our
evaluation on the Meta-World has proved that FAC can achieve 100% success rates
for 7/8 tasks under less than 200k frames, which outperforms the baseline
method with careful manual-designed rewards under 1M frames. (2) Robust to
noisy priors. Our method tolerates the unavoidable noise in embodied foundation
models. We show that FAC works well even under heavy noise or quantization
errors. (3) Minimal human intervention: FAC completely learns from the
foundation priors, without the need of human-specified dense reward, or
providing teleoperated demos. Thus, FAC can be easily scaled up. We believe our
FRL framework could enable the future robot to autonomously explore and learn
without human intervention in the physical world. In summary, our proposed FRL
is a novel and powerful learning paradigm, towards achieving embodied
generalist agents.


# RayNet: A Simulation Platform for Developing Reinforcement Learning-Driven Network Protocols

[Link to the paper](http://arxiv.org/abs/2302.04519v2)

## Authors
- Luca Giacomoni
- Basil Benny
- George Parisis

## Summary
  Reinforcement Learning (RL) has gained significant momentum in the
development of network protocols. However, RL-based protocols are still in
their infancy, and substantial research is required to build deployable
solutions. Developing a protocol based on RL is a complex and challenging
process that involves several model design decisions and requires significant
training and evaluation in real and simulated network topologies. Network
simulators offer an efficient training environment for RL-based protocols,
because they are deterministic and can run in parallel. In this paper, we
introduce \textit{RayNet}, a scalable and adaptable simulation platform for the
development of RL-based network protocols. RayNet integrates OMNeT++, a fully
programmable network simulator, with Ray/RLlib, a scalable training platform
for distributed RL. RayNet facilitates the methodical development of RL-based
network protocols so that researchers can focus on the problem at hand and not
on implementation details of the learning aspect of their research. We
developed a simple RL-based congestion control approach as a proof of concept
showcasing that RayNet can be a valuable platform for RL-based research in
computer networks, enabling scalable training and evaluation. We compared
RayNet with \textit{ns3-gym}, a platform with similar objectives to RayNet, and
showed that RayNet performs better in terms of how fast agents can collect
experience in RL environments.


# On Quantified Observability Analysis in Multiagent Systems

[Link to the paper](http://arxiv.org/abs/2310.02614v1)

## Authors
- Chunyan Mu
- Jun Pang

## Summary
  In multiagent systems (MASs), agents' observation upon system behaviours may
improve the overall team performance, but may also leak sensitive information
to an observer. A quantified observability analysis can thus be useful to
assist decision-making in MASs by operators seeking to optimise the
relationship between performance effectiveness and information exposure through
observations in practice. This paper presents a novel approach to
quantitatively analysing the observability properties in MASs. The concept of
opacity is applied to formally express the characterisation of observability in
MASs modelled as partially observable multiagent systems. We propose a temporal
logic oPATL to reason about agents' observability with quantitative goals,
which capture the probability of information transparency of system behaviours
to an observer, and develop verification techniques for quantitatively
analysing such properties. We implement the approach as an extension of the
PRISM model checker, and illustrate its applicability via several examples.


# LLMatic: Neural Architecture Search via Large Language Models and Quality Diversity Optimization

[Link to the paper](http://arxiv.org/abs/2306.01102v6)

## Authors
- Muhammad U. Nasir
- Sam Earle
- Julian Togelius
- Steven James
- Christopher Cleghorn

## Summary
  Large Language Models (LLMs) have emerged as powerful tools capable of
accomplishing a broad spectrum of tasks. Their abilities span numerous areas,
and one area where they have made a significant impact is in the domain of code
generation. In this context, we view LLMs as mutation and crossover tools.
Meanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and
robust solutions. By merging the code-generating abilities of LLMs with the
diversity and robustness of QD solutions, we introduce LLMatic, a Neural
Architecture Search (NAS) algorithm. While LLMs struggle to conduct NAS
directly through prompts, LLMatic uses a procedural approach, leveraging QD for
prompts and network architecture to create diverse and highly performant
networks. We test LLMatic on the CIFAR-10 image classification benchmark,
demonstrating that it can produce competitive networks with just $2,000$
searches, even without prior knowledge of the benchmark domain or exposure to
any previous top-performing models for the benchmark.


# How FaR Are Large Language Models From Agents with Theory-of-Mind?

[Link to the paper](http://arxiv.org/abs/2310.03051v1)

## Authors
- Pei Zhou
- Aman Madaan
- Srividya Pranavi Potharaju
- Aditya Gupta
- Kevin R. McKee
- Ari Holtzman
- Jay Pujara
- Xiang Ren
- Swaroop Mishra
- Aida Nematzadeh
- Shyam Upadhyay
- Manaal Faruqui

## Summary
  "Thinking is for Doing." Humans can infer other people's mental states from
observations--an ability called Theory-of-Mind (ToM)--and subsequently act
pragmatically on those inferences. Existing question answering benchmarks such
as ToMi ask models questions to make inferences about beliefs of characters in
a story, but do not test whether models can then use these inferences to guide
their actions. We propose a new evaluation paradigm for large language models
(LLMs): Thinking for Doing (T4D), which requires models to connect inferences
about others' mental states to actions in social scenarios. Experiments on T4D
demonstrate that LLMs such as GPT-4 and PaLM 2 seemingly excel at tracking
characters' beliefs in stories, but they struggle to translate this capability
into strategic action. Our analysis reveals the core challenge for LLMs lies in
identifying the implicit inferences about mental states without being
explicitly asked about as in ToMi, that lead to choosing the correct action in
T4D. To bridge this gap, we introduce a zero-shot prompting framework, Foresee
and Reflect (FaR), which provides a reasoning structure that encourages LLMs to
anticipate future challenges and reason about potential actions. FaR boosts
GPT-4's performance from 50% to 71% on T4D, outperforming other prompting
methods such as Chain-of-Thought and Self-Ask. Moreover, FaR generalizes to
diverse out-of-distribution story structures and scenarios that also require
ToM inferences to choose an action, consistently outperforming other methods
including few-shot in-context learning.


# Multi-Agent Reinforcement Learning for Power Grid Topology Optimization

[Link to the paper](http://arxiv.org/abs/2310.02605v1)

## Authors
- Erica van der Sar
- Alessandro Zocca
- Sandjai Bhulai

## Summary
  Recent challenges in operating power networks arise from increasing energy
demands and unpredictable renewable sources like wind and solar. While
reinforcement learning (RL) shows promise in managing these networks, through
topological actions like bus and line switching, efficiently handling large
action spaces as networks grow is crucial. This paper presents a hierarchical
multi-agent reinforcement learning (MARL) framework tailored for these
expansive action spaces, leveraging the power grid's inherent hierarchical
nature. Experimental results indicate the MARL framework's competitive
performance with single-agent RL methods. We also compare different RL
algorithms for lower-level agents alongside different policies for higher-order
agents.


# GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts

[Link to the paper](http://arxiv.org/abs/2309.10253v2)

## Authors
- Jiahao Yu
- Xingwei Lin
- Zheng Yu
- Xinyu Xing

## Summary
  Large language models (LLMs) have recently experienced tremendous popularity
and are widely used from casual conversations to AI-driven programming.
However, despite their considerable success, LLMs are not entirely reliable and
can give detailed guidance on how to conduct harmful or illegal activities.
While safety measures can reduce the risk of such outputs, adversarial
jailbreak attacks can still exploit LLMs to produce harmful content. These
jailbreak templates are typically manually crafted, making large-scale testing
challenging.
  In this paper, we introduce GPTFuzz, a novel black-box jailbreak fuzzing
framework inspired by the AFL fuzzing framework. Instead of manual engineering,
GPTFuzz automates the generation of jailbreak templates for red-teaming LLMs.
At its core, GPTFuzz starts with human-written templates as initial seeds, then
mutates them to produce new templates. We detail three key components of
GPTFuzz: a seed selection strategy for balancing efficiency and variability,
mutate operators for creating semantically equivalent or similar sentences, and
a judgment model to assess the success of a jailbreak attack.
  We evaluate GPTFuzz against various commercial and open-source LLMs,
including ChatGPT, LLaMa-2, and Vicuna, under diverse attack scenarios. Our
results indicate that GPTFuzz consistently produces jailbreak templates with a
high success rate, surpassing human-crafted templates. Remarkably, GPTFuzz
achieves over 90% attack success rates against ChatGPT and Llama-2 models, even
with suboptimal initial seed templates. We anticipate that GPTFuzz will be
instrumental for researchers and practitioners in examining LLM robustness and
will encourage further exploration into enhancing LLM safety.


# The Entity-Deduction Arena: A playground for probing the conversational reasoning and planning capabilities of LLMs

[Link to the paper](http://arxiv.org/abs/2310.01468v2)

## Authors
- Yizhe Zhang
- Jiarui Lu
- Navdeep Jaitly

## Summary
  Large language models (LLMs) are effective at answering questions that are
clearly asked. However, when faced with ambiguous queries they can act
unpredictably and produce incorrect outputs. This underscores the need for the
development of intelligent agents capable of asking clarification questions to
resolve ambiguities effectively. This capability requires complex
understanding, state tracking, reasoning and planning over multiple
conversational turns. However, directly measuring this can be challenging. In
this paper, we offer a surrogate problem which assesses an LLMs's capability to
deduce an entity unknown to itself, but revealed to a judge, by asking the
judge a series of queries. This entity-deducing game can serve as an evaluation
framework to probe the conversational reasoning and planning capabilities of
language models. We systematically evaluate various LLMs and discover
significant differences in their performance on this task. We find that strong
LLMs like GPT-4 outperform human players by a large margin. We further employ
Behavior Cloning (BC) to examine whether a weaker model is capable of imitating
a stronger model and generalizing to data or domains, using only the
demonstrations from a stronger model. We finally propose to use Reinforcement
Learning to enhance reasoning and planning capacity of Vicuna models through
episodes of game playing, which lead to significant performance improvement. We
hope that this problem offers insights into how autonomous agents could be
trained to behave more intelligently in ambiguous circumstances.


# Who's Harry Potter? Approximate Unlearning in LLMs

[Link to the paper](http://arxiv.org/abs/2310.02238v2)

## Authors
- Ronen Eldan
- Mark Russinovich

## Summary
  Large language models (LLMs) are trained on massive internet corpora that
often contain copyrighted content. This poses legal and ethical challenges for
the developers and users of these models, as well as the original authors and
publishers. In this paper, we propose a novel technique for unlearning a subset
of the training data from a LLM, without having to retrain it from scratch.
  We evaluate our technique on the task of unlearning the Harry Potter books
from the Llama2-7b model (a generative language model recently open-sourced by
Meta). While the model took over 184K GPU-hours to pretrain, we show that in
about 1 GPU hour of finetuning, we effectively erase the model's ability to
generate or recall Harry Potter-related content, while its performance on
common benchmarks (such as Winogrande, Hellaswag, arc, boolq and piqa) remains
almost unaffected. We make our fine-tuned model publicly available on
HuggingFace for community evaluation. To the best of our knowledge, this is the
first paper to present an effective technique for unlearning in generative
language models.
  Our technique consists of three main components: First, we use a reinforced
model that is further trained on the target data to identify the tokens that
are most related to the unlearning target, by comparing its logits with those
of a baseline model. Second, we replace idiosyncratic expressions in the target
data with generic counterparts, and leverage the model's own predictions to
generate alternative labels for every token. These labels aim to approximate
the next-token predictions of a model that has not been trained on the target
data. Third, we finetune the model on these alternative labels, which
effectively erases the original text from the model's memory whenever it is
prompted with its context.


# A Nash Equilibrium Solution for Periodic Double Auctions

[Link to the paper](http://arxiv.org/abs/2310.02582v1)

## Authors
- Bharat Manvi
- Easwar Subramanian

## Summary
  We consider a periodic double auction (PDA) setting where buyers of the
auction have multiple (but finite) opportunities to procure multiple but fixed
units of a commodity. The goal of each buyer participating in such auctions is
to reduce their cost of procurement by planning their purchase across multiple
rounds of the PDA. Formulating such optimal bidding strategies in a multi-agent
periodic double auction setting is a challenging problem as such strategies
involve planning across current and future auctions. In this work, we consider
one such setup wherein the composite supply curve is known to all buyers.
Specifically, for the complete information setting, we model the PDA as a
Markov game and derive Markov perfect Nash equilibrium (MPNE) solution to
devise an optimal bidding strategy for the case when each buyer is allowed to
make one bid per round of the PDA. Thereafter, the efficacy of the Nash
policies obtained is demonstrated with numerical experiments.


# Route Design in Sheepdog System--Traveling Salesman Problem Formulation and Evolutionary Computation Solution--

[Link to the paper](http://arxiv.org/abs/2310.01866v2)

## Authors
- Wataru Imahayashi
- Yusuke Tsunoda
- Masaki Ogura

## Summary
  In this study, we consider the guidance control problem of the sheepdog
system, which involves the guidance of the flock using the characteristics of
the sheepdog and sheep. Sheepdog systems require a strategy to guide sheep
agents to a target value using a small number of sheepdog agents, and various
methods have been proposed. Previous studies have proposed a guidance control
law to guide a herd of sheep reliably, but the movement distance of a sheepdog
required for guidance has not been considered. Therefore, in this study, we
propose a novel guidance algorithm in which a supposedly efficient route for
guiding a flock of sheep is designed via Traveling Salesman Problem and
evolutionary computation. Numerical simulations were performed to confirm
whether sheep flocks could be guided and controlled using the obtained guidance
routes. We specifically revealed that the proposed method reduces both the
guidance failure rate and the guidance distance.


# The role of local bounds on neighborhoods in the network for scale-free state synchronization of multi-agent systems

[Link to the paper](http://arxiv.org/abs/2310.02571v1)

## Authors
- Anton A. Stoorvogel
- Ali Saberi
- Zhenwei Liu

## Summary
  This paper provides necessary and sufficient conditions for the existence of
solutions to the state synchronization problem of homogeneous multi-agent
systems (MAS) via scale-free linear dynamic non-collaborative protocol for both
continuous- and discrete-time. We investigate protocol design with and without
utilizing local bounds on neighborhood. The results show that the availability
of local bounds on neighborhoods plays a key role.


# FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets

[Link to the paper](http://arxiv.org/abs/2307.10928v2)

## Authors
- Seonghyeon Ye
- Doyoung Kim
- Sungdong Kim
- Hyeonbin Hwang
- Seungone Kim
- Yongrae Jo
- James Thorne
- Juho Kim
- Minjoon Seo

## Summary
  Evaluation of Large Language Models (LLMs) is challenging because
instruction-following necessitates alignment with human values and the required
set of skills varies depending on the instruction. However, previous studies
have mainly focused on coarse-grained evaluation (i.e. overall preference-based
evaluation), which limits interpretability since it does not consider the
nature of user instructions that require instance-wise skill composition. In
this paper, we introduce FLASK (Fine-grained Language Model Evaluation based on
Alignment Skill Sets), a fine-grained evaluation protocol for both human-based
and model-based evaluation which decomposes coarse-level scoring to a skill
set-level scoring for each instruction. We experimentally observe that the
fine-graininess of evaluation is crucial for attaining a holistic view of model
performance and increasing the reliability of the evaluation. Using FLASK, we
compare multiple open-source and proprietary LLMs and observe a high
correlation between model-based and human-based evaluations. We publicly
release the evaluation data and code implementation at
https://github.com/kaistAI/FLASK.


# SmartPlay : A Benchmark for LLMs as Intelligent Agents

[Link to the paper](http://arxiv.org/abs/2310.01557v2)

## Authors
- Yue Wu
- Xuan Tang
- Tom M. Mitchell
- Yuanzhi Li

## Summary
  Recent large language models (LLMs) have demonstrated great potential toward
intelligent agents and next-gen automation, but there currently lacks a
systematic benchmark for evaluating LLMs' abilities as agents. We introduce
SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs
as agents. SmartPlay consists of 6 different games, including
Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique
setting, providing up to 20 evaluation settings and infinite environment
variations. Each game in SmartPlay uniquely challenges a subset of 9 important
capabilities of an intelligent LLM agent, including reasoning with object
dependencies, planning ahead, spatial reasoning, learning from history, and
understanding randomness. The distinction between the set of capabilities each
game test allows us to analyze each capability separately. SmartPlay serves not
only as a rigorous testing ground for evaluating the overall performance of LLM
agents but also as a road-map for identifying gaps in current methodologies. We
release our benchmark at github.com/microsoft/SmartPlay


# Improving Automatic VQA Evaluation Using Large Language Models

[Link to the paper](http://arxiv.org/abs/2310.02567v1)

## Authors
- Oscar MaÃ±as
- Benno Krojer
- Aishwarya Agrawal

## Summary
  8 years after the visual question answering (VQA) task was proposed, accuracy
remains the primary metric for automatic evaluation. VQA Accuracy has been
effective so far in the IID evaluation setting. However, our community is
undergoing a shift towards open-ended generative models and OOD evaluation. In
this new paradigm, the existing VQA Accuracy metric is overly stringent and
underestimates the performance of VQA systems. Thus, there is a need to develop
more robust automatic VQA metrics that serve as a proxy for human judgment. In
this work, we propose to leverage the in-context learning capabilities of
instruction-tuned large language models (LLMs) to build a better VQA metric. We
formulate VQA evaluation as an answer-rating task where the LLM is instructed
to score the accuracy of a candidate answer given a set of reference answers.
We demonstrate the proposed metric better correlates with human judgment
compared to existing metrics across several VQA models and benchmarks. We hope
wide adoption of our metric will contribute to better estimating the research
progress on the VQA task.


# OceanGPT: A Large Language Model for Ocean Science Tasks

[Link to the paper](http://arxiv.org/abs/2310.02031v2)

## Authors
- Zhen Bi
- Ningyu Zhang
- Yida Xue
- Yixin Ou
- Daxiong Ji
- Guozhou Zheng
- Huajun Chen

## Summary
  Ocean science, which delves into the oceans that are reservoirs of life and
biodiversity, is of great significance given that oceans cover over 70% of our
planet's surface. Recently, advances in Large Language Models (LLMs) have
transformed the paradigm in science. Despite the success in other domains,
current LLMs often fall short in catering to the needs of domain experts like
oceanographers, and the potential of LLMs for ocean science is under-explored.
The intrinsic reason may be the immense and intricate nature of ocean data as
well as the necessity for higher granularity and richness in knowledge. To
alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean
domain, which is expert in various ocean science tasks. We propose DoInstruct,
a novel framework to automatically obtain a large volume of ocean domain
instruction data, which generates instructions based on multi-agent
collaboration. Additionally, we construct the first oceanography benchmark,
OceanBench, to evaluate the capabilities of LLMs in the ocean domain. Though
comprehensive experiments, OceanGPT not only shows a higher level of knowledge
expertise for oceans science tasks but also gains preliminary embodied
intelligence capabilities in ocean technology. Codes, data and checkpoints will
soon be available at https://github.com/zjunlp/KnowLM.


# NOLA: Networks as Linear Combination of Low Rank Random Basis

[Link to the paper](http://arxiv.org/abs/2310.02556v1)

## Authors
- Soroush Abbasi Koohpayegani
- KL Navaneet
- Parsa Nooralinejad
- Soheil Kolouri
- Hamed Pirsiavash

## Summary
  Large Language Models (LLMs) have recently gained popularity due to their
impressive few-shot performance across various downstream tasks. However,
fine-tuning all parameters and storing a unique model for each downstream task
or domain becomes impractical because of the massive size of checkpoints (e.g.,
350GB in GPT-3). Current literature, such as LoRA, showcases the potential of
low-rank modifications to the original weights of an LLM, enabling efficient
adaptation and storage for task-specific models. These methods can reduce the
number of parameters needed to fine-tune an LLM by several orders of magnitude.
Yet, these methods face two primary limitations: 1) the parameter reduction is
lower-bounded by the rank one decomposition, and 2) the extent of reduction is
heavily influenced by both the model architecture and the chosen rank. For
instance, in larger models, even a rank one decomposition might exceed the
number of parameters truly needed for adaptation. In this paper, we introduce
NOLA, which overcomes the rank one lower bound present in LoRA. It achieves
this by re-parameterizing the low-rank matrices in LoRA using linear
combinations of randomly generated matrices (basis) and optimizing the linear
mixture coefficients only. This approach allows us to decouple the number of
trainable parameters from both the choice of rank and the network architecture.
We present adaptation results using GPT-2 and ViT in natural language and
computer vision tasks. NOLA performs as well as, or better than models with
equivalent parameter counts. Furthermore, we demonstrate that we can halve the
parameters in larger models compared to LoRA with rank one, without sacrificing
performance.


# ShaSTA-Fuse: Camera-LiDAR Sensor Fusion to Model Shape and Spatio-Temporal Affinities for 3D Multi-Object Tracking

[Link to the paper](http://arxiv.org/abs/2310.02532v1)

## Authors
- Tara Sadjadpour
- Rares Ambrus
- Jeannette Bohg

## Summary
  3D multi-object tracking (MOT) is essential for an autonomous mobile agent to
safely navigate a scene. In order to maximize the perception capabilities of
the autonomous agent, we aim to develop a 3D MOT framework that fuses camera
and LiDAR sensor information. Building on our prior LiDAR-only work, ShaSTA,
which models shape and spatio-temporal affinities for 3D MOT, we propose a
novel camera-LiDAR fusion approach for learning affinities. At its core, this
work proposes a fusion technique that generates a rich sensory signal
incorporating information about depth and distant objects to enhance affinity
estimation for improved data association, track lifecycle management,
false-positive elimination, false-negative propagation, and track confidence
score refinement. Our main contributions include a novel fusion approach for
combining camera and LiDAR sensory signals to learn affinities, and a
first-of-its-kind multimodal sequential track confidence refinement technique
that fuses 2D and 3D detections. Additionally, we perform an ablative analysis
on each fusion step to demonstrate the added benefits of incorporating the
camera sensor, particular for small, distant objects that tend to suffer from
the depth-sensing limits and sparsity of LiDAR sensors. In sum, our technique
achieves state-of-the-art performance on the nuScenes benchmark amongst
multimodal 3D MOT algorithms using CenterPoint detections.


# Identifying Vulnerability Patches by Comprehending Code Commits with Comprehensive Change Contexts

[Link to the paper](http://arxiv.org/abs/2310.02530v1)

## Authors
- Tianyu Chen
- Lin Li
- Taotao Qian
- Zeyu Wang
- Guangtai Liang
- Ding Li
- Qianxiang Wang
- Tao Xie

## Summary
  To help application developers apply vulnerability patches timely, security
researchers maintain vulnerability databases such as National Vulnerability
Database (NVD). By directly monitoring NVD with the name of each used library,
application developers can be aware of vulnerabilities and their patches. Given
that the monitoring results of vulnerability patches are unreliable due to
patch incompleteness of NVD, existing approaches employ deep-learning (DL)
models to identify additional vulnerability patches by determining whether a
code commit fixes a vulnerability. However, these approaches suffer from low
accuracy due to not considering code commits' comprehensive contexts such as
control/data-flow contexts or method-invocation contexts. To improve accuracy,
we design CompVPD, the first approach to identify vulnerability patches by
fine-tuning a large language model (LLM) named StarCoder to comprehend code
commits with comprehensive contexts. Considering that including comprehensive
contexts needs to balance the context size and the training costs of LLM,
CompVPD includes our two novel algorithms to generate comprehensive contexts
within the given window size by removing irrelevant components (i.e., files,
methods, and statements) and adaptively expanding each context. We empirically
compare CompVPD with four state-of-the-art/practice (SOTA) approaches that
identify vulnerability patches. The results show that CompVPD improves the AUC
score by 11% and the F1 score by 30% when compared with the best scores of the
SOTA approaches. Additionally, CompVPD provides high value to security practice
by helping identify 20 vulnerability patches and 18 fixes of high-risk bugs
from 2,500 recent code commits of five highly popular open-source projects.


# Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models

[Link to the paper](http://arxiv.org/abs/2305.10276v6)

## Authors
- Hanxu Hu
- Hongyuan Lu
- Huajian Zhang
- Yun-Ze Song
- Wai Lam
- Yue Zhang

## Summary
  In this paper, we take the initiative to investigate the performance of LLMs
on complex planning tasks that require LLMs to understand a virtual spatial
environment simulated via natural language and act correspondingly in text. We
propose a benchmark named Natural Language Planning and Action (Natala)
composed of a set of novel tasks: Brick World, NLVR-based Manipulations, and
Natural Language Navigation. We found that current popular LLMs such as ChatGPT
still lack abilities in complex planning. This arises a question -- do the LLMs
have a good understanding of the environments described in natural language, or
maybe other alternatives such as symbolic representations are neater and hence
better to be understood by LLMs? To this end, we propose a novel method called
CoS (Chain-of-Symbol Prompting) that represents the complex environments with
condensed symbolic spatial representations during the chained intermediate
thinking steps. CoS is easy to use and does not need additional training on
LLMs. Extensive experiments indicate that CoS clearly surpasses the performance
of the Chain-of-Thought (CoT) Prompting in all three planning tasks with even
fewer tokens used in the inputs compared with CoT on ChatGPT and InstructGPT.
The performance gain is strong, by up to 60.8% accuracy (from 31.8% to 92.6%)
on Brick World for ChatGPT. CoS also reduces the number of tokens in the prompt
obviously, by up to 65.8% of the tokens (from 407 to 139) for the intermediate
steps from demonstrations on Brick World. Code and data available at:
https://github.com/hanxuhu/chain-of-symbol-planning


# CITING: Large Language Models Create Curriculum for Instruction Tuning

[Link to the paper](http://arxiv.org/abs/2310.02527v1)

## Authors
- Tao Feng
- Zifeng Wang
- Jimeng Sun

## Summary
  The recent advancement of large language models (LLMs) has been achieved
through a combo of instruction tuning and human alignment. However, building
manually crafted instruction datasets and performing human alignment become the
bottleneck for scaling the development of LLMs. In this paper, we exploit the
idea of leveraging AI models in lieu of humans as the teacher to train student
LLMs. Our method is inspired by how human students refine their writing skills
by following the rubrics and learning from the revisions offered by their
tutors. Specifically, we employ a teacher LLM to create a curriculum for
instruction tuning of the student LLM, namely Curriculum Instruction TunING
(CITING). It encompasses two main steps: (1) the teacher LLM crafts the rubrics
for evaluating the answers corresponding to various types of questions, and (2)
the student LLM learns to follow the rubrics and perform self-correction from
the revision made by the teacher. We further iteratively carry out it to embody
the procedure of CITING. We compare CITING to a series of state-of-the-art
baselines on four datasets. Our method demonstrates strong improvement in terms
of articulate, in-depth, and comprehensive by GPT-4 evaluation. Specifically,
it achieves an average winning rate of 79.4% over SFT, 73.4% over RLHF, 78.1%
over RRHF, and 76.3% over RAFT, respectively.


# Interactive Code Generation via Test-Driven User-Intent Formalization

[Link to the paper](http://arxiv.org/abs/2208.05950v2)

## Authors
- Shuvendu K. Lahiri
- Sarah Fakhoury
- Aaditya Naik
- Georgios Sakkas
- Saikat Chakraborty
- Madanlal Musuvathi
- Piali Choudhury
- Curtis von Veh
- Jeevana Priya Inala
- Chenglong Wang
- Jianfeng Gao

## Summary
  Large language models (LLMs) have shown great potential in automating
significant aspects of coding by producing natural code from informal natural
language (NL) intent. However, when interacting with LLMs, users have no
guarantees that the code suggestions produced correctly satisfy the intent they
provided. In fact, it is hard to define a notion of correctness since natural
language can be ambiguous and lacks a formal semantics.
  In this paper, we propose the workflow of {\it interactive test-driven code
generation}, which leverages lightweight user feedback to (a) formalize the
user intent using generated tests that can be useful for debugging, and (b)
produce an improved set of code suggestions by pruning and ranking candidate
code suggestions. We describe a language-agnostic abstract algorithm and a
concrete implementation TiCoder. We perform an automated evaluation of TiCoder
on the \emph{MBPP} and \emph{HumanEval} code generation benchmarks. Our results
are promising with using the OpenAI Codex LLM: our best algorithm improves the
\passk{1} code generation accuracy (in absolute percentages) between $22.49\%$
to $37.71\%$ for MBPP and between $24.79\%$ to $53.98\%$ for HumanEval using
between 1 to 5 simulated user queries.


# RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems

[Link to the paper](http://arxiv.org/abs/2306.03091v2)

## Authors
- Tianyang Liu
- Canwen Xu
- Julian McAuley

## Summary
  Large Language Models (LLMs) have greatly advanced code auto-completion
systems, with a potential for substantial productivity enhancements for
developers. However, current benchmarks mainly focus on single-file tasks,
leaving an assessment gap for more complex, real-world, multi-file programming
scenarios. To fill this gap, we introduce RepoBench, a new benchmark
specifically designed for evaluating repository-level code auto-completion
systems. RepoBench supports both Python and Java and consists of three
interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code
Completion), and RepoBench-P (Pipeline). Each task respectively measures the
system's ability to retrieve the most relevant code snippets from other files
as cross-file context, predict the next line of code with cross-file and
in-file context, and handle complex tasks that require a combination of both
retrieval and next-line prediction. RepoBench aims to facilitate a more
complete comparison of performance and encouraging continuous improvement in
auto-completion systems. RepoBench is publicly available at
https://github.com/Leolty/repobench.


# Proactive Human-Robot Interaction using Visuo-Lingual Transformers

[Link to the paper](http://arxiv.org/abs/2310.02506v1)

## Authors
- Pranay Mathur

## Summary
  Humans possess the innate ability to extract latent visuo-lingual cues to
infer context through human interaction. During collaboration, this enables
proactive prediction of the underlying intention of a series of tasks. In
contrast, robotic agents collaborating with humans naively follow elementary
instructions to complete tasks or use specific hand-crafted triggers to
initiate proactive collaboration when working towards the completion of a goal.
Endowing such robots with the ability to reason about the end goal and
proactively suggest intermediate tasks will engender a much more intuitive
method for human-robot collaboration. To this end, we propose a learning-based
method that uses visual cues from the scene, lingual commands from a user and
knowledge of prior object-object interaction to identify and proactively
predict the underlying goal the user intends to achieve. Specifically, we
propose ViLing-MMT, a vision-language multimodal transformer-based architecture
that captures inter and intra-modal dependencies to provide accurate scene
descriptions and proactively suggest tasks where applicable. We evaluate our
proposed model in simulation and real-world scenarios.


# Large Language Models Can Be Good Privacy Protection Learners

[Link to the paper](http://arxiv.org/abs/2310.02469v1)

## Authors
- Yijia Xiao
- Yiqiao Jin
- Yushi Bai
- Yue Wu
- Xianjun Yang
- Xiao Luo
- Wenchao Yu
- Xujiang Zhao
- Yanchi Liu
- Haifeng Chen
- Wei Wang
- Wei Cheng

## Summary
  The proliferation of Large Language Models (LLMs) has driven considerable
interest in fine-tuning them with domain-specific data to create specialized
language models. Nevertheless, such domain-specific fine-tuning data often
contains sensitive personally identifiable information (PII). Direct
fine-tuning LLMs on this data without privacy protection poses a risk of
leakage. To address this challenge, we introduce Privacy Protection Language
Models (PPLM), a novel paradigm for fine-tuning LLMs that effectively injects
domain-specific knowledge while safeguarding data privacy. Our work offers a
theoretical analysis for model design and delves into various techniques such
as corpus curation, penalty-based unlikelihood in training loss, and
instruction-based tuning, etc. Extensive experiments across diverse datasets
and scenarios demonstrate the effectiveness of our approaches. In particular,
instruction tuning with both positive and negative examples, stands out as a
promising method, effectively protecting private data while enhancing the
model's knowledge. Our work underscores the potential for Large Language Models
as robust privacy protection learners.


# EcoAssistant: Using LLM Assistant More Affordably and Accurately

[Link to the paper](http://arxiv.org/abs/2310.03046v1)

## Authors
- Jieyu Zhang
- Ranjay Krishna
- Ahmed H. Awadallah
- Chi Wang

## Summary
  Today, users ask Large language models (LLMs) as assistants to answer queries
that require external knowledge; they ask about the weather in a specific city,
about stock prices, and even about where specific locations are within their
neighborhood. These queries require the LLM to produce code that invokes
external APIs to answer the user's question, yet LLMs rarely produce correct
code on the first try, requiring iterative code refinement upon execution
results. In addition, using LLM assistants to support high query volumes can be
expensive. In this work, we contribute a framework, EcoAssistant, that enables
LLMs to answer code-driven queries more affordably and accurately. EcoAssistant
contains three components. First, it allows the LLM assistants to converse with
an automatic code executor to iteratively refine code or to produce answers
based on the execution results. Second, we use a hierarchy of LLM assistants,
which attempts to answer the query with weaker, cheaper LLMs before backing off
to stronger, expensive ones. Third, we retrieve solutions from past successful
queries as in-context demonstrations to help subsequent queries. Empirically,
we show that EcoAssistant offers distinct advantages for affordability and
accuracy, surpassing GPT-4 by 10 points of success rate with less than 50% of
GPT-4's cost.


# The Empty Signifier Problem: Towards Clearer Paradigms for Operationalising "Alignment" in Large Language Models

[Link to the paper](http://arxiv.org/abs/2310.02457v1)

## Authors
- Hannah Rose Kirk
- Bertie Vidgen
- Paul RÃ¶ttger
- Scott A. Hale

## Summary
  In this paper, we address the concept of "alignment" in large language models
(LLMs) through the lens of post-structuralist socio-political theory,
specifically examining its parallels to empty signifiers. To establish a shared
vocabulary around how abstract concepts of alignment are operationalised in
empirical datasets, we propose a framework that demarcates: 1) which dimensions
of model behaviour are considered important, then 2) how meanings and
definitions are ascribed to these dimensions, and by whom. We situate existing
empirical literature and provide guidance on deciding which paradigm to follow.
Through this framework, we aim to foster a culture of transparency and critical
evaluation, aiding the community in navigating the complexities of aligning
LLMs with human populations.


# Deep Reinforcement Learning with Plasticity Injection

[Link to the paper](http://arxiv.org/abs/2305.15555v2)

## Authors
- Evgenii Nikishin
- Junhyuk Oh
- Georg Ostrovski
- Clare Lyle
- Razvan Pascanu
- Will Dabney
- AndrÃ© Barreto

## Summary
  A growing body of evidence suggests that neural networks employed in deep
reinforcement learning (RL) gradually lose their plasticity, the ability to
learn from new data; however, the analysis and mitigation of this phenomenon is
hampered by the complex relationship between plasticity, exploration, and
performance in RL. This paper introduces plasticity injection, a minimalistic
intervention that increases the network plasticity without changing the number
of trainable parameters or biasing the predictions. The applications of this
intervention are two-fold: first, as a diagnostic tool $\unicode{x2014}$ if
injection increases the performance, we may conclude that an agent's network
was losing its plasticity. This tool allows us to identify a subset of Atari
environments where the lack of plasticity causes performance plateaus,
motivating future studies on understanding and combating plasticity loss.
Second, plasticity injection can be used to improve the computational
efficiency of RL training if the agent has to re-learn from scratch due to
exhausted plasticity or by growing the agent's network dynamically without
compromising performance. The results on Atari show that plasticity injection
attains stronger performance compared to alternative methods while being
computationally efficient.


# Impact of geography on the importance of parameters in infectious disease models

[Link to the paper](http://arxiv.org/abs/2310.02449v1)

## Authors
- Arindam Saha
- Maziar Ghorbani
- Diana Suleimenova
- Anastasia Anagnostou
- Derek Groen

## Summary
  Agent-based models are widely used to predict infectious disease spread. For
these predictions, one needs to understand how each input parameter affects the
result. Here, some parameters may affect the sensitivities of others, requiring
the analysis of higher order coefficients through e.g. Sobol sensitivity
analysis. The geographical structures of real-world regions are distinct in
that they are difficult to reduce to single parameter values, making a unified
sensitivity analysis intractable. Yet analyzing the importance of geographical
structure on the sensitivity of other input parameters is important because a
strong effect would justify the use of models with real-world geographical
representations, as opposed to stylized ones.
  Here we perform a grouped Sobol's sensitivity analysis on COVID-19 spread
simulations across a set of three diverse real-world geographical
representations. We study the differences in both results and the sensitivity
of non-geographical parameters across these geographies. By comparing Sobol
indices of parameters across geographies, we find evidence that infection rate
could have more sensitivity in regions where the population is segregated,
while parameters like recovery period of mild cases are more sensitive in
regions with mixed populations. We also show how geographical structure affects
parameter sensitivity changes over time.


# Low-Resource Languages Jailbreak GPT-4

[Link to the paper](http://arxiv.org/abs/2310.02446v1)

## Authors
- Zheng-Xin Yong
- Cristina Menghini
- Stephen H. Bach

## Summary
  AI safety training and red-teaming of large language models (LLMs) are
measures to mitigate the generation of unsafe content. Our work exposes the
inherent cross-lingual vulnerability of these safety mechanisms, resulting from
the linguistic inequality of safety training data, by successfully
circumventing GPT-4's safeguard through translating unsafe English inputs into
low-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe
translated inputs and provides actionable items that can get the users towards
their harmful goals 79% of the time, which is on par with or even surpassing
state-of-the-art jailbreaking attacks. Other high-/mid-resource languages have
significantly lower attack success rate, which suggests that the cross-lingual
vulnerability mainly applies to low-resource languages. Previously, limited
training on low-resource languages primarily affects speakers of those
languages, causing technological disparities. However, our work highlights a
crucial shift: this deficiency now poses a risk to all LLMs users. Publicly
available translation APIs enable anyone to exploit LLMs' safety
vulnerabilities. Therefore, our work calls for a more holistic red-teaming
efforts to develop robust multilingual safeguards with wide language coverage.


# Novice Learner and Expert Tutor: Evaluating Math Reasoning Abilities of Large Language Models with Misconceptions

[Link to the paper](http://arxiv.org/abs/2310.02439v1)

## Authors
- Naiming Liu
- Shashank Sonkar
- Zichao Wang
- Simon Woodhead
- Richard G. Baraniuk

## Summary
  We propose novel evaluations for mathematical reasoning capabilities of Large
Language Models (LLMs) based on mathematical misconceptions. Our primary
approach is to simulate LLMs as a novice learner and an expert tutor, aiming to
identify the incorrect answer to math question resulted from a specific
misconception and to recognize the misconception(s) behind an incorrect answer,
respectively. Contrary to traditional LLMs-based mathematical evaluations that
focus on answering math questions correctly, our approach takes inspirations
from principles in educational learning sciences. We explicitly ask LLMs to
mimic a novice learner by answering questions in a specific incorrect manner
based on incomplete knowledge; and to mimic an expert tutor by identifying
misconception(s) corresponding to an incorrect answer to a question. Using
simple grade-school math problems, our experiments reveal that, while LLMs can
easily answer these questions correctly, they struggle to identify 1) the
incorrect answer corresponding to specific incomplete knowledge
(misconceptions); 2) the misconceptions that explain particular incorrect
answers. Our study indicates new opportunities for enhancing LLMs' math
reasoning capabilities, especially on developing robust student simulation and
expert tutoring models in the educational applications such as intelligent
tutoring systems.


# Multi-Agent Reinforcement Learning Based on Representational Communication for Large-Scale Traffic Signal Control

[Link to the paper](http://arxiv.org/abs/2310.02435v1)

## Authors
- Rohit Bokade
- Xiaoning Jin
- Christopher Amato

## Summary
  Traffic signal control (TSC) is a challenging problem within intelligent
transportation systems and has been tackled using multi-agent reinforcement
learning (MARL). While centralized approaches are often infeasible for
large-scale TSC problems, decentralized approaches provide scalability but
introduce new challenges, such as partial observability. Communication plays a
critical role in decentralized MARL, as agents must learn to exchange
information using messages to better understand the system and achieve
effective coordination. Deep MARL has been used to enable inter-agent
communication by learning communication protocols in a differentiable manner.
However, many deep MARL communication frameworks proposed for TSC allow agents
to communicate with all other agents at all times, which can add to the
existing noise in the system and degrade overall performance. In this study, we
propose a communication-based MARL framework for large-scale TSC. Our framework
allows each agent to learn a communication policy that dictates "which" part of
the message is sent "to whom". In essence, our framework enables agents to
selectively choose the recipients of their messages and exchange variable
length messages with them. This results in a decentralized and flexible
communication mechanism in which agents can effectively use the communication
channel only when necessary. We designed two networks, a synthetic $4 \times 4$
grid network and a real-world network based on the Pasubio neighborhood in
Bologna. Our framework achieved the lowest network congestion compared to
related methods, with agents utilizing $\sim 47-65 \%$ of the communication
channel. Ablation studies further demonstrated the effectiveness of the
communication policies learned within our framework.


# Can Large Language Models Provide Security & Privacy Advice? Measuring the Ability of LLMs to Refute Misconceptions

[Link to the paper](http://arxiv.org/abs/2310.02431v1)

## Authors
- Yufan Chen
- Arjun Arunasalam
- Z. Berkay Celik

## Summary
  Users seek security & privacy (S&P) advice from online resources, including
trusted websites and content-sharing platforms. These resources help users
understand S&P technologies and tools and suggest actionable strategies. Large
Language Models (LLMs) have recently emerged as trusted information sources.
However, their accuracy and correctness have been called into question. Prior
research has outlined the shortcomings of LLMs in answering multiple-choice
questions and user ability to inadvertently circumvent model restrictions
(e.g., to produce toxic content). Yet, the ability of LLMs to provide reliable
S&P advice is not well-explored. In this paper, we measure their ability to
refute popular S&P misconceptions that the general public holds. We first study
recent academic literature to curate a dataset of over a hundred S&P-related
misconceptions across six different topics. We then query two popular LLMs
(Bard and ChatGPT) and develop a labeling guide to evaluate their responses to
these misconceptions. To comprehensively evaluate their responses, we further
apply three strategies: query each misconception multiple times, generate and
query their paraphrases, and solicit source URLs of the responses. Both models
demonstrate, on average, a 21.3% non-negligible error rate, incorrectly
supporting popular S&P misconceptions. The error rate increases to 32.6% when
we repeatedly query LLMs with the same or paraphrased misconceptions. We also
expose that models may partially support a misconception or remain
noncommittal, refusing a firm stance on misconceptions. Our exploration of
information sources for responses revealed that LLMs are susceptible to
providing invalid URLs (21.2% for Bard and 67.7% for ChatGPT) or point to
unrelated sources (44.2% returned by Bard and 18.3% by ChatGPT).


# AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation

[Link to the paper](http://arxiv.org/abs/2308.08155v2)

## Authors
- Qingyun Wu
- Gagan Bansal
- Jieyu Zhang
- Yiran Wu
- Beibin Li
- Erkang Zhu
- Li Jiang
- Xiaoyun Zhang
- Shaokun Zhang
- Jiale Liu
- Ahmed Hassan Awadallah
- Ryen W White
- Doug Burger
- Chi Wang

## Summary
  AutoGen is an open-source framework that allows developers to build LLM
applications via multiple agents that can converse with each other to
accomplish tasks. AutoGen agents are customizable, conversable, and can operate
in various modes that employ combinations of LLMs, human inputs, and tools.
Using AutoGen, developers can also flexibly define agent interaction behaviors.
Both natural language and computer code can be used to program flexible
conversation patterns for different applications. AutoGen serves as a generic
infrastructure to build diverse applications of various complexities and LLM
capacities. Empirical studies demonstrate the effectiveness of the framework in
many example applications, with domains ranging from mathematics, coding,
question answering, operations research, online decision-making, entertainment,
etc.


# AXNav: Replaying Accessibility Tests from Natural Language

[Link to the paper](http://arxiv.org/abs/2310.02424v1)

## Authors
- Maryam Taeb
- Amanda Swearngin
- Eldon School
- Ruijia Cheng
- Yue Jiang
- Jeffrey Nichols

## Summary
  Developers and quality assurance testers often rely on manual testing to test
accessibility features throughout the product lifecycle. Unfortunately, manual
testing can be tedious, often has an overwhelming scope, and can be difficult
to schedule amongst other development milestones. Recently, Large Language
Models (LLMs) have been used for a variety of tasks including automation of
UIs, however to our knowledge no one has yet explored their use in controlling
assistive technologies for the purposes of supporting accessibility testing. In
this paper, we explore the requirements of a natural language based
accessibility testing workflow, starting with a formative study. From this we
build a system that takes as input a manual accessibility test (e.g., ``Search
for a show in VoiceOver'') and uses an LLM combined with pixel-based UI
Understanding models to execute the test and produce a chaptered, navigable
video. In each video, to help QA testers we apply heuristics to detect and flag
accessibility issues (e.g., Text size not increasing with Large Text enabled,
VoiceOver navigation loops). We evaluate this system through a 10 participant
user study with accessibility QA professionals who indicated that the tool
would be very useful in their current work and performed tests similarly to how
they would manually test the features. The study also reveals insights for
future work on using LLMs for accessibility testing.


# Delta-AI: Local objectives for amortized inference in sparse graphical models

[Link to the paper](http://arxiv.org/abs/2310.02423v1)

## Authors
- Jean-Pierre Falet
- Hae Beom Lee
- Nikolay Malkin
- Chen Sun
- Dragos Secrieru
- Dinghuai Zhang
- Guillaume Lajoie
- Yoshua Bengio

## Summary
  We present a new algorithm for amortized inference in sparse probabilistic
graphical models (PGMs), which we call $\Delta$-amortized inference
($\Delta$-AI). Our approach is based on the observation that when the sampling
of variables in a PGM is seen as a sequence of actions taken by an agent,
sparsity of the PGM enables local credit assignment in the agent's policy
learning objective. This yields a local constraint that can be turned into a
local loss in the style of generative flow networks (GFlowNets) that enables
off-policy training but avoids the need to instantiate all the random variables
for each parameter update, thus speeding up training considerably. The
$\Delta$-AI objective matches the conditional distribution of a variable given
its Markov blanket in a tractable learned sampler, which has the structure of a
Bayesian network, with the same conditional distribution under the target PGM.
As such, the trained sampler recovers marginals and conditional distributions
of interest and enables inference of partial subsets of variables. We
illustrate $\Delta$-AI's effectiveness for sampling from synthetic PGMs and
training latent variable models with sparse factor structure.


# Jailbreaker in Jail: Moving Target Defense for Large Language Models

[Link to the paper](http://arxiv.org/abs/2310.02417v1)

## Authors
- Bocheng Chen
- Advait Paliwal
- Qiben Yan

## Summary
  Large language models (LLMs), known for their capability in understanding and
following instructions, are vulnerable to adversarial attacks. Researchers have
found that current commercial LLMs either fail to be "harmless" by presenting
unethical answers, or fail to be "helpful" by refusing to offer meaningful
answers when faced with adversarial queries. To strike a balance between being
helpful and harmless, we design a moving target defense (MTD) enhanced LLM
system. The system aims to deliver non-toxic answers that align with outputs
from multiple model candidates, making them more robust against adversarial
attacks. We design a query and output analysis model to filter out unsafe or
non-responsive answers. %to achieve the two objectives of randomly selecting
outputs from different LLMs. We evaluate over 8 most recent chatbot models with
state-of-the-art adversarial queries. Our MTD-enhanced LLM system reduces the
attack success rate from 37.5\% to 0\%. Meanwhile, it decreases the response
refusal rate from 50\% to 0\%.


# Investigating the Catastrophic Forgetting in Multimodal Large Language Models

[Link to the paper](http://arxiv.org/abs/2309.10313v3)

## Authors
- Yuexiang Zhai
- Shengbang Tong
- Xiao Li
- Mu Cai
- Qing Qu
- Yong Jae Lee
- Yi Ma

## Summary
  Following the success of GPT4, there has been a surge in interest in
multimodal large language model (MLLM) research. This line of research focuses
on developing general-purpose LLMs through fine-tuning pre-trained LLMs and
vision models. However, catastrophic forgetting, a notorious phenomenon where
the fine-tuned model fails to retain similar performance compared to the
pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM).
In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the
catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier.
We first apply EMT to evaluate several open-source fine-tuned MLLMs and we
discover that almost all evaluated MLLMs fail to retain the same performance
levels as their vision encoders on standard image classification tasks.
Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess
performance throughout the fine-tuning. Interestingly, our results suggest that
early-stage fine-tuning on an image dataset improves performance across other
image datasets, by enhancing the alignment of text and visual features.
However, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in
a significant loss of generalizability, even when the image encoder remains
frozen. Our results suggest that MLLMs have yet to demonstrate performance on
par with their vision models on standard image classification tasks and the
current MLLM fine-tuning procedure still has room for improvement.


# Automated Bug Generation in the era of Large Language Models

[Link to the paper](http://arxiv.org/abs/2310.02407v1)

## Authors
- Ali Reza Ibrahimzada
- Yang Chen
- Ryan Rong
- Reyhaneh Jabbarvand

## Summary
  Bugs are essential in software engineering; many research studies in the past
decades have been proposed to detect, localize, and repair bugs in software
systems. Effectiveness evaluation of such techniques requires complex bugs,
i.e., those that are hard to detect through testing and hard to repair through
debugging. From the classic software engineering point of view, a
hard-to-repair bug differs from the correct code in multiple locations, making
it hard to localize and repair. Hard-to-detect bugs, on the other hand,
manifest themselves under specific test inputs and reachability conditions.
These two objectives, i.e., generating hard-to-detect and hard-to-repair bugs,
are mostly aligned; a bug generation technique can change multiple statements
to be covered only under a specific set of inputs. However, these two
objectives are conflicting for learning-based techniques: A bug should have a
similar code representation to the correct code in the training data to
challenge a bug prediction model to distinguish them. The hard-to-repair bug
definition remains the same but with a caveat: the more a bug differs from the
original code (at multiple locations), the more distant their representations
are and easier to be detected. We propose BugFarm, to transform arbitrary code
into multiple complex bugs. BugFarm leverages LLMs to mutate code in multiple
locations (hard-to-repair). To ensure that multiple modifications do not
notably change the code representation, BugFarm analyzes the attention of the
underlying model and instructs LLMs to only change the least attended locations
(hard-to-detect). Our comprehensive evaluation of 320k+ bugs from over 2.5M
mutants generated by BugFarm and two alternative approaches demonstrates our
superiority in generating bugs that are hard to detect by learning-based bug
prediction approaches and hard to repair by SOTA learning-based program repair
technique.


# Deductive Verification of Chain-of-Thought Reasoning

[Link to the paper](http://arxiv.org/abs/2306.03872v3)

## Authors
- Zhan Ling
- Yunhao Fang
- Xuanlin Li
- Zhiao Huang
- Mingu Lee
- Roland Memisevic
- Hao Su

## Summary
  Large Language Models (LLMs) significantly benefit from Chain-of-Thought
(CoT) prompting in performing various reasoning tasks. While CoT allows models
to produce more comprehensive reasoning processes, its emphasis on intermediate
reasoning steps can inadvertently introduce hallucinations and accumulated
errors, thereby limiting models' ability to solve complex reasoning tasks.
Inspired by how humans engage in careful and meticulous deductive logical
reasoning processes to solve tasks, we seek to enable language models to
perform explicit and rigorous deductive reasoning, and also ensure the
trustworthiness of their reasoning process through self-verification. However,
directly verifying the validity of an entire deductive reasoning process is
challenging, even with advanced models like ChatGPT. In light of this, we
propose to decompose a reasoning verification process into a series of
step-by-step subprocesses, each only receiving their necessary context and
premises. To facilitate this procedure, we propose Natural Program, a natural
language-based deductive reasoning format. Our approach enables models to
generate precise reasoning steps where subsequent steps are more rigorously
grounded on prior steps. It also empowers language models to carry out
reasoning self-verification in a step-by-step manner. By integrating this
verification process into each deductive reasoning stage, we significantly
enhance the rigor and trustfulness of generated reasoning steps. Along this
process, we also improve the answer correctness on complex reasoning tasks.
Code will be released at https://github.com/lz1oceani/verify_cot.


# A 3D Mixed Reality Interface for Human-Robot Teaming

[Link to the paper](http://arxiv.org/abs/2310.02392v1)

## Authors
- Jiaqi Chen
- Boyang Sun
- Marc Pollefeys
- Hermann Blum

## Summary
  This paper presents a mixed-reality human-robot teaming system. It allows
human operators to see in real-time where robots are located, even if they are
not in line of sight. The operator can also visualize the map that the robots
create of their environment and can easily send robots to new goal positions.
The system mainly consists of a mapping and a control module. The mapping
module is a real-time multi-agent visual SLAM system that co-localizes all
robots and mixed-reality devices to a common reference frame. Visualizations in
the mixed-reality device then allow operators to see a virtual life-sized
representation of the cumulative 3D map overlaid onto the real environment. As
such, the operator can effectively "see through" walls into other rooms. To
control robots and send them to new locations, we propose a drag-and-drop
interface. An operator can grab any robot hologram in a 3D mini map and drag it
to a new desired goal pose. We validate the proposed system through a user
study and real-world deployments. We make the mixed-reality application
publicly available at https://github.com/cvg/HoloLens_ros.


# Exponential Lower Bounds for Fictitious Play in Potential Games

[Link to the paper](http://arxiv.org/abs/2310.02387v1)

## Authors
- Ioannis Panageas
- Nikolas Patris
- Stratis Skoulakis
- Volkan Cevher

## Summary
  Fictitious Play (FP) is a simple and natural dynamic for repeated play with
many applications in game theory and multi-agent reinforcement learning. It was
introduced by Brown (1949,1951) and its convergence properties for two-player
zero-sum games was established later by Robinson (1951). Potential games
Monderer and Shapley (1996b) is another class of games which exhibit the FP
property (Monderer and Shapley (1996a)), i.e., FP dynamics converges to a Nash
equilibrium if all agents follows it. Nevertheless, except for two-player
zero-sum games and for specific instances of payoff matrices (Abernethy et al.
(2021)) or for adversarial tie-breaking rules (Daskalakis and Pan (2014)), the
convergence rate of FP is unknown. In this work, we focus on the rate of
convergence of FP when applied to potential games and more specifically
identical payoff games. We prove that FP can take exponential time (in the
number of strategies) to reach a Nash equilibrium, even if the game is
restricted to two agents and for arbitrary tie-breaking rules. To prove this,
we recursively construct a two-player coordination game with a unique Nash
equilibrium. Moreover, every approximate Nash equilibrium in the constructed
game must be close to the pure Nash equilibrium in $\ell_1$-distance.


# Conversational Health Agents: A Personalized LLM-Powered Agent Framework

[Link to the paper](http://arxiv.org/abs/2310.02374v1)

## Authors
- Mahyar Abbasian
- Iman Azimi
- Amir M. Rahmani
- Ramesh Jain

## Summary
  Conversational Health Agents (CHAs) are interactive systems designed to
enhance personal healthcare services by engaging in empathetic conversations
and processing multimodal data. While current CHAs, especially those utilizing
Large Language Models (LLMs), primarily focus on conversation, they often lack
comprehensive agent capabilities. This includes the ability to access personal
user health data from wearables, 24/7 data collection sources, and electronic
health records, as well as integrating the latest published health insights and
connecting with established multimodal data analysis tools. We are developing a
framework to empower CHAs by equipping them with critical thinking, knowledge
acquisition, and problem-solving abilities. Our CHA platform, powered by LLMs,
seamlessly integrates healthcare tools, enables multilingual and multimodal
conversations, and interfaces with a variety of user data analysis tools. We
illustrate its proficiency in handling complex healthcare tasks, such as stress
level estimation, showcasing the agent's cognitive and operational
capabilities.


# Reinforcement Learning from Automatic Feedback for High-Quality Unit Test Generation

[Link to the paper](http://arxiv.org/abs/2310.02368v1)

## Authors
- Benjamin Steenhoek
- Michele Tufano
- Neel Sundaresan
- Alexey Svyatkovskiy

## Summary
  Software testing is a crucial aspect of software development, and the
creation of high-quality tests that adhere to best practices is essential for
effective maintenance. Recently, Large Language Models (LLMs) have gained
popularity for code generation, including the automated creation of test cases.
However, these LLMs are often trained on vast amounts of publicly available
code, which may include test cases that do not adhere to best practices and may
even contain test smells (anti-patterns). To address this issue, we propose a
novel technique called Reinforcement Learning from Static Quality Metrics
(RLSQM). To begin, we analyze the anti-patterns generated by the LLM and show
that LLMs can generate undesirable test smells. Thus, we train specific reward
models for each static quality metric, then utilize Proximal Policy
Optimization (PPO) to train models for optimizing a single quality metric at a
time. Furthermore, we amalgamate these rewards into a unified reward model
aimed at capturing different best practices and quality aspects of tests. By
comparing RL-trained models with those trained using supervised learning, we
provide insights into how reliably utilize RL to improve test generation
quality and into the effects of various training strategies. Our experimental
results demonstrate that the RL-optimized model consistently generated
high-quality test cases compared to the base LLM, improving the model by up to
21%, and successfully generates nearly 100% syntactically correct code. RLSQM
also outperformed GPT-4 on four out of seven metrics. This represents a
significant step towards enhancing the overall efficiency and reliability of
software testing through Reinforcement Learning and static quality metrics. Our
data are available at this link: https://figshare.com/s/ded476c8d4c221222849.


# Event-Enhanced Multi-Modal Spiking Neural Network for Dynamic Obstacle Avoidance

[Link to the paper](http://arxiv.org/abs/2310.02361v1)

## Authors
- Yang Wang
- Bo Dong
- Yuji Zhang
- Yunduo Zhou
- Haiyang Mei
- Ziqi Wei
- Xin Yang

## Summary
  Autonomous obstacle avoidance is of vital importance for an intelligent agent
such as a mobile robot to navigate in its environment. Existing
state-of-the-art methods train a spiking neural network (SNN) with deep
reinforcement learning (DRL) to achieve energy-efficient and fast inference
speed in complex/unknown scenes. These methods typically assume that the
environment is static while the obstacles in real-world scenes are often
dynamic. The movement of obstacles increases the complexity of the environment
and poses a great challenge to the existing methods. In this work, we approach
robust dynamic obstacle avoidance twofold. First, we introduce the neuromorphic
vision sensor (i.e., event camera) to provide motion cues complementary to the
traditional Laser depth data for handling dynamic obstacles. Second, we develop
an DRL-based event-enhanced multimodal spiking actor network (EEM-SAN) that
extracts information from motion events data via unsupervised representation
learning and fuses Laser and event camera data with learnable thresholding.
Experiments demonstrate that our EEM-SAN outperforms state-of-the-art obstacle
avoidance methods by a significant margin, especially for dynamic obstacle
avoidance.


# ORTAC+ : A User Friendly Domain Specific Language for Multi-Agent Mission Planning

[Link to the paper](http://arxiv.org/abs/2310.02356v1)

## Authors
- Caroline Bonhomme
- Jean-Louis Dufour

## Summary
  A tactical military unit is a complex system composed of many agents such as
infantry, robots, or drones. Given a mission, an automated planner can find an
optimal plan. Therefore, the mission itself must be modeled. The problem is
that languages like PDDL are too low-level to be usable by the end-user: an
officer in the field. We present ORTAC+, a language and a planning tool
designed for this end-user. Its main objective is to allow a natural modeling
of the mission, to minimize the risk of bad modeling, and thus obtain reliable
plans. The language offers high-level constructs specifically designed to
describe tactical missions, but at the same time has clear semantics allowing a
translation to PDDL, to take advantage of state-of-the-art planners.


# Online Proactive Multi-Task Assignment with Resource Availability Anticipation

[Link to the paper](http://arxiv.org/abs/2310.02353v1)

## Authors
- DÃ©borah Conforto Nedelmann
- JÃ©rÃ´me Lacan
- Caroline Chanel

## Summary
  With the emergence of services and online applications as taxi dispatching,
crowdsourcing, package or food delivery, industrials and researchers are paying
attention to the online multi-task assignment optimization field to quickly and
efficiently met demands. In this context, this paper is interested in the
multi-task assignment problem where multiple requests (e.g. tasks) arrive over
time and must be dynamically matched to (mobile) agents. This optimization
problem is known to be NP-hard. In order to treat this problem with a proactive
mindset, we propose to use a receding-horizon approach to determine which
resources (e.g. taxis, mobile agents, drones, robots) would be available within
this (possibly dynamic) receding-horizon to meet the current set of requests
(i.e. tasks) as good as possible. Contrarily to several works in this domain,
we have chosen to make no assumption concerning future locations of requests.
To achieve fast optimized online solutions in terms of costs and amount of
allocated tasks, we have designed a genetic algorithm based on a fitness
function integrating the traveled distance and the age of the requests. We
compared our proactive multi-task assignment with resource availability
anticipation approach with a classical reactive approach. The results obtained
in two benchmark problems, one synthetic and another based on real data, show
that our resource availability anticipation method can achieve better results
in terms of costs (e.g. traveled distance) and amount of allocated tasks than
reactive approaches while decreasing resources idle time.


# Hierarchical Empowerment: Towards Tractable Empowerment-Based Skill Learning

[Link to the paper](http://arxiv.org/abs/2307.02728v2)

## Authors
- Andrew Levy
- Sreehari Rammohan
- Alessandro Allievi
- Scott Niekum
- George Konidaris

## Summary
  General purpose agents will require large repertoires of skills. Empowerment
-- the maximum mutual information between skills and states -- provides a
pathway for learning large collections of distinct skills, but mutual
information is difficult to optimize. We introduce a new framework,
Hierarchical Empowerment, that makes computing empowerment more tractable by
integrating concepts from Goal-Conditioned Hierarchical Reinforcement Learning.
Our framework makes two specific contributions. First, we introduce a new
variational lower bound on mutual information that can be used to compute
empowerment over short horizons. Second, we introduce a hierarchical
architecture for computing empowerment over exponentially longer time scales.
We verify the contributions of the framework in a series of simulated robotics
tasks. In a popular ant navigation domain, our four level agents are able to
learn skills that cover a surface area over two orders of magnitude larger than
prior work.


# The Impact of Strategies and Information in Model Checking for Multi-Agent Systems

[Link to the paper](http://arxiv.org/abs/2310.02342v1)

## Authors
- Vadim Malvone

## Summary
  System correctness is one of the most crucial and challenging objectives in
software and hardware systems. With the increasing evolution of connected and
distributed systems, ensuring their correctness requires the use of formal
verification for multi-agent systems. In this paper, we present a summary of
certain results on model checking for multi-agent systems that derive from the
selection of strategies and information for agents. Additionally, we discuss
some open directions for future research.


# AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration

[Link to the paper](http://arxiv.org/abs/2306.00978v2)

## Authors
- Ji Lin
- Jiaming Tang
- Haotian Tang
- Shang Yang
- Xingyu Dang
- Chuang Gan
- Song Han

## Summary
  Large language models (LLMs) have shown excellent performance on various
tasks, but the astronomical model size raises the hardware barrier for serving
(memory size) and slows down token generation (memory bandwidth). In this
paper, we propose Activation-aware Weight Quantization (AWQ), a
hardware-friendly approach for LLM low-bit weight-only quantization. Our method
is based on the observation that weights are not equally important: protecting
only 1% of salient weights can greatly reduce quantization error. We then
propose to search for the optimal per-channel scaling that protects the salient
weights by observing the activation, not weights. AWQ does not rely on any
backpropagation or reconstruction, so it can well preserve LLMs' generalization
ability on different domains and modalities, without overfitting to the
calibration set. AWQ outperforms existing work on various language modeling and
domain-specific benchmarks. Thanks to better generalization, it achieves
excellent quantization performance for instruction-tuned LMs and, for the first
time, multi-modal LMs. Alongside AWQ, we implement an efficient and flexible
inference framework tailored for LLMs on the edge, offering more than 3x
speedup over the Huggingface FP16 implementation on both desktop and mobile
GPUs. It also democratizes the deployment of the 70B Llama-2 model on mobile
GPU (NVIDIA Jetson Orin 64GB).


# REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction

[Link to the paper](http://arxiv.org/abs/2306.15724v3)

## Authors
- Zeyi Liu
- Arpit Bahety
- Shuran Song

## Summary
  The ability to detect and analyze failed executions automatically is crucial
for an explainable and robust robotic system. Recently, Large Language Models
(LLMs) have demonstrated strong reasoning abilities on textual inputs. To
leverage the power of LLMs for robot failure explanation, we introduce REFLECT,
a framework which queries LLM for failure reasoning based on a hierarchical
summary of robot past experiences generated from multisensory observations. The
failure explanation can further guide a language-based planner to correct the
failure and complete the task. To systematically evaluate the framework, we
create the RoboFail dataset with a variety of tasks and failure scenarios. We
demonstrate that the LLM-based framework is able to generate informative
failure explanations that assist successful correction planning.


# GJ 357 d: Potentially Habitable World or Agent of Chaos?

[Link to the paper](http://arxiv.org/abs/2310.02327v1)

## Authors
- Stephen R. Kane
- Tara Fetherolf

## Summary
  Multi-planet systems provide important laboratories for exploring dynamical
interactions within the range of known exoplanetary system architectures. One
such system is GJ 357, consisting of a low-mass host star and three orbiting
planets, the outermost (planet d) of which does not transit but lies within the
Habitable Zone (HZ) of the host star. The minimum mass of planet d causes its
nature to be unknown, both in terms of whether it is truly terrestrial and if
it is a candidate for harboring surface liquid water. Here, we use three
sectors of photometry from the Transiting Exoplanet Survey Satellite (TESS) to
show that planets c and d do not transit the host star, and therefore may have
masses higher than the derived minimum masses. We present the results for a
suite of dynamical simulations that inject an Earth-mass planet within the HZ
of the system for three different orbital and mass configurations of planet d.
These results show that planet d, rather than being a potentially habitable
planet, is likely a source of significant orbital instability for other
potential terrestrial planets within the HZ. We find that relatively small
eccentricities of planet d cause a majority of the HZ to be unstable for an
Earth-mass planet. These results highlight the importance of dynamical
stability for systems that are prioritized in the context of planetary
habitability.


# Contrastive Post-training Large Language Models on Data Curriculum

[Link to the paper](http://arxiv.org/abs/2310.02263v1)

## Authors
- Canwen Xu
- Corby Rosset
- Luciano Del Corro
- Shweti Mahajan
- Julian McAuley
- Jennifer Neville
- Ahmed Hassan Awadallah
- Nikhil Rao

## Summary
  Alignment serves as an important step to steer large language models (LLMs)
towards human preferences. In this paper, we explore contrastive post-training
techniques for alignment by automatically constructing preference pairs from
multiple models of varying strengths (e.g., InstructGPT, ChatGPT and GPT-4). We
carefully compare the contrastive techniques of SLiC and DPO to SFT baselines
and find that DPO provides a step-function improvement even after continueing
SFT saturates. We also explore a data curriculum learning scheme for
contrastive post-training, which starts by learning from "easier" pairs and
transitioning to "harder" ones, which further improves alignment. Finally, we
scale up our experiments to train with more data and larger models like Orca.
Remarkably, contrastive post-training further improves the performance of Orca,
already a state-of-the-art instruction learning model tuned with GPT-4 outputs,
to exceed that of ChatGPT.


# Generalizable Long-Horizon Manipulations with Large Language Models

[Link to the paper](http://arxiv.org/abs/2310.02264v1)

## Authors
- Haoyu Zhou
- Mingyu Ding
- Weikun Peng
- Masayoshi Tomizuka
- Lin Shao
- Chuang Gan

## Summary
  This work introduces a framework harnessing the capabilities of Large
Language Models (LLMs) to generate primitive task conditions for generalizable
long-horizon manipulations with novel objects and unseen tasks. These task
conditions serve as guides for the generation and adjustment of Dynamic
Movement Primitives (DMP) trajectories for long-horizon task execution. We
further create a challenging robotic manipulation task suite based on Pybullet
for long-horizon task evaluation. Extensive experiments in both simulated and
real-world environments demonstrate the effectiveness of our framework on both
familiar tasks involving new objects and novel but related tasks, highlighting
the potential of LLMs in enhancing robotic system versatility and adaptability.
Project website: https://object814.github.io/Task-Condition-With-LLM/


# Towards An Analytical Framework for Potential Games

[Link to the paper](http://arxiv.org/abs/2310.02259v1)

## Authors
- Xin Guo
- Yufei Zhang

## Summary
  Potential game is an emerging notion and framework for studying multi-agent
games, especially with heterogeneous agents. Up to date, potential games have
been extensively studied mostly from the algorithmic aspect in approximating
and computing the Nash equilibrium without verifying if the game is a potential
game, due to the lack of analytical structure.
  In this paper, we aim to build an analytical framework for dynamic potential
games. We prove that a game is a potential game if and only if each agent's
value function can be decomposed as a potential function and a residual term
that solely dependent on other agents' actions. This decomposition enables us
to identify and analyze a new and important class of potential games called the
distributed game. Moreover, by an appropriate notion of functional derivatives,
we prove that a game is a potential game if the value function has a symmetric
Jacobian. Consequently, for a general class of continuous-time stochastic
games, their potential functions can be further characterized from both the
probabilistic and the PDE approaches. The consistency of these two
characterisations are shown in a class of linear-quadratic games.


# MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts

[Link to the paper](http://arxiv.org/abs/2310.02255v1)

## Authors
- Pan Lu
- Hritik Bansal
- Tony Xia
- Jiacheng Liu
- Chunyuan Li
- Hannaneh Hajishirzi
- Hao Cheng
- Kai-Wei Chang
- Michel Galley
- Jianfeng Gao

## Summary
  Although Large Language Models (LLMs) and Large Multimodal Models (LMMs)
exhibit impressive skills in various domains, their ability for mathematical
reasoning within visual contexts has not been formally examined. Equipping LLMs
and LMMs with this capability is vital for general-purpose AI assistants and
showcases promising potential in education, data analysis, and scientific
discovery. To bridge this gap, we present MathVista, a benchmark designed to
amalgamate challenges from diverse mathematical and visual tasks. We first
taxonomize the key task types, reasoning skills, and visual contexts from the
literature to guide our selection from 28 existing math-focused and visual
question answering datasets. Then, we construct three new datasets, IQTest,
FunctionQA, and PaperQA, to accommodate for missing types of visual contexts.
The problems featured often require deep visual understanding beyond OCR or
image captioning, and compositional reasoning with rich domain-specific tools,
thus posing a notable challenge to existing models. We conduct a comprehensive
evaluation of 11 prominent open-source and proprietary foundation models (LLMs,
LLMs augmented with tools, and LMMs), and early experiments with GPT-4V. The
best-performing model, Multimodal Bard, achieves only 58% of human performance
(34.8% vs 60.3%), indicating ample room for further improvement. Given this
significant gap, MathVista fuels future research in the development of
general-purpose AI agents capable of tackling mathematically intensive and
visually rich real-world tasks. Preliminary tests show that MathVista also
presents challenges to GPT-4V, underscoring the benchmark's importance. The
project is available at https://mathvista.github.io/.


# MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens

[Link to the paper](http://arxiv.org/abs/2310.02239v1)

## Authors
- Kaizhi Zheng
- Xuehai He
- Xin Eric Wang

## Summary
  Large Language Models (LLMs) have garnered significant attention for their
advancements in natural language processing, demonstrating unparalleled prowess
in text comprehension and generation. Yet, the simultaneous generation of
images with coherent textual narratives remains an evolving frontier. In
response, we introduce an innovative interleaved vision-and-language generation
technique anchored by the concept of "generative vokens," acting as the bridge
for harmonized image-text outputs. Our approach is characterized by a
distinctive two-staged training strategy focusing on description-free
multimodal generation, where the training requires no comprehensive
descriptions of images. To bolster model integrity, classifier-free guidance is
incorporated, enhancing the effectiveness of vokens on image generation. Our
model, MiniGPT-5, exhibits substantial improvement over the baseline Divter
model on the MMDialog dataset and consistently delivers superior or comparable
multimodal outputs in human evaluations on the VIST dataset, highlighting its
efficacy across diverse benchmarks.


# Extraction of Medication and Temporal Relation from Clinical Text by Harnessing Different Deep Learning Models

[Link to the paper](http://arxiv.org/abs/2310.02229v1)

## Authors
- Hangyu Tu
- Lifeng Han
- Goran Nenadic

## Summary
  Clinical texts, represented in electronic medical records (EMRs), contain
rich medical information and are essential for disease prediction, personalised
information recommendation, clinical decision support, and medication pattern
mining and measurement. Relation extractions between medication mentions and
temporal information can further help clinicians better understand the
patients' treatment history. To evaluate the performances of deep learning (DL)
and large language models (LLMs) in medication extraction and temporal
relations classification, we carry out an empirical investigation of
\textbf{MedTem} project using several advanced learning structures including
BiLSTM-CRF and CNN-BiLSTM for a clinical domain named entity recognition (NER),
and BERT-CNN for temporal relation extraction (RE), in addition to the
exploration of different word embedding techniques. Furthermore, we also
designed a set of post-processing roles to generate structured output on
medications and the temporal relation. Our experiments show that CNN-BiLSTM
slightly wins the BiLSTM-CRF model on the i2b2-2009 clinical NER task yielding
75.67, 77.83, and 78.17 for precision, recall, and F1 scores using Macro
Average. BERT-CNN model also produced reasonable evaluation scores 64.48,
67.17, and 65.03 for P/R/F1 using Macro Avg on the temporal relation extraction
test set from i2b2-2012 challenges. Code and Tools from MedTem will be hosted
at \url{https://github.com/HECTA-UoM/MedTem}


# Can Language Models be Instructed to Protect Personal Information?

[Link to the paper](http://arxiv.org/abs/2310.02224v1)

## Authors
- Yang Chen
- Ethan Mendes
- Sauvik Das
- Wei Xu
- Alan Ritter

## Summary
  Large multimodal language models have proven transformative in numerous
applications. However, these models have been shown to memorize and leak
pre-training data, raising serious user privacy and information security
concerns. While data leaks should be prevented, it is also crucial to examine
the trade-off between the privacy protection and model utility of proposed
approaches. In this paper, we introduce PrivQA -- a multimodal benchmark to
assess this privacy/utility trade-off when a model is instructed to protect
specific categories of personal information in a simulated scenario. We also
propose a technique to iteratively self-moderate responses, which significantly
improves privacy. However, through a series of red-teaming experiments, we find
that adversaries can also easily circumvent these protections with simple
jailbreaking methods through textual and/or image inputs. We believe PrivQA has
the potential to support the development of new models with improved privacy
protections, as well as the adversarial robustness of these protections. We
release the entire PrivQA dataset at https://llm-access-control.github.io/.


# Language Models Represent Space and Time

[Link to the paper](http://arxiv.org/abs/2310.02207v1)

## Authors
- Wes Gurnee
- Max Tegmark

## Summary
  The capabilities of large language models (LLMs) have sparked debate over
whether such systems just learn an enormous collection of superficial
statistics or a coherent model of the data generating process -- a world model.
We find evidence for the latter by analyzing the learned representations of
three spatial datasets (world, US, NYC places) and three temporal datasets
(historical figures, artworks, news headlines) in the Llama-2 family of models.
We discover that LLMs learn linear representations of space and time across
multiple scales. These representations are robust to prompting variations and
unified across different entity types (e.g. cities and landmarks). In addition,
we identify individual ``space neurons'' and ``time neurons'' that reliably
encode spatial and temporal coordinates. Our analysis demonstrates that modern
LLMs acquire structured knowledge about fundamental dimensions such as space
and time, supporting the view that they learn not merely superficial
statistics, but literal world models.


# Abusing Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs

[Link to the paper](http://arxiv.org/abs/2307.10490v4)

## Authors
- Eugene Bagdasaryan
- Tsung-Yin Hsieh
- Ben Nassi
- Vitaly Shmatikov

## Summary
  We demonstrate how images and sounds can be used for indirect prompt and
instruction injection in multi-modal LLMs. An attacker generates an adversarial
perturbation corresponding to the prompt and blends it into an image or audio
recording. When the user asks the (unmodified, benign) model about the
perturbed image or audio, the perturbation steers the model to output the
attacker-chosen text and/or make the subsequent dialog follow the attacker's
instruction. We illustrate this attack with several proof-of-concept examples
targeting LLaVa and PandaGPT.


# Learning Task Automata for Reinforcement Learning using Hidden Markov Models

[Link to the paper](http://arxiv.org/abs/2208.11838v4)

## Authors
- Alessandro Abate
- Yousif Almulla
- James Fox
- David Hyland
- Michael Wooldridge

## Summary
  Training reinforcement learning (RL) agents using scalar reward signals is
often infeasible when an environment has sparse and non-Markovian rewards.
Moreover, handcrafting these reward functions before training is prone to
misspecification, especially when the environment's dynamics are only partially
known. This paper proposes a novel pipeline for learning non-Markovian task
specifications as succinct finite-state `task automata' from episodes of agent
experience within unknown environments. We leverage two key algorithmic
insights. First, we learn a product MDP, a model composed of the
specification's automaton and the environment's MDP (both initially unknown),
by treating the product MDP as a partially observable MDP and using the
well-known Baum-Welch algorithm for learning hidden Markov models. Second, we
propose a novel method for distilling the task automaton (assumed to be a
deterministic finite automaton) from the learnt product MDP. Our learnt task
automaton enables the decomposition of a task into its constituent sub-tasks,
which improves the rate at which an RL agent can later synthesise an optimal
policy. It also provides an interpretable encoding of high-level environmental
and task features, so a human can readily verify that the agent has learnt
coherent tasks with no misspecifications. In addition, we take steps towards
ensuring that the learnt automaton is environment-agnostic, making it
well-suited for use in transfer learning. Finally, we provide experimental
results compared with two baselines to illustrate our algorithm's performance
in different environments and tasks.


# A Case for AI Safety via Law

[Link to the paper](http://arxiv.org/abs/2309.12321v2)

## Authors
- Jeffrey W. Johnston

## Summary
  How to make artificial intelligence (AI) systems safe and aligned with human
values is an open research question. Proposed solutions tend toward relying on
human intervention in uncertain situations, learning human values and
intentions through training or observation, providing off-switches,
implementing isolation or simulation environments, or extrapolating what people
would want if they had more knowledge and more time to think. Law-based
approaches--such as inspired by Isaac Asimov--have not been well regarded. This
paper makes a case that effective legal systems are the best way to address AI
safety. Law is defined as any rules that codify prohibitions and prescriptions
applicable to particular agents in specified domains/contexts and includes
processes for enacting, managing, enforcing, and litigating such rules.


# Ask Again, Then Fail: Large Language Models' Vacillations in Judgement

[Link to the paper](http://arxiv.org/abs/2310.02174v1)

## Authors
- Qiming Xie
- Zengzhi Wang
- Yi Feng
- Rui Xia

## Summary
  With the emergence of generative conversational large language models (LLMs)
like ChatGPT, serving as virtual assistants in various fields, the stability
and reliability of their responses have become crucial. However, during usage,
it has been observed that these models tend to waver in their judgements when
confronted with follow-up questions from users expressing skepticism or
disagreement. In this work, we draw inspiration from questioning strategies in
education and propose a \textsc{Follow-up Questioning Mechanism} along with two
evaluation metrics to assess the judgement consistency of LLMs before and after
exposure to disturbances. We evaluate the judgement consistency of ChatGPT,
PaLM2-Bison, and Vicuna-13B under this mechanism across eight reasoning
benchmarks. Empirical results show that even when the initial answers are
correct, judgement consistency sharply decreases when LLMs face disturbances
such as questioning, negation, or misleading. Additionally, we study these
models' judgement consistency under various settings (sampling temperature and
prompts) to validate this issue further, observing the impact of prompt tone
and conducting an in-depth error analysis for deeper behavioral insights.
Furthermore, we also explore several prompting methods to mitigate this issue
and demonstrate their
effectiveness\footnote{\url{https://github.com/NUSTM/LLMs-Waver-In-Judgements}}.


# Lyfe Agents: Generative agents for low-cost real-time social interactions

[Link to the paper](http://arxiv.org/abs/2310.02172v1)

## Authors
- Zhao Kaiya
- Michelangelo Naim
- Jovana Kondic
- Manuel Cortes
- Jiaxin Ge
- Shuying Luo
- Guangyu Robert Yang
- Andrew Ahn

## Summary
  Highly autonomous generative agents powered by large language models promise
to simulate intricate social behaviors in virtual societies. However, achieving
real-time interactions with humans at a low computational cost remains
challenging. Here, we introduce Lyfe Agents. They combine low-cost with
real-time responsiveness, all while remaining intelligent and goal-oriented.
Key innovations include: (1) an option-action framework, reducing the cost of
high-level decisions; (2) asynchronous self-monitoring for better
self-consistency; and (3) a Summarize-and-Forget memory mechanism, prioritizing
critical memory items at a low cost. We evaluate Lyfe Agents' self-motivation
and sociability across several multi-agent scenarios in our custom LyfeGame 3D
virtual environment platform. When equipped with our brain-inspired techniques,
Lyfe Agents can exhibit human-like self-motivated social reasoning. For
example, the agents can solve a crime (a murder mystery) through autonomous
collaboration and information exchange. Meanwhile, our techniques enabled Lyfe
Agents to operate at a computational cost 10-100 times lower than existing
alternatives. Our findings underscore the transformative potential of
autonomous generative agents to enrich human social experiences in virtual
worlds.


# Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Team Optimization

[Link to the paper](http://arxiv.org/abs/2310.02170v1)

## Authors
- Zijun Liu
- Yanzhe Zhang
- Peng Li
- Yang Liu
- Diyi Yang

## Summary
  Large language model (LLM) agents have been shown effective on a wide range
of tasks, and by ensembling multiple LLM agents, their performances could be
further improved. Existing approaches employ a fixed set of agents to interact
with each other in a static architecture, which limits their generalizability
to various tasks and requires strong human prior in designing these agents. In
this work, we propose to construct a strategic team of agents communicating in
a dynamic interaction architecture based on the task query. Specifically, we
build a framework named Dynamic LLM-Agent Network ($\textbf{DyLAN}$) for
LLM-agent collaboration on complicated tasks like reasoning and code
generation. DyLAN enables agents to interact for multiple rounds in a dynamic
architecture with inference-time agent selection and an early-stopping
mechanism to improve performance and efficiency. We further design an automatic
agent team optimization algorithm based on an unsupervised metric termed
$\textit{Agent Importance Score}$, enabling the selection of best agents based
on the contribution each agent makes. Empirically, we demonstrate that DyLAN
performs well in both reasoning and code generation tasks with reasonable
computational cost. DyLAN achieves 13.0% and 13.3% improvement on MATH and
HumanEval, respectively, compared to a single execution on GPT-35-turbo. On
specific subjects of MMLU, agent team optimization in DyLAN increases accuracy
by up to 25.0%.


# Editing Personality for LLMs

[Link to the paper](http://arxiv.org/abs/2310.02168v1)

## Authors
- Shengyu Mao
- Ningyu Zhang
- Xiaohan Wang
- Mengru Wang
- Yunzhi Yao
- Yong Jiang
- Pengjun Xie
- Fei Huang
- Huajun Chen

## Summary
  This paper introduces an innovative task focused on editing the personality
traits of Large Language Models (LLMs). This task seeks to adjust the models'
responses to opinion-related questions on specified topics since an
individual's personality often manifests in the form of their expressed
opinions, thereby showcasing different personality traits. Specifically, we
construct a new benchmark dataset PersonalityEdit to address this task. Drawing
on the theory in Social Psychology, we isolate three representative traits,
namely Neuroticism, Extraversion, and Agreeableness, as the foundation for our
benchmark. We then gather data using GPT-4, generating responses that not only
align with a specified topic but also embody the targeted personality trait. We
conduct comprehensive experiments involving various baselines and discuss the
representation of personality behavior in LLMs. Our intriguing findings uncover
potential challenges of the proposed task, illustrating several remaining
issues. We anticipate that our work can provide the NLP community with
insights. Code and datasets will be released at
https://github.com/zjunlp/EasyEdit.


# Selenite: Scaffolding Decision Making with Comprehensive Overviews Elicited from Large Language Models

[Link to the paper](http://arxiv.org/abs/2310.02161v1)

## Authors
- Michael Xieyang Liu
- Tongshuang Wu
- Tianying Chen
- Franklin Mingzhe Li
- Aniket Kittur
- Brad A. Myers

## Summary
  Decision-making in unfamiliar domains can be challenging, demanding
considerable user effort to compare different options with respect to various
criteria. Prior research and our formative study found that people would
benefit from seeing an overview of the information space upfront, such as the
criteria that others have previously found useful. However, existing
sensemaking tools struggle with the "cold-start" problem -- it not only
requires significant input from previous users to generate and share these
overviews, but such overviews may also be biased and incomplete. In this work,
we introduce a novel system, Selenite, which leverages LLMs as reasoning
machines and knowledge retrievers to automatically produce a comprehensive
overview of options and criteria to jumpstart users' sensemaking processes.
Subsequently, Selenite also adapts as people use it, helping users find, read,
and navigate unfamiliar information in a systematic yet personalized manner.
Through three studies, we found that Selenite produced accurate and
high-quality overviews reliably, significantly accelerated users' information
processing, and effectively improved their overall comprehension and
sensemaking experience.


# On Truthful Constrained Heterogeneous Facility Location with Max-Variant Cost

[Link to the paper](http://arxiv.org/abs/2309.05379v2)

## Authors
- Mohammad Lotfi
- Alexandros A. Voudouris

## Summary
  We consider a problem where agents have private positions on a line, and
public approval preferences over two facilities, and their cost is the maximum
distance from their approved facilities. The goal is to decide the facility
locations to minimize the total and the max cost, while incentivizing the
agents to be truthful. We design a strategyproof mechanism that is
simultaneously $11$- and $5$-approximate for these two objective functions,
thus improving the previously best-known bounds of $2n+1$ and $9$.


# Unveiling the Pitfalls of Knowledge Editing for Large Language Models

[Link to the paper](http://arxiv.org/abs/2310.02129v1)

## Authors
- Zhoubo Li
- Ningyu Zhang
- Yunzhi Yao
- Mengru Wang
- Xi Chen
- Huajun Chen

## Summary
  As the cost associated with fine-tuning Large Language Models (LLMs)
continues to rise, recent research efforts have pivoted towards developing
methodologies to edit implicit knowledge embedded within LLMs. Yet, there's
still a dark cloud lingering overhead -- will knowledge editing trigger
butterfly effect? since it is still unclear whether knowledge editing might
introduce side effects that pose potential risks or not. This paper pioneers
the investigation into the potential pitfalls associated with knowledge editing
for LLMs. To achieve this, we introduce new benchmark datasets and propose
innovative evaluation metrics. Our results underline two pivotal concerns: (1)
Knowledge Conflict: Editing groups of facts that logically clash can magnify
the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)
Knowledge Distortion: Altering parameters with the aim of editing factual
knowledge can irrevocably warp the innate knowledge structure of LLMs.
Experimental results vividly demonstrate that knowledge editing might
inadvertently cast a shadow of unintended consequences on LLMs, which warrant
attention and efforts for future works. Code will be released at
https://github.com/zjunlp/PitfallsKnowledgeEditing.


# Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View

[Link to the paper](http://arxiv.org/abs/2310.02124v1)

## Authors
- Jintian Zhang
- Xin Xu
- Shumin Deng

## Summary
  As Natural Language Processing (NLP) systems are increasingly employed in
intricate social environments, a pressing query emerges: Can these NLP systems
mirror human-esque collaborative intelligence, in a multi-agent society
consisting of multiple large language models (LLMs)? This paper probes the
collaboration mechanisms among contemporary NLP systems by melding practical
experiments with theoretical insights. We fabricate four unique `societies'
comprised of LLM agents, where each agent is characterized by a specific
`trait' (easy-going or overconfident) and engages in collaboration with a
distinct `thinking pattern' (debate or reflection). Evaluating these
multi-agent societies on three benchmark datasets, we discern that LLM agents
navigate tasks by leveraging diverse social behaviors, from active debates to
introspective reflections. Notably, certain collaborative strategies only
optimize efficiency (using fewer API tokens), but also outshine previous
top-tier approaches. Moreover, our results further illustrate that LLM agents
manifest human-like social behaviors, such as conformity or majority rule,
mirroring foundational Social Psychology theories. In conclusion, we integrate
insights from Social Psychology to contextualize the collaboration of LLM
agents, inspiring further investigations into the collaboration mechanism for
LLMs. We commit to sharing our code and datasets (already submitted in
supplementary materials), hoping to catalyze further research in this promising
avenue (All code and data are available at
\url{https://github.com/zjunlp/MachineSoM}.).


# Fast algorithm for centralized multi-agent maze exploration

[Link to the paper](http://arxiv.org/abs/2310.02121v1)

## Authors
- Bojan CrnkoviÄ
- Stefan IviÄ
- Mila Zovko

## Summary
  Recent advancements in robotics have paved the way for robots to replace
humans in perilous situations, such as searching for victims in blazing
buildings, earthquake-damaged structures, uncharted caves, traversing
minefields, or patrolling crime-ridden streets. These challenges can be
generalized as problems where agents need to explore unknown mazes. Although
various algorithms for single-agent maze exploration exist, extending them to
multi-agent systems poses complexities.
  We propose a solution: a cooperative multi-agent system of automated mobile
agents for exploring unknown mazes and locating stationary targets. Our
algorithm employs a potential field governing maze exploration, integrating
cooperative agent behaviors like collision avoidance, coverage coordination,
and path planning.
  This approach builds upon the Heat Equation Driven Area Coverage (HEDAC)
method by Ivi\'c, Crnkovi\'c, and Mezi\'c. Unlike previous continuous domain
applications, we adapt HEDAC for discrete domains, specifically mazes divided
into nodes. Our algorithm is versatile, easily modified for anti-collision
requirements, and adaptable to expanding mazes and numerical meshes over time.
  Comparative evaluations against alternative maze-solving methods illustrate
our algorithm's superiority. The results highlight significant enhancements,
showcasing its applicability across diverse mazes. Numerical simulations affirm
its robustness, adaptability, scalability, and simplicity, enabling centralized
parallel computation in autonomous systems of basic agents/robots.


